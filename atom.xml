<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dream Maker</title>
  
  <subtitle>Love Math, Science, Biology, Computer science</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yuehhua.github.io/"/>
  <updated>2019-01-19T16:10:25.149Z</updated>
  <id>https://yuehhua.github.io/</id>
  
  <author>
    <name>Yueh-Hua Tu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>溫柔而強大</title>
    <link href="https://yuehhua.github.io/2019/01/19/kindness-and-powerful/"/>
    <id>https://yuehhua.github.io/2019/01/19/kindness-and-powerful/</id>
    <published>2019-01-19T15:07:41.000Z</published>
    <updated>2019-01-19T16:10:25.149Z</updated>
    
    <content type="html"><![CDATA[<p>已經好一段時間沒有用文章紀錄下自己的心情跟想法了。</p><p>這段期間都在專注寫技術文章。</p><a id="more"></a><p>在 2018 年 5 月剛好興趣參與了 Pytorch Taichung 的社群聚會，當時其實只是想去看看 Deep learning 技術發展到什麼階段了，自己當時也讀了一些進階的論文，像是 Attention model。</p><p>第一場是我給的開場演講，介紹了機器學習的概論。記得當時的主持人是生澀的副社長敬宇，以國際演討會的規格介紹整個流程，看來不是怎麼熟練，卻很有精神。</p><p>在經營社群來說，我也在群眾前演講一段時間了。一般社群小聚互動其實沒有那麼拘謹，我也就輕鬆開始我的演講。</p><p>記得演講結束之後，大家不太發言，是由副社問了 dropout 機制的問題，展開了一系列的討論。</p><p>往後一直到我出了我第一本書，這本書成為一個契機。敬宇跟我敲碗很久了，也就剛好拿到書的那個禮拜就有機會拿給他，就一起吃了午餐。</p><p>兩個人聊的很開心，也不顧時間到了，依依不捨。記得他下午還要趕去做實驗呢。</p><p>以往社群的人很欣賞我論文的報告，不流於形式，好理解，敬宇更是成為了迷弟。</p><p>後來就成為了很要好的朋友，無所不聊，從數學物理電腦到經濟哲學價值觀。</p><p>慢慢了解到敬宇的一些事情，發現他是需要幫助的孩子。</p><p>表面上沈默，私底下非常努力的孩子，慢熟而堅強。</p><p>身為好友應該可以給一些支援，雖然他很少主動開口。</p><p>對我而言，有能力幫助別人也是一種成功，成為一個 Giver。</p><p>一個溫柔而強大的 Giver。</p><p>大概是我這一年目標想做的的事情吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已經好一段時間沒有用文章紀錄下自己的心情跟想法了。&lt;/p&gt;
&lt;p&gt;這段期間都在專注寫技術文章。&lt;/p&gt;
    
    </summary>
    
      <category term="My Style" scheme="https://yuehhua.github.io/categories/My-Style/"/>
    
      <category term="Friends" scheme="https://yuehhua.github.io/categories/My-Style/Friends/"/>
    
    
  </entry>
  
  <entry>
    <title>The Deterministic Information Bottleneck</title>
    <link href="https://yuehhua.github.io/2019/01/18/deterministic-information-bottleneck/"/>
    <id>https://yuehhua.github.io/2019/01/18/deterministic-information-bottleneck/</id>
    <published>2019-01-18T03:39:06.000Z</published>
    <updated>2019-01-19T16:10:31.676Z</updated>
    
    <content type="html"><![CDATA[<p>在 Information Bottleneck 之後出現了不少驚豔的呼喊，也出現了指出這個方法的缺點及詮釋錯誤。</p><p>在這之後有人專注在確定性這件事上。</p><a id="more"></a><iframe width="560" height="315" src="https://www.youtube.com/embed/-D5b_zCJxrs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><pre class="mermaid">graph LR    X -->|"p(x, y)"|Y    X -->|"q(t|x)"|T    T --- Y    X((X))    Y((Y))    T((T))</pre><p>$T$:</p><ul><li>soft sufficient statistics (for statistics)</li><li>lossy compression (for signal)</li><li>maximally informative clustering (for machine learning)</li></ul><h2 id="IB"><a href="#IB" class="headerlink" title="IB"></a>IB</h2><p>$$<br>min\ \mathcal{L} [q(t|x)] = I(T; X) - \beta I(T; Y), \beta &gt; 0<br>$$</p><p>$I(T; X)$: compression<br>$I(T; Y)$: relevance</p><p>Markov constraint: $T \leftarrow X \leftrightarrow Y$</p><p>$$<br>q(t|x) = \frac{q(t)}{Z(x, \beta)} exp(- \beta D_{KL} [p(y|x) || q(y|t)])) \\<br>q(t) = \sum_x p(x)q(t|x) \\<br>q(y|t) = \frac{1}{q(t)} \sum_x p(y|x)q(t|x)p(x)<br>$$</p><p>$I(T; X)$ from channel coding, rate distortion theory</p><h2 id="DIB"><a href="#DIB" class="headerlink" title="DIB"></a>DIB</h2><p>$$<br>min\ \mathcal{L} [q(t|x)] = H(T) - \beta I(T; Y)<br>$$</p><p>$H(T)$: penalize coding itself<br>$I(T; Y)$: lead to deterministic $\mathcal{L}_{IB}$</p><p>$$<br>\mathcal{L} _{IB} - \mathcal{L} _{DIB} = I(T; X) - H(T) = -H(T|X)<br>$$</p><p>$\mathcal{L}_{IB}$: implicit encourage of stochastic</p><h2 id="Generalized-IB"><a href="#Generalized-IB" class="headerlink" title="Generalized IB"></a>Generalized IB</h2><p>$$<br>\mathcal{L}_{\alpha} = H(T) - \alpha H(T|X) - \beta I(Y; T)<br>$$</p><p>$\alpha = 1 \Rightarrow \mathcal{L} _{IB}$: stochastic $\rightarrow$ soft clustering<br>$\alpha = 0 \Rightarrow \mathcal{L} _{DIB}$: deterministic $\rightarrow$ hard clustering</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Information Bottleneck 之後出現了不少驚豔的呼喊，也出現了指出這個方法的缺點及詮釋錯誤。&lt;/p&gt;
&lt;p&gt;在這之後有人專注在確定性這件事上。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="https://yuehhua.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>共形映射</title>
    <link href="https://yuehhua.github.io/2019/01/16/conformal-map/"/>
    <id>https://yuehhua.github.io/2019/01/16/conformal-map/</id>
    <published>2019-01-16T06:22:27.000Z</published>
    <updated>2019-01-16T06:41:37.404Z</updated>
    
    <content type="html"><![CDATA[<p>$<br>\Omega \subset \mathbb{R}^2, f: \Omega \rightarrow \mathbb{R}^2<br>$</p><p>$$<br>f(x, y) = (u(x, y), v(x, y))<br>$$</p><p>$$<br>J(x, y) =<br>\begin{bmatrix}<br>\frac{\partial u}{\partial x}&amp; \frac{\partial u}{\partial y} \\<br>\frac{\partial v}{\partial x}&amp; \frac{\partial v}{\partial y}<br>\end{bmatrix}<br>$$</p><p>$<br>(x, y) \in \Omega, J(x, y) = s(x, y)R(x, y)<br>$</p><p>$s$ is a non-zero scalar.</p><p>$R$ is a $2 \times 2$ rotation matrix.</p><h5 id="mathcal-prop"><a href="#mathcal-prop" class="headerlink" title="$\mathcal{prop.}$"></a>$\mathcal{prop.}$</h5><ol><li>$f: \Omega \rightarrow \mathbb{R}^2 \text{ and } g: f(\Omega) \rightarrow \mathbb{R}^2$ are conformal map, then $g \circ f$ is conformal map</li><li>$f: \Omega \rightarrow \mathbb{R}^2$ is conformal map, $f^{-1}$ is conformal map</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$&lt;br&gt;\Omega \subset \mathbb{R}^2, f: \Omega \rightarrow \mathbb{R}^2&lt;br&gt;$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;f(x, y) = (u(x, y), v(x, y))&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;J(
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>保角映射</title>
    <link href="https://yuehhua.github.io/2019/01/16/angle-preserving-map/"/>
    <id>https://yuehhua.github.io/2019/01/16/angle-preserving-map/</id>
    <published>2019-01-16T03:47:36.000Z</published>
    <updated>2019-01-16T06:33:29.124Z</updated>
    
    <content type="html"><![CDATA[<h5 id="mathcal-Def"><a href="#mathcal-Def" class="headerlink" title="$\mathcal{Def.}$"></a>$\mathcal{Def.}$</h5><p>$A$ 為保角映射（angle-preserving map）</p><p>$$<br>\frac{(Ax)^T(Ay)}{||Ax|| \cdot ||Ay||} = \frac{x^Ty}{||x|| \cdot ||y||} \\<br>(\Rightarrow A\text{ is invertible})<br>$$</p><p>$$<br>\Rightarrow A = sQ, Q^TQ = I, s \ne 0<br>$$</p><p>$s$ 代表伸縮量</p><p>$det Q = 1$: 伸縮 + 旋轉<br>$det Q = -1$: 伸縮 + 鏡射</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;mathcal-Def&quot;&gt;&lt;a href=&quot;#mathcal-Def&quot; class=&quot;headerlink&quot; title=&quot;$\mathcal{Def.}$&quot;&gt;&lt;/a&gt;$\mathcal{Def.}$&lt;/h5&gt;&lt;p&gt;$A$ 為保角映射（angle-preservi
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>等距同構</title>
    <link href="https://yuehhua.github.io/2019/01/16/isometry/"/>
    <id>https://yuehhua.github.io/2019/01/16/isometry/</id>
    <published>2019-01-16T03:47:19.000Z</published>
    <updated>2019-01-16T06:07:41.143Z</updated>
    
    <content type="html"><![CDATA[<p>let $A \in \mathbb{R}^{n \times n}, f(x) = Ax$</p><p>$A$ 為保長映射（length-preserving map）或等距同構（isometry），以下為等價定義方式：</p><h5 id="mathcal-Def"><a href="#mathcal-Def" class="headerlink" title="$\mathcal{Def}$."></a>$\mathcal{Def}$.</h5><ol><li>$A$ is orthogonal matrix</li><li>$||Ax|| = ||x||$</li><li>$||Ax - Ay|| = ||x - y||$</li><li>$(Ax)^T(Ay) = x^Ty$</li></ol><p>$\Rightarrow$ 旋轉</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;let $A \in \mathbb{R}^{n \times n}, f(x) = Ax$&lt;/p&gt;
&lt;p&gt;$A$ 為保長映射（length-preserving map）或等距同構（isometry），以下為等價定義方式：&lt;/p&gt;
&lt;h5 id=&quot;mathcal-Def&quot;
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>Functional</title>
    <link href="https://yuehhua.github.io/2019/01/15/functional/"/>
    <id>https://yuehhua.github.io/2019/01/15/functional/</id>
    <published>2019-01-15T12:34:54.000Z</published>
    <updated>2019-01-15T12:55:39.273Z</updated>
    
    <content type="html"><![CDATA[<p>一般我們數學上稱 $f: \mathbb{R} \rightarrow \mathbb{R}$ 為函數 function。</p><p>然而，如果一個函數可以接受另一個函數作為他的輸入變數，而輸出是一個純量，$F: S \rightarrow \mathbb{R}$ 泛函 functional，其中 $S$ 是一個向量空間，函數是一種廣義的向量。</p><p>在最佳化理論或是機器學習當中最常遇到的就是損失函數 $\mathcal{L}$，他其實是一個泛函。</p><p>$$<br>\mathcal{L}[f] = \frac{1}{N} \sum_{i=1}^{N} (y_i - f(x_i))^2<br>$$</p><p>當我們有不同的資料，需要計算這些資料的 mean square error 的時候就會寫成像上面這個樣子。</p><p>$$<br>\mathcal{L}[f] = \lim_{n \rightarrow \infty} \int_b^a |f(x) - \sum_{i=1}^N a_i f_i(x)|^2 dx<br>$$</p><p>如果我們處理的不是資料，而是一段連續的空間，那我們就可以用以上這個連續的版本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一般我們數學上稱 $f: \mathbb{R} \rightarrow \mathbb{R}$ 為函數 function。&lt;/p&gt;
&lt;p&gt;然而，如果一個函數可以接受另一個函數作為他的輸入變數，而輸出是一個純量，$F: S \rightarrow \mathbb{R}$ 泛函
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>Compare Clustering and Embedding</title>
    <link href="https://yuehhua.github.io/2019/01/15/compare-clustering-and-embedding/"/>
    <id>https://yuehhua.github.io/2019/01/15/compare-clustering-and-embedding/</id>
    <published>2019-01-15T01:10:33.000Z</published>
    <updated>2019-01-15T01:22:16.348Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th></th><th>Clustering</th><th>Embedding</th></tr></thead><tbody><tr><td>Target space</td><td>discrete</td><td>continuous</td></tr><tr><td>Target dimension</td><td>$d$</td><td>$\mathbb{R}^d$</td></tr><tr><td>Transformed result can be</td><td>composable</td><td>correlated</td></tr><tr><td>Assumption</td><td>globally static context (dataset)</td><td>globally dynamic context (dataset)</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Clustering&lt;/th&gt;
&lt;th&gt;Embedding&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Target space&lt;/td&gt;
&lt;td&gt;discrete&lt;/td&gt;
&lt;t
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>方均根、標準差、馬克士威-波茲曼分佈</title>
    <link href="https://yuehhua.github.io/2018/12/20/rms-and-variance/"/>
    <id>https://yuehhua.github.io/2018/12/20/rms-and-variance/</id>
    <published>2018-12-19T16:19:39.000Z</published>
    <updated>2019-01-01T13:24:51.817Z</updated>
    
    <content type="html"><![CDATA[<p>不久前跟好朋友聊天聊到 random walk 的問題，一個醉漢會在一維的空間上隨機往前或往後走一步。</p><p>這樣的話，他最後會走到哪裡去呢？</p><a id="more"></a><p>基本上如果有看過隨機過程的話，會知道當 $n \rightarrow \infty$ 的時候會收斂到原點。</p><h2 id="平均位移"><a href="#平均位移" class="headerlink" title="平均位移"></a>平均位移</h2><p>如果我們考慮走了 $N$ 步之後的位移 $S$</p><p>$$<br>S = \sum_{i=1}^{N} x_i<br>$$</p><p>那麼平均位移說起來就是</p><p>$$<br>\mathbb{E}[S] = \mathbb{E}[\sum_{i=1}^{N} x_i] = \sum_{i=1}^{N} \mathbb{E}[x_i]<br>$$</p><p>當中的 $\mathbb{E}[x_i]$，由於每次要不是往前走一步或是往後退一步，而且兩者發生的機率一樣，所以每步的位移平均是 0，所以整體平均位移也是 0。</p><p>$$<br>\mathbb{E}[S] = 0<br>$$</p><p>如果你真的用電腦跑模擬，去紀錄多次走 5000 步（或是更多）最終的位移會是多少。</p><p>如此一來，你會得到一個以 0 為平均的常態分佈曲線。</p><h2 id="有沒有更有意義的資訊？"><a href="#有沒有更有意義的資訊？" class="headerlink" title="有沒有更有意義的資訊？"></a>有沒有更有意義的資訊？</h2><p>我們除了可以看平均以外還可以看什麼？</p><p>我們或許可以看位移平方後的平均</p><p>$$<br>\mathbb{E}[S^2] = \mathbb{E}[(\sum_{i=1}^{N} x_i)^2] \\<br>= \mathbb{E}[(x_1 + x_2 + \dots + x_N)^2] \\<br>= \mathbb{E}[(x_1^2 + x_2^2 + \dots + x_N^2) + 2(x_1x_2 + x_1x_3 + \dots + x_{N-1}x_N)] \\<br>= \mathbb{E}[x_1^2] + \mathbb{E}[x_2^2] + \dots + \mathbb{E}[x_N^2] + 2(\mathbb{E}[x_1x_2] + \mathbb{E}[x_1x_3] + \dots + \mathbb{E}[x_{N-1}x_N)]) \\<br>$$</p><p>如同前面的假設，如果我們將位移給平方了，那我們會得到每一項都是 1。至於相乘項的部份，可以自己動手試試看計算比較小的組合，不過理論上會是 0。</p><p>$$<br>= 1 + 1 + \dots + 1 + 2(0 + 0 + \dots + 0) \\<br>= N<br>$$</p><p>我們得到了位移平方後的平均是 $N$！</p><h2 id="方均根"><a href="#方均根" class="headerlink" title="方均根"></a>方均根</h2><p>大家可能在高中物理中聽到方均根（root-mean-square）這個計算方式，我們也可以求得方均根位移。</p><p>只要再開個根號就可以了，$\sqrt{\mathbb{E}[S^2]} = \sqrt{N}$。</p><p>這東西是不是看起來跟統計上的標準差很像呢？</p><p>$$<br>\sigma = \sqrt{Var[X]} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x - \mu)^2}<br>$$</p><p>大概差別只會在於裏面有沒有把 $\mu$ 給減掉而已，不過也可以藉由將 $\mu$ 設定成 0 來達到同樣的效果。</p><p>意思也就是說，當醉漢走了 $N$ 步之後，會呈現一個常態分佈，而平均值是 0，標準差則是 $\sqrt{N}$。</p><p>也就是當醉漢走愈多步，終究會回歸原點，但是也會有機率距離原點一段距離，而這段距離會隨著步數的增加而變長。</p><p>在隨機過程中，這是個很經典的問題。</p><p>既然談到了方均根，就不難聯想到高中物理中講到的方均根速度。</p><h2 id="氣體動力論"><a href="#氣體動力論" class="headerlink" title="氣體動力論"></a>氣體動力論</h2><p>在氣體動力論當中，我們可以去計算一個空間中的氣體分子運動速度，以方均根的形式表示</p><p>$$<br>v_{rms} = \sqrt{\frac{3kT}{m}}<br>$$</p><p>而這個氣體速度會呈現一個分佈情形，稱為馬克士威-波茲曼速率分佈。</p><h2 id="馬克士威-波茲曼速率分佈"><a href="#馬克士威-波茲曼速率分佈" class="headerlink" title="馬克士威-波茲曼速率分佈"></a>馬克士威-波茲曼速率分佈</h2><p>我們可以從維基百科上找到以下的速度分佈。</p><blockquote><p>Maxwell–Boltzmann velocity distribution</p></blockquote><p>$$<br>f(\nu_x, \nu_y, \nu_z) = (\frac{m}{2 \pi kT})^{3/2} exp \big[ - \frac{m(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2kT} \big]<br>$$</p><p>他描述了一個氣體分子在三維空間上有 $\nu_x, \nu_y, \nu_z$ 三種不同的速度分量，可以利用這些分量來計算出整體的速度分佈情形。</p><p>不覺得上式跟常態分佈有點相似嗎？</p><p>來呼叫一下常態分佈。</p><p>$$<br>f(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} exp \big[ - \frac{(x - \mu)^2}{2 \sigma^2} \big]<br>$$</p><p>那我們來動動手，做點簡單的驗證吧！</p><p>$$<br>f(\nu_x, \nu_y, \nu_z) = (\frac{m}{2 \pi kT})^{3/2} exp \big[ - \frac{m(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2kT} \big] \\<br>= (\frac{1}{\sqrt{2 \pi} \sqrt{\frac{kT}{m}} })^3 exp \big[ - \frac{(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2 (\sqrt{\frac{kT}{m}})^2 } \big]<br>$$</p><p>我們做點簡單的整理，然後將標準差抓出來。</p><p>let $\sigma = \sqrt{\frac{kT}{m}}$</p><p>代入之後就會成為</p><p>$$<br>= (\frac{1}{\sqrt{2 \pi} \sigma })^3 exp \big[ - \frac{(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2 \sigma^2 } \big]<br>$$</p><p>是不是變得更像了呢？那麼指數項中的速度平方和怎麼處理？</p><p>當然是把指數拆開囉！</p><p>$$<br>= \big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_x^2}{2 \sigma^2 } ] \big)<br>\big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_y^2}{2 \sigma^2 } ] \big)<br>\big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_z^2}{2 \sigma^2 } ] \big)<br>$$</p><p>我們會發現馬克士威-波茲曼速率分佈其實是三個常態分佈的乘積，或是多元常態分佈（Multivariate normal distribution）！</p><p>$$<br>= f_x \cdot f_y \cdot f_z<br>$$</p><p>三個常態分佈各是對映三維空間中的三個速度分量，也就是不同速度分量之間是各自獨立，不互相影響的。</p><p>跟真正的常態分佈的差異仍舊是有沒有將平均值減掉。</p><p>$$<br>\nu_x = v_x - \mu_x<br>$$</p><p>$\nu_x$：相對速度</p><p>$v_x$：絕對速度</p><p>或許我們可以這樣解釋，在這整個空間中，整個氣體是靜止不動的，所以他的整體平均速度是 0，而氣體的微觀速度 $\nu_x$ 就是他真正的速度。如果整體氣體是有一個速度在移動的，那麼你可以透過相對速度及平均速度來推得氣體的絕對速度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不久前跟好朋友聊天聊到 random walk 的問題，一個醉漢會在一維的空間上隨機往前或往後走一步。&lt;/p&gt;
&lt;p&gt;這樣的話，他最後會走到哪裡去呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Physics" scheme="https://yuehhua.github.io/categories/Physics/"/>
    
    
  </entry>
  
  <entry>
    <title>紀念 - 風超大的高美濕地</title>
    <link href="https://yuehhua.github.io/2018/12/15/ning/"/>
    <id>https://yuehhua.github.io/2018/12/15/ning/</id>
    <published>2018-12-15T06:26:27.000Z</published>
    <updated>2018-12-15T06:29:57.428Z</updated>
    
    <content type="html"><![CDATA[<p>很久沒有寫文章了。</p><p>一個陽光耀眼愜意的下午，舒適的溫度。</p><p>下午約了好朋友一起去逛逛，從綠園道一路逛到科博館。</p><a id="more"></a><p>一個可以談天說地的朋友，常常可以聊整個晚上到半夜的朋友。</p><p>沿途談論科學的精確性，測量的誤差永遠是有的，但取決於想要看到多麼精細的結果。</p><p>雖然不同的學科背景，數學是很有感觸跟交集的領域。</p><p>果不其然的走入了科博館的數學館，在第一個展區把玩裝置，期待肥皂泡可以形成四維的超正方體結構。</p><p>陸續看了不少展區，可以從解析幾何通往丘成桐先生的研究。談談物理，也聊聊數學。</p><p>數學是發現吧？朋友一臉驚訝的地看著我。我輕輕的搖頭，表示自然數也可以被定義，而邏輯與語言正是用來定義自然數的工具。</p><p>這些工具本身卻是人們「定義」出來的。那是發明或是發現呢？</p><p>時間差不多就前往高美濕地，愈往濱海就感受到風力的強勁，方向盤緊握著。</p><p>誤以為這幾天天氣晴朗，不過到了之後發現雲很厚，沒辦法看到夕陽。</p><p>兩個人坐在棧道上吃著零食，隨意的聊著，儘管冷風颼颼，有好朋友一起比較不顯得冷，不孤獨。</p><p>對於孤獨，朋友似乎習以為常，畢竟能一起走的實在是太少了。</p><p>陪伴，讓集結的力量可以更強大，也可以走得比較長遠。</p><p>我也在尋找可以一起往前的伙伴，共享知識，一起成長，共享喜悅。</p><p>覺得很開心，也很幸運能遇到這樣的好朋友。</p><p>之後的路還很長，可以的話就一起冒險吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很久沒有寫文章了。&lt;/p&gt;
&lt;p&gt;一個陽光耀眼愜意的下午，舒適的溫度。&lt;/p&gt;
&lt;p&gt;下午約了好朋友一起去逛逛，從綠園道一路逛到科博館。&lt;/p&gt;
    
    </summary>
    
      <category term="My Style" scheme="https://yuehhua.github.io/categories/My-Style/"/>
    
      <category term="Friends" scheme="https://yuehhua.github.io/categories/My-Style/Friends/"/>
    
    
  </entry>
  
  <entry>
    <title>31 Variational autoencoder</title>
    <link href="https://yuehhua.github.io/2018/11/14/31-variational-autoencoder/"/>
    <id>https://yuehhua.github.io/2018/11/14/31-variational-autoencoder/</id>
    <published>2018-11-14T08:34:28.000Z</published>
    <updated>2019-01-01T14:04:26.947Z</updated>
    
    <content type="html"><![CDATA[<p>在 autoencoder 的模型裏面，會希望以一個 unsupervised 方法來做到特徵萃取的目的。</p><p>你也可以說他是一種降維的方法或是有損壓縮的方法。</p><p>基本上就是透過一個線性轉換將原來的特徵，映射到比較低維度的特徵空間上。</p><p><img src="/images/autoencoder1.svg" alt=""></p><a id="more"></a><p>圖中就是一個基本的 autoencoder 的樣子，而中間的 hidden layer 就是我們希望的特徵萃取結果。</p><p>我們希望所萃取到的特徵，可以被 <strong>還原</strong> 成原本圖形的樣子。他就會是類似壓縮跟解壓縮的概念。</p><p><img src="/images/autoencoder2.svg" alt=""></p><p>如果我們想看看他會壓縮成什麼樣子，我們可以將中間的 hidden layer 換成兩個 node 就好，如此一來，我們就可以將他視覺化。</p><p><img src="/images/autoencoder3.svg" alt=""></p><h2 id="問題"><a href="#問題" class="headerlink" title="問題"></a>問題</h2><p>常常在訓練 autoencoder 之後，我們希望他的 decoder，也就是後半的部份，可以被拿出來作為一個獨立的 generative model 使用。例如，我可以把 MNIST 資料集當中的數字 1 的圖片輸入 encoder 會得到一個特徵向量 $\mathbf{z}$，相對，這個特徵向量 $\mathbf{z}$ 可以被放到 decoder 中還原回原本的數字 1 圖片，我們希望如果日後知道某個特徵向量 $\mathbf{z}$ 也可以用同樣的作法還原出他原本的樣子。</p><p><img src="/images/autoencoder3.svg" alt=""></p><p>但往往行不通，在 $\mathbf{z}$ 上有些許的差異，就有可能產生很奇怪的結果。原因在於訓練資料通過 encoder 之後所產生的特徵向量的（流形）空間分佈，只有在訓練資料相對應的特徵向量分佈的附近才能產生出好的結果，離這些特徵向量太遠的是沒辦法產生好的結果，所以模型沒有看到過的資料就無法產生好的結果。</p><p>Variational autoencoder（VAE） 就是為了解決這樣的問題引進了 evidence lower bound（ELOB）的方法，並且將他改進成可用在 gradient-based method 上的模型。接下來會以兩種觀點切入講解，先講比較直觀的觀點。</p><h2 id="直觀觀點"><a href="#直觀觀點" class="headerlink" title="直觀觀點"></a>直觀觀點</h2><p>如果不在點附近的區域就無法產生合理的結果，我們可以去找出這些點是不是會呈現什麼樣的分佈，並且去得到這些分佈的參數。</p><p>如果是使用機率分佈的話，就可以減少空間上有 <strong>洞</strong> 的情形發生，也就是比較不會有模型沒看過的資料的地方，導致產生的圖不合理的狀況。</p><p>在 VAE 中，假設特徵向量會呈現常態分佈，所以我們會去計算這個分佈的平均值 $\mu$ 與標準差 $\sigma$，每個 hidden layer 的 node 都有一個相對應的平均值與標準差向量。</p><p><img src="/images/VAE1.svg" alt=""></p><p>相似的特徵向量會在空間分佈上在比較相近的位置，不同的特徵向量就各自形成各自的分佈情形。舉例來說，數字 1 的特徵向量會在空間分佈上比較接近，所以會形成一個分佈，跟數字 2 的分佈是不同的。如此一來，就可以用機率分佈的方式去涵蓋一些沒有被模型看過的資料。</p><p><img src="/images/VAE2.svg" alt=""></p><p>但是這要怎麼接續後面的 decoder 呢？當我們知道分佈之後，我們可以從分佈中做抽樣阿！</p><p>既然是數字 1 的分佈，我就可以從這個分佈當中做抽樣，抽樣出來的向量應該要可以還原回原本的樣子。</p><p>我們就可以將 encoder 跟 decoder 一起做訓練了！</p><h2 id="機率圖模型觀點"><a href="#機率圖模型觀點" class="headerlink" title="機率圖模型觀點"></a>機率圖模型觀點</h2><p>接下來會以比較抽象的觀點來說明。</p><h3 id="隱含因素"><a href="#隱含因素" class="headerlink" title="隱含因素"></a>隱含因素</h3><p>我們的目標是想造出一個 generative model，這個 generative model 需要產出不同的結果（$\mathbf{x}$），例如 MNIST 數字，而且我們會給一個 input（$\mathbf{z}$）來決定要產生出什麼數字，這個 input 就是決定要產生出什麼數字的 <strong>因素</strong>，我們希望從模型自己去學出來。整體來說，他是一個 unsupervised 問題，我們只能從結果（$\mathbf{x}$）去做回推這個 generative model（$p(\mathbf{x}, \mathbf{z})$）的長相，並且試圖猜測 input（$\mathbf{z}$）。</p><p>作者從貝氏定理出發，我們手上的資料只有 $\mathbf{x}$，想去求 $\mathbf{z}$，我們會假設資料產生的過程是個隨機過程，他牽涉到一個無法觀察的變數 $\mathbf{z}$。</p><p>$$<br>p_{\theta}(\mathbf{z} | \mathbf{x}) = \frac{p_{\theta}(\mathbf{x}, \mathbf{z})}{p_{\theta}(\mathbf{x})} = \frac{p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z})}{p_{\theta}(\mathbf{x})}<br>$$</p><p>這個過程包含了兩個部份：</p><ol><li>$p(\mathbf{z})$，$\mathbf{z}$ 是怎麼被產生的？</li><li>$p(\mathbf{x} | \mathbf{z})$，$\mathbf{x}$ 是如何從 $\mathbf{z}$ 產生的？</li></ol><h3 id="難題"><a href="#難題" class="headerlink" title="難題"></a>難題</h3><p>一般來說，使用貝氏定理會遇到一個難題，也就是需要知道分母怎麼估算，其中需要對 $\mathbf{z}$ 積分，那麼就需要知道所有的 $\mathbf{z}$ 排列組合，但是這是不可能的。</p><p>$$<br>p_{\theta}(\mathbf{x}) = \int p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z}) d\mathbf{z}<br>$$</p><p>目前貝氏的方法是用抽樣的方法（Markov chain Monte Calro）去估 $p_{\theta}(\mathbf{x})$，避開了直接去積分他。而且這樣的方法也太慢了，所以作者希望用 SGD 的方法來取代抽樣的方法。</p><h3 id="Evidence-lower-bound"><a href="#Evidence-lower-bound" class="headerlink" title="Evidence lower bound"></a>Evidence lower bound</h3><p>在這邊作者使用了 evidence lower bound（ELOB）的方法，既然要估 $p_{\theta}(\mathbf{x})$ 很困難，那麼我們直接去估 $p_{\theta}(\mathbf{z} | \mathbf{x})$ 如何？</p><p>那要怎麼估 $p_{\theta}(\mathbf{z} | \mathbf{x})$ 呢？如果直接算的話就是回到上面的方法，所以 ELOB 方法假設了另一個類似的 $q_{\phi}(\mathbf{z} | \mathbf{x})$ 來逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$。</p><p>要逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$，也就是要求兩個機率分佈的 KL divergence（$D_{KL}[q_{\phi}(\mathbf{z} | \mathbf{x}) || p_{\theta}(\mathbf{z} | \mathbf{x})]$）愈小愈好。</p><h3 id="架構"><a href="#架構" class="headerlink" title="架構"></a>架構</h3><p><img src="/images/VAE3.svg" alt=""></p><p>原本的問題就從左圖變成右圖，也就是以 $q_{\phi}(\mathbf{z} | \mathbf{x})$ 來逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$。$q_{\phi}(\mathbf{z} | \mathbf{x})$ 就是虛線箭頭的部份。</p><p><img src="/images/VAE4.svg" alt=""></p><p>我們再把右圖當中的兩個箭頭拆開，$q_{\phi}(\mathbf{z} | \mathbf{x})$ 就是 encoder，而在</p><p>$$<br>p_{\theta}(\mathbf{z} | \mathbf{x}) = \frac{p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z})}{p_{\theta}(\mathbf{x})}<br>$$</p><p>當中的 $p_{\theta}(\mathbf{x} | \mathbf{z})$ 就是 decoder 的部份。Encoder 對應到實際神經網路模型中，則是一個 $\mathbf{z} = f(\mathbf{x})$ 函數，decoder 對應的是另一個函數 $\mathbf{x} = g(\mathbf{z})$。</p><p><img src="/images/VAE5.svg" alt=""></p><p>我們可以把他轉換成這個樣子，這樣就形成了 autoencoder 的架構雛型了。</p><h3 id="抽樣"><a href="#抽樣" class="headerlink" title="抽樣"></a>抽樣</h3><p>回到前面的老問題，我們用一些資料通過 encoder 之後可以得到被壓縮的資料，而我們去估計出這些資料的機率分佈，我們要如何從這些機率分佈繼續往下計算呢？</p><p>是的！就是抽樣！我們會從這些機率分佈當中重新做抽樣，把他作為 decoder 的輸入，並且繼續做訓練。</p><p>但是在模型中間卡一個抽樣的動作，這個動作引進了機率這個不確定因素，這樣要怎麼做 gradient descent 呢？</p><h3 id="Reparametrization-trick"><a href="#Reparametrization-trick" class="headerlink" title="Reparametrization trick"></a>Reparametrization trick</h3><p><img src="/images/1606.05908.svg" alt=""></p><blockquote><p>圖片來自 <a href="https://arxiv.org/abs/1606.05908" target="_blank" rel="noopener">Tutorial on Variational Autoencoders</a></p></blockquote><p>左圖就是原本的抽樣的模型。要讓模型可以進行 gradient descent，這時候作者做了重新參數化的技巧，這樣的技巧就如同將抽樣的動作抽離出來，就如同右圖那樣。</p><p>重新參數化技巧是從標準常態分佈當中去隨機抽樣，抽樣的結果將他乘上標準差，並且加上平均值，這樣就可以模擬從隱藏層的分佈抽樣的動作。但是這並非重新參數化技巧的巧妙之處，巧妙之處是在於重新參數化之後，就可以將抽樣這個動作從反向傳遞（backpropagation）的路徑上移除，這樣子我們就可以輕鬆的做訓練了！</p><p>將抽樣的動作從反向傳遞的路徑上移除，你可以將這樣的抽樣動作看成資料的一部份，也就是原本的資料上會多加一筆從標準常態分佈的抽樣數值。而資料不也是抽樣而來的嗎？其實兩者的概念是等價的。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>這個模型結合了 Variational inference 的技巧，並且以 reparametrization trick 來讓模型可以用 gradient descent。這是非常重大的突破，他也開啟了生成型模型的一條路。真的非常推荐大家讀讀這個模型！不過原論文有點難懂就是了…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 autoencoder 的模型裏面，會希望以一個 unsupervised 方法來做到特徵萃取的目的。&lt;/p&gt;
&lt;p&gt;你也可以說他是一種降維的方法或是有損壓縮的方法。&lt;/p&gt;
&lt;p&gt;基本上就是透過一個線性轉換將原來的特徵，映射到比較低維度的特徵空間上。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/autoencoder1.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>30 結語</title>
    <link href="https://yuehhua.github.io/2018/10/31/30-conclusions/"/>
    <id>https://yuehhua.github.io/2018/10/31/30-conclusions/</id>
    <published>2018-10-30T16:14:33.000Z</published>
    <updated>2018-10-30T16:14:33.301Z</updated>
    
    <content type="html"><![CDATA[<p>原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。</p><p>我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。</p><p>這次的系列文章是在基礎之上，讓大家得以理解不同模型之間的來龍去脈以及變化性，這樣才有辦法更進一步進展到深度學習的領域。</p><p>完成這次鐵人賽的意義在於讓大家理解模型的來龍去脈，以及為什麼要用什麼樣的數學元件去兜一個模型。</p><p>當你遇到問題的時候，不可能會有一個 ready-to-use 的模型等在那邊給你用，對於人工智慧的技術應用，你必須要為自己所面對的問題和狀況自己去量身訂作自己的模型。</p><p>是的！你沒看錯，必須要由領域專家去理解自己要的是什麼，然後自己做出專門給這個情境的模型。</p><p>當你的情境非常特殊的時候，讓深度學習專家來深入其他知識領域是非常花時間的。如果以領域專家及深度學習專家之間以合作模式進行，那將會花費更高的成本在溝通上，因為人工智慧的技術應用需要對特定領域非常敏感。我個人認為只有領域專家繼續鑽研成為深度學習專家才有辦法徹底解決特定領域的問題。當然這條路非常的漫長，等於是需要一個特定領域的博士，以及深度學習的博士的等級。這些問題，只有當領域專家自己 <strong>理解</strong> 之後，不是只有 <strong>解決</strong> 問題，才能夠算是真正的 <strong>解決</strong> 了。</p><p>這系列文獻給擁有機器學習基礎，想繼續晉升到深度學習領域的朋友們。</p><p>感謝大家的支持！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。&lt;/p&gt;
&lt;p&gt;我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。&lt;/p&gt;
&lt;p&gt;這次的系列文章是在基礎之上，讓大家得以理解不同模型之間
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>29 Autoregressive generative model</title>
    <link href="https://yuehhua.github.io/2018/10/29/29-autoregressive-generative-model/"/>
    <id>https://yuehhua.github.io/2018/10/29/29-autoregressive-generative-model/</id>
    <published>2018-10-29T15:07:42.000Z</published>
    <updated>2018-10-31T07:44:57.360Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。</p><p>在 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a> 這篇文章以及他的論文當中又在重述了這件事。</p><p>他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？</p><a id="more"></a><p>答案幾乎是肯定的。為什麼說幾乎是肯定的呢？因為他需要滿足一些條件，才能達成訓練上模型的穩定性要求。</p><h2 id="Stable-recurrent-models"><a href="#Stable-recurrent-models" class="headerlink" title="Stable recurrent models"></a>Stable recurrent models</h2><p>問題描述是這樣的：</p><blockquote><p>理論上來說，一個有 <strong>好行為（well-behaved）</strong> 的 recurrent neural network 是否總是可以被差不多大小的 feed-forward network 取代，在不損失效能的情況下？</p></blockquote><p>這時候我們就需要知道什麼樣的 RNN 是有好行為（well-behaved）的？</p><p>當然你可以設計一個非線性的 RNN 讓 feed-forward network 無法取代，只要讓他無法用 gradient descent 訓練起來就可以了。</p><p>也就是說，<strong>好行為</strong> 的 RNN 就是，有辦法用 gradient descent 訓練起來，而不會讓梯度爆炸或是消失的模型。這樣穩定（stable）的模型就有辦法用 feed-forward network 去逼近。</p><p>論文中證明了一個重要的定理（論文中有正式的版本），我先寫他的原始描述，然後解釋：</p><blockquote><p>Thm.</p></blockquote><p>Assume the system $\phi$ is <em>$\lambda$-contractive</em>. Under <em>additional smoothness</em> and <em>Lipschitz assumptions</em> on the system $\phi$, the prediction function $f$, and the loss $p$, if</p><p>$$<br>k \ge O(log(N^{1/(1-\lambda)^3} / (\epsilon (1-\lambda)^2)))<br>$$</p><p>then after N steps of projected gradient descent with decaying step size $\alpha_t = O(1/t)$, $||w_{recurr} - w_{trunc}|| \le \epsilon$, which in turn implies $||y_t(w_{recurr}) - y_t^k(w_{trunc})|| \le O(\epsilon)$.</p><p>當你把以上定理認真看完之後你就會昏了。基本上是說，一個模型本身會要滿足幾個條件：</p><ol><li>$\lambda$-contractive</li><li>additional smoothness</li><li>Lipschitz assumptions</li></ol><p>這幾個條件簡單來說，就是你的 loss function 需要是平滑的，那麼你的梯度就不會起伏太大，導致梯度爆炸或是消失的狀況。在這樣的狀況下，就可以用 feed-forward network 去逼近。</p><h2 id="Feed-forward-network-逼近"><a href="#Feed-forward-network-逼近" class="headerlink" title="Feed-forward network 逼近"></a>Feed-forward network 逼近</h2><p><img src="/images/wavenet.gif" alt=""></p><blockquote><p>動畫取自 <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">WaveNet 官網</a></p></blockquote><p>那用 feed-forward network 去逼近有什麼好處呢？</p><p>文章中提到三大好處：</p><ol><li>平行化：你可以善用 GPU 加速</li><li>可訓練：以往 RNN 都不好訓練，但是如果可以換成 feed-forward network 就會比較容易訓練起來</li><li>推論速度：在速度上更快</li></ol><p>近年來非常多的模型都採用了 auto-regressive 的架構，從前面提到的 Transformer，到新版的 Google 小姐 - WaveNet 都用了這樣的架構。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。&lt;/p&gt;
&lt;p&gt;在 &lt;a href=&quot;http://www.offconvex.org/2018/07/27/approximating-recurrent/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;When Recurrent Models Don’t Need to be Recurrent&lt;/a&gt; 這篇文章以及他的論文當中又在重述了這件事。&lt;/p&gt;
&lt;p&gt;他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>28 Transformer</title>
    <link href="https://yuehhua.github.io/2018/10/28/28-transformer/"/>
    <id>https://yuehhua.github.io/2018/10/28/28-transformer/</id>
    <published>2018-10-28T14:58:09.000Z</published>
    <updated>2018-10-31T07:44:47.883Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。</p><p>這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。</p><p>Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。</p><p>（跟變形金剛一樣的名字耶！帥吧！）</p><a id="more"></a><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>這個架構上延續了 encoder-decoder 的架構，encoder 會將輸入的序列 $(x_1, x_2, …, x_n)$ 轉換成 $\mathbf{z} = (z_1, z_2, …, z_n)$，而 decoder 會將 $\mathbf{z}$ 轉換成 $(y_1, y_2, …, y_m)$，一次轉換一個。在每一步當中，模型都是 auto-regressive 的，也就是說，前一次產生的結果會被當成下一次的輸入進行運算。</p><p>整個 Transformer 的架構就是在 encoder 及 decoder 上使用了 stacked self-attention 以及全連接的網路。我們來看圖，在圖的左半邊就是 encoder，右半邊就是 decoder：</p><p><img src="/images/transformer1.svg" alt=""></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder 由 N 個完全一樣的層堆疊（stack）起來（$N = 6$）。每層包含兩個子層，第一個是一個 multi-head self-attention 的機制，第二個是簡單的全連接層網路。每個子層外都包了 residual connection 以及 layer normalization，看起來就像 $LayerNorm(x + Sublayer(x))$。</p><p>Residual connection 主要可以將前層的資訊繞過一層，直接與下一層做運算。Layer normalization 有穩定學習效果的作用。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder 大致上與 encoder 相同，差別是在用了兩個 multi-head self-attention 的機制，所以總共有3個子層。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>在 attention 的機制上，我們在上一篇講過了。這邊我們要進一步探討這個模型用到的 scaled dot product attention。在這邊就是分成 query、key 跟 value 三者，首先要先將 query 跟所有 key 做過一次的內積，並且除以 $\sqrt{d_k}$，然後過一次 softmax 函數。計算到這邊就是權重的部份，最後權重再跟 value 去計算結果。其中 $d_k$ 是 key 向量的維度。公式在這邊：</p><p>$$<br>Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><p><img src="/images/transformer2.svg" alt=""></p><p>圖的左邊是 scaled dot product attention。為什麼要除以 $\sqrt{d_k}$ 呢？文章中有提到，內積會讓整個結果變很大，會讓梯度變得極小，這會不利於訓練，所以需要除以 $\sqrt{d_k}$。</p><p>在圖的右邊，是 multi-head self-attention，核心就是平行計算多個 scaled dot product attention 的結果，並把這些結果都串接起來。有了這樣的機制就可以不只注意一個地方，可以有多個關注點。</p><p>在 self-attention 的機制，意味著所有的 query、key 跟 value 都來自於自己。不像之前的 attention 橫跨 encoder 跟 decoder，所以資訊會從雙方而來。</p><p>在 Transformer 模型當中，有一個是 encoder-decoder attention layer，然後 encoder 跟 decoder 各有一個 self-attention layer，就是各自的第一個子層。</p><p>如此構成了整個 Transformer 模型，如果各位想知道這個模型的應用跟效能的話，請移駕去看論文，論文寫的還蠻簡單易懂的。</p><p>當然這麼模型當中有不少巧思在裡頭，有需要說明的話就提問囉！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。&lt;/p&gt;
&lt;p&gt;這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。&lt;/p&gt;
&lt;p&gt;Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。&lt;/p&gt;
&lt;p&gt;（跟變形金剛一樣的名字耶！帥吧！）&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>27 Attention model</title>
    <link href="https://yuehhua.github.io/2018/10/27/27-attention-model/"/>
    <id>https://yuehhua.github.io/2018/10/27/27-attention-model/</id>
    <published>2018-10-27T15:10:46.000Z</published>
    <updated>2018-10-31T07:44:40.087Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。</p><p>Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。</p><p>Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。</p><a id="more"></a><p>有幾篇論文算是開始用這樣的機制：</p><ol><li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li><li><a href="https://arxiv.org/abs/1509.00685" target="_blank" rel="noopener">A Neural Attention Model for Abstractive Sentence Summarization</a></li><li><a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noopener">Reasoning about Entailment with Neural Attention</a></li></ol><p>Attention model 在很多模型當中都是做為 encoder-decoder 之間的橋樑，原本的 encoder 跟 decoder 之間是只有一個 vector 來傳遞所有的訊息，但是多了 attention mechanism 就不一樣了。</p><p><img src="/images/seq2seq2.svg" alt=""></p><p>Attention mechanism 主要可以動態的去抓到 encoder 中傳遞的訊息，並且將這些訊息與 decoder 輸出的前一個訊息互相比對之後，透過線性組合之後輸出。這樣的輸出有什麼效果呢？他可以動態地去找到兩邊最相符的資訊，並且將他重要的部份以權重的方式凸顯出來，所以這部份是做線性組合。</p><p><img src="/images/attention.svg" alt=""></p><p>我看到一個廣義的描述方法，他是這樣說的，我們可以把 attention model 想成是一個函數，這個函數會吃兩種東西，一種是 query，另一種是 key-value 的資料結構，讓 query 去比對所有的 key 找到吻合的，會透過一個 compatibility function 去計算吻合的程度，並且作為權重，最後將權重與相對應的 value 做內積。在這邊 query、key、value 三者都是向量。</p><p>在這邊 query 會是 decoder 的 $z_0$，而 key 就是 encoder 的 $h^1, h^2, …$。每一個 key 都會去跟 query 個別通過 compatibility function 算一次吻合程度 $\alpha_0^1, \alpha_0^2, …$。這些 $\alpha_0^1, \alpha_0^2, …$ 就是權重，會去跟 value $h^1, h^2, …$ 做線性組合。在這邊為了簡單所以讓 value 跟 key 是一樣的，其實可以是不同的東西。計算出來的結果 $c^1$ 就是 context vector，會作為 decoder 的輸入，與 $z_0$ 一起計算出 $z_1$。</p><p>這樣就算是完成一輪 attention mechanism 了。下一次再繼續用 $z_1$ 當成 query 進行比對。</p><p>如此一來，就可以以動態的方式去產生序列了。Encoder 負責的是將輸入的序列轉成固定大小的向量，decoder 將這樣的向量轉換回序列，而中間需要動態調整的部份就像人的注意力一樣，會去掃視跟比對哪個部份的翻譯是最吻合的，然後將他做一個線性組合的調整。</p><p>今天的解析就到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。&lt;/p&gt;
&lt;p&gt;Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。&lt;/p&gt;
&lt;p&gt;Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>26 seq2seq model</title>
    <link href="https://yuehhua.github.io/2018/10/26/26-sequence-to-sequence-model/"/>
    <id>https://yuehhua.github.io/2018/10/26/26-sequence-to-sequence-model/</id>
    <published>2018-10-26T15:40:23.000Z</published>
    <updated>2018-10-27T03:40:37.108Z</updated>
    
    <content type="html"><![CDATA[<p>前面有提到 seq2seq model，我們就從這邊開始。</p><p>Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！</p><p><img src="/images/seq2seq.svg" alt=""></p><a id="more"></a><p>在以往的 language model 的作法裡，會是把一個 word 塞進 RNN 裡，那麼 RNN 就會立刻吐出一個相對應的 word 出來。</p><p>像是放進一個英文字，會吐出一個相對應的法文字，然後將這一層的預測結果帶給下一層。</p><p>這麼做雖然很直覺，但是他並不能完整的翻譯一個句子。</p><p>語言的語法各不相同，所以很難將詞語一一對映做成翻譯。</p><p>這個 seq2seq model 採用了不同的作法，將一組 LSTM 作為 encoder，負責將要翻譯的句子轉換成固定長度的向量，再將這個向量交給另一個 LSTM 轉換成目標句子，後面這個 LSTM 就是 decoder 的角色。</p><p>這樣的架構之下將一個模型拆成 encoder 跟 decoder 的兩個部份，讓兩個部份都可以各自接受或是產生不同長度的句子，並且得到很好的分數。</p><p>在實作上，的確是將兩個 LSTM 接起來，所以就沒什麼細節好講的。</p><p>但是這個模型確實的解決了以不同長度的句子產生不同長度的句子的問題。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面有提到 seq2seq model，我們就從這邊開始。&lt;/p&gt;
&lt;p&gt;Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/seq2seq.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>25 Recurrent model 之死</title>
    <link href="https://yuehhua.github.io/2018/10/25/25-death-of-recurrent-model/"/>
    <id>https://yuehhua.github.io/2018/10/25/25-death-of-recurrent-model/</id>
    <published>2018-10-25T15:21:36.000Z</published>
    <updated>2018-10-25T15:21:36.182Z</updated>
    
    <content type="html"><![CDATA[<p>當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。</p><p>不要再用 RNN 為基礎的模型了！！</p><p>就是這篇 <a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" target="_blank" rel="noopener">The fall of RNN / LSTM</a></p><p>為什麼呢？</p><p>基本上裏面提到 vanishing gradient 的問題一直沒有解決以外，還有沒有辦法善用硬體的侷限在。</p><p>像這種循序型的模型，模型天生無法平行化運算，所以 GPU 就無用武之地，只能靠 CPU 慢慢跑。</p><p>那有什麼解決辦法呢？</p><h2 id="Self-attention-model"><a href="#Self-attention-model" class="headerlink" title="Self-attention model"></a>Self-attention model</h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> 這篇文章提出了 Transformer 這個模型，基本上這個模型使用了 self-attention 的機制。</p><p>要講這個之前我們要先聊聊 attention model。在 attention model 之前，sequence-to-sequence model 做出了重大的突破。一個具有彈性，可以任意組合的模型誕生了，管你是要生成句子還是怎麼樣。原本是只有 RNN 一個單元一個單元慢慢去對映 X 到 Y，sequence-to-sequence model 將這樣的對應關係解耦，由一個 encoder 負責將 X 的資訊萃取出來，再經由 decoder 將資訊轉換成 Y 輸出。</p><p>但是 LSTM 還是沒辦法記憶夠長的，後來 attention model 就誕生了。乾脆就將 encoder 所萃取到的資訊紀錄下來，變成一個，然後再丟到 decoder 去將資訊還原成目標語言，就可以完成機器翻譯了。</p><p>但是這種方式還是不脫 recurrent model，那就乾脆做成 self-attention 的機制，也就是這邊的 Transformer，完全摒棄了 recurrent 的限制。</p><h2 id="Autoregressive-generative-model"><a href="#Autoregressive-generative-model" class="headerlink" title="Autoregressive generative model"></a>Autoregressive generative model</h2><p>接著是今年6月的文章 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a>，當你的 recurrent model 不必再 recurrent！</p><p>也就是將 RNN 的問題又重述了一遍，並且提出大家都漸漸以 autoregressive generative model 來解決這樣的問題。</p><p>這篇算這引言，我接下來會開始一一解釋模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。&lt;/p&gt;
&lt;p&gt;不要再用 RNN 為基礎的模型了！！&lt;/p&gt;
&lt;p&gt;就是這篇 &lt;a href=&quot;https://towardsdatascience.com/the-fall-of-rnn-lstm-
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>24 Recurrent neural network</title>
    <link href="https://yuehhua.github.io/2018/10/23/24-recurrent-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/23/24-recurrent-neural-network/</id>
    <published>2018-10-23T13:43:49.000Z</published>
    <updated>2018-10-23T13:43:49.912Z</updated>
    
    <content type="html"><![CDATA[<p>接續上一篇。</p><h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p><ul><li>狀態都是 <strong>連續</strong> 的。</li><li>時間是離散的。</li><li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li><li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li><li><strong>允許在每個時間點給輸入</strong></li><li><strong>引入非線性</strong></li></ul><p>首先，在這邊的狀態會以一個向量做表示，大家應該也知道 RNN 的 input 是一個向量，當中的狀態也是一個向量，最後的 output 也是一個向量。而這些向量當中的的值都是連續的 $\mathbb{R}^n$（假設向量大小為 n），不像上面的模型都是離散的 $k$（假設有 k 個狀態），所以在空間上的大小可以說是擴大非常多。</p><p>接下來我們來看看時間的狀態轉換：</p><p><br></p><p><img src="/images/rnn_time.svg" alt=""></p><p><br></p><p><img src="/images/rnn_expand_time.svg" alt=""></p><p><br></p><p>在 RNN 中一樣含有內在狀態，但不同的是 RNN 可以在每個時間點上給輸入向量（$\mathbf{x^{(t)}}$），所以可以根據前一個時間點的內在狀態（$\mathbf{h^{(t)}}$）跟輸入向量去計算輸出，或是外在狀態（$\mathbf{y^{(t)}}$）。</p><p>所以大家會在一些論文上看到模型的狀態關係式長下面這個樣子：</p><p>$$<br>\mathbf{h^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}}) = \mathbf{x^{(t)}} W_x + \mathbf{h^{(t-1)}} W_h + \mathbf{b}<br>$$</p><p>$$<br>\mathbf{y^{(t)}} = g(\mathbf{h^{(t)}}) = sigm(\mathbf{h^{(t)}} W_y)<br>$$</p><p>這邊特別引入了非線性的轉換（$sigm$）來讓模型更強大。</p><p>隨著從一開始的馬可夫模型到這邊應該對這幾個模型有點感覺，其實 RNN 可以說是很大的突破，在假設上放了很多元素讓模型變得更強大。</p><h2 id="Long-short-term-memory"><a href="#Long-short-term-memory" class="headerlink" title="Long short-term memory"></a>Long short-term memory</h2><p>人們為了改進 RNN這個模型的記憶性，希望他可以記住更遠以前的東西，所以設計了 LSTM 來替換他的 hidden layer 的運作模式，後期更有 GRU，還有人說只需要 forget gate 就有很強大的效能的 MGU。這些都是對於記憶性做的改進，個人覺得這些在工程上的貢獻比較大，真正學術上的突破其實還好。</p><p>今天的整理就先到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;接續上一篇。&lt;/p&gt;
&lt;h2 id=&quot;Recurrent-neural-network&quot;&gt;&lt;a href=&quot;#Recurrent-neural-network&quot; class=&quot;headerlink&quot; title=&quot;Recurrent neural network&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>23 Markov chain 及 HMM</title>
    <link href="https://yuehhua.github.io/2018/10/23/23-markov-chain-and-hmm/"/>
    <id>https://yuehhua.github.io/2018/10/23/23-markov-chain-and-hmm/</id>
    <published>2018-10-23T01:21:38.000Z</published>
    <updated>2018-10-23T01:21:38.594Z</updated>
    
    <content type="html"><![CDATA[<p>上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。</p><p>這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。</p><p>在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。</p><a id="more"></a><h2 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h2><p>馬可夫模型是這樣的，他假設一個變數有不同種的狀態，例如下圖：</p><p><br></p><p><img src="/images/markov_model.svg" alt=""></p><p><br></p><p>在這邊有4個狀態，一個圓圈代表一個狀態，狀態跟狀態之間會隨著時間改變，每個狀態會有一定機率變成其他狀態，或是維持原本的狀態不變。</p><p>我們可以把目前的狀態用一個向量來表達：</p><p>$$<br>\mathbf{y} =<br>\begin{bmatrix}<br>y_1&amp;y_2&amp;y_3&amp;y_4 \\<br>\end{bmatrix}<br>$$</p><blockquote><p>注意：我們這邊使用的向量為列向量（row vector），一般在其他數學領域使用的則是行向量（column vector），兩者是可以藉由轉置來互換的，並不影響運算結果</p></blockquote><p>我們這邊用 $\mathbf{y}$ 代表他是可以被觀察到的。狀態變化我們可以用一個矩陣來表達：</p><p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p><p>其中 $a_{ij}$ 代表的是由狀態 $i$ 變成狀態 $j$ 的機率，這邊要注意的是，每一個列（row）的機率總和要是 1。</p><blockquote><p>補充：<br>由於我們的向量都是 row vector<br>$A$ 為一 right stochastic matrix，運算形式為 $\mathbf{y} A$<br>由狀態 $i$ 變成其他狀態之機率和為 1（$A \mathbf{1} = \mathbf{1}$）<br>感謝陳杰翰先生幫忙指正</p></blockquote><p>所以不同時間點的狀態變化關係可以寫成以下式子：</p><p>$$<br>\mathbf{y^{(t)}} = \mathbf{y^{(t-1)}} A<br>$$</p><p>$\mathbf{y^{(t)}}$ 的意思是第 t 次（或是時間為 t）的狀態，$\mathbf{y^{(t-1)}}$ 狀態會經過一次轉換或是運算轉變成 $\mathbf{y^{(t)}}$。</p><p>如果你把其中的第一項的運算拆開來看就會長這樣，可以自行檢驗狀態的變化：</p><p>$$<br>y_1^{(t)} = a_{11}y_1^{(t-1)} + a_{12}y_2^{(t-1)} + a_{13}y_3^{(t-1)} + a_{14}y_4^{(t-1)}<br>$$</p><p>從時間軸上來看，我們可以把狀態的轉變畫出來像是這樣：</p><p><br></p><p><img src="/images/markov_model_expand_time.svg" alt=""></p><p><br></p><p>每次的轉變我們都可以看成一個函數 $f$，他其實等同於上面提到的矩陣：</p><p>$$<br>\mathbf{y^{(t)}} = f(\mathbf{y^{(t-1)}}) = \mathbf{y^{(t-1)}} A<br>$$</p><p>所以他的意思是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$，所以這是單純的狀態變化。</p><p>上面的矩陣當中其實內容是機率，我們也可以把他轉成機率的寫法，但是解釋會變得不太一樣：</p><p>$$<br>p = f(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p><p>這邊的解釋是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$ <strong>的機率</strong>。</p><p>下句跟上句的不同在於，上句的描述是肯定的，他只描述了狀態的改變，但是下句多加描述了 <strong>這件事會發生的機率</strong>，所以應該要把 $\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}$ 理解成 <strong>這一件事</strong>，那麼 $f$ 的輸出就是機率了。</p><p>我們可以把上圖 <em>收起來</em>，所以看起來會像這樣：</p><p><br></p><p><img src="/images/markov_model_time.svg" alt=""></p><p><br></p><p>花了點時間把一些符號跟數學概念講完了，來談談他的假設，一般來說，馬可夫模型最大的假設在於：</p><p>$$<br>P(\mathbf{y^{(t)}} \mid \mathbf{y^{(1)}}, \mathbf{y^{(2)}}, \dots, \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p><p>也就是要預測第 t 單位時間的狀態，我們經歷了第 1~(t - 1) 單位時間，但是他只需要用前一個時間單位的狀態就可以預測下一個狀態，前面很多狀態都是不必要的，這我們稱為一階馬可夫模型（first-order Markov chain）。</p><p>當然可以推廣到 m 階馬可夫模型（m th-order Markov chain），那代表需要前 m 個狀態來預測下一個狀態，順帶一提，有零階馬可夫模型，那就跟我們一般的機率分佈模型（$P(\mathbf{y^{(t)}}）$）一樣。</p><p>沒有特別提的話，通常大家談的馬可夫模型都是一階馬可夫模型。一般來說，他有個非常重要的特性，就是 <strong>無記憶性</strong>，也就是他不會去記住他所經歷的狀態，他只需要用現在的狀態就可以預測下一個狀態。</p><p>不過我要特別提一下這個模型的一些其他假設：</p><ul><li>狀態是離散的。在馬可夫模型的狀態空間中是離散的，也就是你可以用一個正整數來數出有幾種狀態存在。</li><li>時間是離散的。我們剛剛有看到他計算的是第 t 單位時間，下一次就是乘上一個矩陣之後成為第 t+1 單位時間。</li><li>狀態是可被觀察的。</li><li>以一個隨機變數作為一個狀態。</li></ul><p>接下來我們來談談另一個模型。</p><p><br></p><h2 id="Hidden-Markov-model"><a href="#Hidden-Markov-model" class="headerlink" title="Hidden Markov model"></a>Hidden Markov model</h2><p>接下來是進階版的隱馬可夫模型（hidden Markov model），他的假設是這樣的，在一個系統中存在一些我們看不到的狀態，是系統的內在狀態，隨著系統的內在狀態不同，他所表現出來的外在狀態也不同，而外在狀態是我們可以觀測到的。</p><p><br></p><p><img src="/images/hmm.svg" alt=""></p><p><br></p><p>大家可以看到這個圖跟剛剛的很相似，帶是又多了一些東西。較大的圈圈是內在狀態，小的圈圈是外在狀態。隨著時間改變，內在狀態會隨著變動，內在狀態的變動我們可以用一個矩陣來表示：</p><p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p><p>裏面裝的一樣是機率。接下來，不同的內在狀態有不同的機率會噴出（emit）外在狀態，這也會用另一個矩陣表示：</p><p>$$<br>B = [b_{ij}] =<br>\begin{bmatrix}<br>b_{11}&amp; b_{12}&amp; b_{13}&amp; b_{14} \\<br>b_{21}&amp; b_{22}&amp; b_{23}&amp; b_{24} \\<br>b_{31}&amp; b_{32}&amp; b_{33}&amp; b_{34} \\<br>\end{bmatrix}<br>$$</p><p>寫成狀態轉移的關係式的話會變成：</p><p>$$<br>\mathbf{h^{(t)}} = \mathbf{h^{(t-1)}} A<br>$$</p><p>$\mathbf{h^{(t)}}$ 代表在第 t 單位時間的內在狀態。</p><p>$$<br>\mathbf{y^{(t)}} = \mathbf{h^{(t)}} B<br>$$</p><p>$\mathbf{y^{(t)}}$ 代表在第 t 單位時間根據內在狀態噴出的外在狀態。</p><p>如果在時間軸上表達的話是這個樣子：</p><p><br></p><p><img src="/images/hmm_expand_time.svg" alt=""></p><p><br></p><p><img src="/images/hmm_time.svg" alt=""></p><p><br></p><p>由於在這邊又多了一個內在狀態，所以在模型的表達力上遠遠超越馬可夫模型。舉個例子好了，假設小明很好奇在不同天氣的時候外面的人吃冰淇淋的狀況是如何，但是小明又很懶得出門看天氣，這時候他就假設天氣（晴天、陰天、雨天）是內在狀態（看不到），然後他觀察路上的人吃冰淇淋（外在狀態，吃、不吃）的多寡，這時候這麼模型就可以派上用場，他藉由持續觀察路人有沒有吃冰淇淋，可以推論外面天氣的變化狀況。</p><p>這時候我們也來總結一下，這個模型的假設：</p><ul><li>內在狀態跟外在狀態都是離散的。</li><li>時間是離散的。</li><li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li><li>以一個隨機變數作為一個狀態。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。&lt;/p&gt;
&lt;p&gt;這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。&lt;/p&gt;
&lt;p&gt;在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>22 Convolutional encoder-decoder 架構</title>
    <link href="https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/"/>
    <id>https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/</id>
    <published>2018-10-22T15:10:35.000Z</published>
    <updated>2018-10-23T00:53:57.844Z</updated>
    
    <content type="html"><![CDATA[<p>標題這不是一個專有名詞。</p><p>在電腦視覺的領域中有幾個有名的問題：</p><ol><li>影像辨識（Image recognition）</li><li>物件辨識（Object detection）</li><li>語意分割（Semantic segmentation）</li></ol><a id="more"></a><p><img src="/images/cv-tasks.jpg" alt=""></p><p>影像辨識是給一張影像，希望模型可以辨識出當中的東西是什麼。輸入模型的會是影像向量，輸出的會是類別向量。</p><p>物件辨識給的同樣是一張影像，除了需要辨識出當中的物件以外，還要給出這個物體所在的位置，輸出的除了類別向量以外，還有座標。</p><p>語意分割可能無法一眼看出當中的含意，對一個句子來說，詞本身帶有一些含意，對比到影像上，句子就是影像，而詞意就是影像中的物體。語意分割是給一張影像，需要將影像中的物件切割出來，所以必須對每個像素做分類。</p><p><img src="/images/semantic_seg.jpeg" alt=""></p><p>三者是各自不同的任務。</p><p>不同的任務有些共通性，這些共通性讓他們可能都可以適用 CNN 的架構。不過這麼說還是太過粗糙了。</p><p>對於影像辨識來說，一般性架構會是有 convolution layer 為主的 feature extractor，接著會是以 fully-connected layer 為主的 classifier。在不同階段有不同的目的，在輸入影像之後要先對影像進一步抽取特徵，有了足夠的特徵之後才進行分類。</p><p>語意分割也有類似的架構，在前面會有 convolution layer 為主的 feature extractor，但是為了將每個樣素做分類，必須對每一個像素做預測，預測像素的類別。在後半的部份，有人提出了 Fully convolution network，試圖做像素的類別預測。</p><p>像素的類別預測這件事從另一個角度切入，會很像是一種生成的過程。也就是，我們在前面要將影像的特徵萃取出來，是一種將資訊壓縮的過程，在後半我們希望將壓縮的資訊還原到某種程度，我們需要產生器（generator）。</p><p><img src="/images/segnet.svg" alt=""></p><blockquote><p><a href="http://mi.eng.cam.ac.uk/projects/segnet/" target="_blank" rel="noopener">SegNet [2015, University of Cambridge]</a></p></blockquote><p>在語意分割這個問題，後來就一路發展到了 encoder-decoder 架構，我們又回到類似 autoencoder 的樣子，讓 encoder 跟 decoder 一起訓練的模型架構。在這邊 encoder 就是 feature extractor，decoder 就是一種 generator。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;標題這不是一個專有名詞。&lt;/p&gt;
&lt;p&gt;在電腦視覺的領域中有幾個有名的問題：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;影像辨識（Image recognition）&lt;/li&gt;
&lt;li&gt;物件辨識（Object detection）&lt;/li&gt;
&lt;li&gt;語意分割（Semantic segmentation）&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>21 Activation functions and ReLU</title>
    <link href="https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/"/>
    <id>https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/</id>
    <published>2018-10-19T14:34:55.000Z</published>
    <updated>2018-10-19T14:34:55.385Z</updated>
    
    <content type="html"><![CDATA[<p>今天我們來談談 activation function 吧！</p><h2 id="先談談線性轉換"><a href="#先談談線性轉換" class="headerlink" title="先談談線性轉換"></a>先談談線性轉換</h2><p>談 activation function 之前先要談談線性轉換。</p><p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。</p><p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p><p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p><a id="more"></a><p>$$<br>A = U \Sigma V^T<br>$$</p><p>這 3 個矩陣分別是有意義的，$U$、$\Sigma$、$V$ 的意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：</p><p><img src="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300" alt=""></p><blockquote><p>圖來自以上提到的文章</p></blockquote><p>所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。</p><p>在模型上，我們還會加上一個 bias 來達到平移的效果。</p><p>我們可以來造一些點：</p><p><img src="/images/activation1.png" alt=""></p><p>我們可以 random 造一個矩陣：</p><p>$$<br>A =<br>\begin{bmatrix}<br>0.35166263&amp; 0.1124966 \\<br>0.25249535&amp; 0.65481947 \\<br>\end{bmatrix}<br>$$</p><p>所以我們可以對每個點做運算 $Ax$：</p><p><img src="/images/activation2.png" alt=""></p><h2 id="去吧！Activation-function！"><a href="#去吧！Activation-function！" class="headerlink" title="去吧！Activation function！"></a>去吧！Activation function！</h2><p>接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？</p><p><img src="/images/activation3.png" alt=""></p><p>大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。</p><p>所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。</p><p>那如果是用 sigmoid 的效果呢？</p><p><img src="/images/activation4.png" alt=""></p><p>他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。</p><p>我們放大來看看他整體形狀有沒有什麼變化。</p><p><img src="/images/activation5.png" alt=""></p><p>整體形狀有些微被扭曲了，不知道大家有沒有發現呢？</p><p>所以在引進 activation function 之後，模型擁有了 <strong>扭曲</strong> 的能力！</p><p>那麼 activation function 到底實際上做了什麼事呢？</p><h2 id="雕塑的工匠"><a href="#雕塑的工匠" class="headerlink" title="雕塑的工匠"></a>雕塑的工匠</h2><p>以 ReLU 來說，他就像一個工匠正在雕塑一個作品。</p><p>ReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：</p><p><img src="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/image/a51d2-dsc_7226.jpg" alt=""></p><blockquote><p>圖來自 <a href="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/" target="_blank" rel="noopener">漫遊‧藝術網＠成大校園</a></p></blockquote><p>原諒我私心用成大的雕塑品當範例。XD</p><p>ReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。</p><h2 id="消失的梯度"><a href="#消失的梯度" class="headerlink" title="消失的梯度"></a>消失的梯度</h2><p>以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。</p><p>其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。</p><p>那如果放在 CNN 的話，就會發生梯度消失的問題。</p><p>在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。</p><p>讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。</p><p>這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。</p><h2 id="模型怎麼知道要從哪裡下刀？"><a href="#模型怎麼知道要從哪裡下刀？" class="headerlink" title="模型怎麼知道要從哪裡下刀？"></a>模型怎麼知道要從哪裡下刀？</h2><p>我想蠻多人應該會有跟我一樣的問題。</p><p>我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？</p><p>答案是 backpropagation (gradient descent method) 會告訴你！</p><p>藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。</p><h2 id="ReLU-與-softplus"><a href="#ReLU-與-softplus" class="headerlink" title="ReLU 與 softplus"></a>ReLU 與 softplus</h2><p>softplus 是一個跟 ReLU 非常像的 activation function。</p><p>$$<br>softplus(x) = log(1 + e^x)<br>$$</p><p>兩者的差別：</p><p><img src="/images/Rectifier_and_softplus_functions.svg" alt=""></p><p>你可以把 softplus 看成 ReLU 的可微分版本，或是將 ReLU 看成 softplus 的簡化版本。</p><p>他在負的區域看起來跟 sigmoid function 很像，另一邊正的區域就會非常接近 identity function。</p><p>在行為上也很符合神經的特性，就是有正向的訊號就會是正向的輸出，如果是負向的訊號就不輸出。</p><p>很有趣的特性吧！</p><h2 id="為什麼-activation-function-一定要長這樣？"><a href="#為什麼-activation-function-一定要長這樣？" class="headerlink" title="為什麼 activation function 一定要長這樣？"></a>為什麼 activation function 一定要長這樣？</h2><p>其實沒有規定 activation function 一定要長怎樣。</p><p>但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。</p><p>所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>不同的模型跟問題其實適用不同的 activation function。</p><p>像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。</p><p>所以在 feature extraction 的階段就需要找適用的 activation function。</p><p>那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。</p><p>這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD</p><p>今天就到這邊告一個段落囉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天我們來談談 activation function 吧！&lt;/p&gt;
&lt;h2 id=&quot;先談談線性轉換&quot;&gt;&lt;a href=&quot;#先談談線性轉換&quot; class=&quot;headerlink&quot; title=&quot;先談談線性轉換&quot;&gt;&lt;/a&gt;先談談線性轉換&lt;/h2&gt;&lt;p&gt;談 activation function 之前先要談談線性轉換。&lt;/p&gt;
&lt;p&gt;有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。&lt;/p&gt;
&lt;p&gt;推薦可以看周老師的線代啟示錄 &lt;a href=&quot;https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;奇異值分解 (SVD)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
</feed>
