<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dream Maker</title>
  
  <subtitle>Love Math, Science, Biology, Computer science</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yuehhua.github.io/"/>
  <updated>2018-10-28T14:58:09.245Z</updated>
  <id>https://yuehhua.github.io/</id>
  
  <author>
    <name>Yueh-Hua Tu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>28 Transformer</title>
    <link href="https://yuehhua.github.io/2018/10/28/28-transformer/"/>
    <id>https://yuehhua.github.io/2018/10/28/28-transformer/</id>
    <published>2018-10-28T14:58:09.000Z</published>
    <updated>2018-10-28T14:58:09.245Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。</p><p>這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。</p><p>Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。</p><p>（跟變形金剛一樣的名字耶！帥吧！）</p><p><img src="/images/transformer1.svg" alt=""></p><p><img src="/images/transformer2.svg" alt=""></p><p>（大家等等我找個時間把後面一次寫完，不然寫一點點而且很敷淺，我自己很痛苦）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。&lt;/p&gt;
&lt;p&gt;這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>27 Attention model</title>
    <link href="https://yuehhua.github.io/2018/10/27/27-attention-model/"/>
    <id>https://yuehhua.github.io/2018/10/27/27-attention-model/</id>
    <published>2018-10-27T15:10:46.000Z</published>
    <updated>2018-10-28T03:17:36.974Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。</p><p>Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。</p><p>Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。</p><p>有幾篇論文算是開始用這樣的機制：</p><ol><li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></li><li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li><li><a href="https://arxiv.org/abs/1509.00685" target="_blank" rel="noopener">A Neural Attention Model for Abstractive Sentence Summarization</a></li><li><a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noopener">Reasoning about Entailment with Neural Attention</a></li></ol><p>Attention model 在很多模型當中都是做為 encoder-decoder 之間的橋樑，原本的 encoder 跟 decoder 之間是只有一個 vector 來傳遞所有的訊息，</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。&lt;/p&gt;
&lt;p&gt;Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention 
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>26 seq2seq model</title>
    <link href="https://yuehhua.github.io/2018/10/26/26-sequence-to-sequence-model/"/>
    <id>https://yuehhua.github.io/2018/10/26/26-sequence-to-sequence-model/</id>
    <published>2018-10-26T15:40:23.000Z</published>
    <updated>2018-10-27T03:40:37.108Z</updated>
    
    <content type="html"><![CDATA[<p>前面有提到 seq2seq model，我們就從這邊開始。</p><p>Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！</p><p><img src="/images/seq2seq.svg" alt=""></p><a id="more"></a><p>在以往的 language model 的作法裡，會是把一個 word 塞進 RNN 裡，那麼 RNN 就會立刻吐出一個相對應的 word 出來。</p><p>像是放進一個英文字，會吐出一個相對應的法文字，然後將這一層的預測結果帶給下一層。</p><p>這麼做雖然很直覺，但是他並不能完整的翻譯一個句子。</p><p>語言的語法各不相同，所以很難將詞語一一對映做成翻譯。</p><p>這個 seq2seq model 採用了不同的作法，將一組 LSTM 作為 encoder，負責將要翻譯的句子轉換成固定長度的向量，再將這個向量交給另一個 LSTM 轉換成目標句子，後面這個 LSTM 就是 decoder 的角色。</p><p>這樣的架構之下將一個模型拆成 encoder 跟 decoder 的兩個部份，讓兩個部份都可以各自接受或是產生不同長度的句子，並且得到很好的分數。</p><p>在實作上，的確是將兩個 LSTM 接起來，所以就沒什麼細節好講的。</p><p>但是這個模型確實的解決了以不同長度的句子產生不同長度的句子的問題。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面有提到 seq2seq model，我們就從這邊開始。&lt;/p&gt;
&lt;p&gt;Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/seq2seq.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>25 Recurrent model 之死</title>
    <link href="https://yuehhua.github.io/2018/10/25/25-death-of-recurrent-model/"/>
    <id>https://yuehhua.github.io/2018/10/25/25-death-of-recurrent-model/</id>
    <published>2018-10-25T15:21:36.000Z</published>
    <updated>2018-10-25T15:21:36.182Z</updated>
    
    <content type="html"><![CDATA[<p>當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。</p><p>不要再用 RNN 為基礎的模型了！！</p><p>就是這篇 <a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" target="_blank" rel="noopener">The fall of RNN / LSTM</a></p><p>為什麼呢？</p><p>基本上裏面提到 vanishing gradient 的問題一直沒有解決以外，還有沒有辦法善用硬體的侷限在。</p><p>像這種循序型的模型，模型天生無法平行化運算，所以 GPU 就無用武之地，只能靠 CPU 慢慢跑。</p><p>那有什麼解決辦法呢？</p><h2 id="Self-attention-model"><a href="#Self-attention-model" class="headerlink" title="Self-attention model"></a>Self-attention model</h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> 這篇文章提出了 Transformer 這個模型，基本上這個模型使用了 self-attention 的機制。</p><p>要講這個之前我們要先聊聊 attention model。在 attention model 之前，sequence-to-sequence model 做出了重大的突破。一個具有彈性，可以任意組合的模型誕生了，管你是要生成句子還是怎麼樣。原本是只有 RNN 一個單元一個單元慢慢去對映 X 到 Y，sequence-to-sequence model 將這樣的對應關係解耦，由一個 encoder 負責將 X 的資訊萃取出來，再經由 decoder 將資訊轉換成 Y 輸出。</p><p>但是 LSTM 還是沒辦法記憶夠長的，後來 attention model 就誕生了。乾脆就將 encoder 所萃取到的資訊紀錄下來，變成一個，然後再丟到 decoder 去將資訊還原成目標語言，就可以完成機器翻譯了。</p><p>但是這種方式還是不脫 recurrent model，那就乾脆做成 self-attention 的機制，也就是這邊的 Transformer，完全摒棄了 recurrent 的限制。</p><h2 id="Autoregressive-generative-model"><a href="#Autoregressive-generative-model" class="headerlink" title="Autoregressive generative model"></a>Autoregressive generative model</h2><p>接著是今年6月的文章 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a>，當你的 recurrent model 不必再 recurrent！</p><p>也就是將 RNN 的問題又重述了一遍，並且提出大家都漸漸以 autoregressive generative model 來解決這樣的問題。</p><p>這篇算這引言，我接下來會開始一一解釋模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。&lt;/p&gt;
&lt;p&gt;不要再用 RNN 為基礎的模型了！！&lt;/p&gt;
&lt;p&gt;就是這篇 &lt;a href=&quot;https://towardsdatascience.com/the-fall-of-rnn-lstm-
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>24 Recurrent neural network</title>
    <link href="https://yuehhua.github.io/2018/10/25/24-recurrent-neural-network-1/"/>
    <id>https://yuehhua.github.io/2018/10/25/24-recurrent-neural-network-1/</id>
    <published>2018-10-25T08:41:15.000Z</published>
    <updated>2018-10-25T08:41:15.480Z</updated>
    
    <content type="html"><![CDATA[<p>接續上一篇。</p><h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p><ul><li>狀態都是 <strong>連續</strong> 的。</li><li>時間是離散的。</li><li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li><li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li><li><strong>允許在每個時間點給輸入</strong></li><li><strong>引入非線性</strong></li></ul><a id="more"></a><p>首先，在這邊的狀態會以一個向量做表示，大家應該也知道 RNN 的 input 是一個向量，當中的狀態也是一個向量，最後的 output 也是一個向量。而這些向量當中的的值都是連續的 $\mathbb{R}^n$（假設向量大小為 n），不像上面的模型都是離散的 $k$（假設有 k 個狀態），所以在空間上的大小可以說是擴大非常多。</p><p>接下來我們來看看時間的狀態轉換：</p><p><br></p><p><img src="/images/rnn_time.svg" alt=""></p><p><br></p><p><img src="/images/rnn_expand_time.svg" alt=""></p><p><br></p><p>在 RNN 中一樣含有內在狀態，但不同的是 RNN 可以在每個時間點上給輸入向量（$\mathbf{x^{(t)}}$），所以可以根據前一個時間點的內在狀態（$\mathbf{h^{(t)}}$）跟輸入向量去計算輸出，或是外在狀態（$\mathbf{y^{(t)}}$）。</p><p>所以大家會在一些論文上看到模型的狀態關係式長下面這個樣子：</p><p>$$<br>\mathbf{h^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}}) = \mathbf{x^{(t)}} W_x + \mathbf{h^{(t-1)}} W_h + \mathbf{b}<br>$$</p><p>$$<br>\mathbf{y^{(t)}} = g(\mathbf{h^{(t)}}) = sigm(\mathbf{h^{(t)}} W_y)<br>$$</p><p>這邊特別引入了非線性的轉換（$sigm$）來讓模型更強大。</p><p>隨著從一開始的馬可夫模型到這邊應該對這幾個模型有點感覺，其實 RNN 可以說是很大的突破，在假設上放了很多元素讓模型變得更強大。</p><h2 id="Long-short-term-memory"><a href="#Long-short-term-memory" class="headerlink" title="Long short-term memory"></a>Long short-term memory</h2><p>人們為了改進 RNN這個模型的記憶性，希望他可以記住更遠以前的東西，所以設計了 LSTM 來替換他的 hidden layer 的運作模式，後期更有 GRU，還有人說只需要 forget gate 就有很強大的效能的 MGU。這些都是對於記憶性做的改進，個人覺得這些在工程上的貢獻比較大，真正學術上的突破其實還好。</p><p>今天的整理就先到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接續上一篇。&lt;/p&gt;
&lt;h2 id=&quot;Recurrent-neural-network&quot;&gt;&lt;a href=&quot;#Recurrent-neural-network&quot; class=&quot;headerlink&quot; title=&quot;Recurrent neural network&quot;&gt;&lt;/a&gt;Recurrent neural network&lt;/h2&gt;&lt;p&gt;那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;狀態都是 &lt;strong&gt;連續&lt;/strong&gt; 的。&lt;/li&gt;
&lt;li&gt;時間是離散的。&lt;/li&gt;
&lt;li&gt;內在狀態是不能被觀察的，外在狀態是可被觀察的。&lt;/li&gt;
&lt;li&gt;以一個 &lt;strong&gt;隨機向量&lt;/strong&gt; 作為一個狀態。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;允許在每個時間點給輸入&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入非線性&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>24 Recurrent neural network</title>
    <link href="https://yuehhua.github.io/2018/10/23/24-recurrent-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/23/24-recurrent-neural-network/</id>
    <published>2018-10-23T13:43:49.000Z</published>
    <updated>2018-10-23T13:43:49.912Z</updated>
    
    <content type="html"><![CDATA[<p>接續上一篇。</p><h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p><ul><li>狀態都是 <strong>連續</strong> 的。</li><li>時間是離散的。</li><li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li><li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li><li><strong>允許在每個時間點給輸入</strong></li><li><strong>引入非線性</strong></li></ul><p>首先，在這邊的狀態會以一個向量做表示，大家應該也知道 RNN 的 input 是一個向量，當中的狀態也是一個向量，最後的 output 也是一個向量。而這些向量當中的的值都是連續的 $\mathbb{R}^n$（假設向量大小為 n），不像上面的模型都是離散的 $k$（假設有 k 個狀態），所以在空間上的大小可以說是擴大非常多。</p><p>接下來我們來看看時間的狀態轉換：</p><p><br></p><p><img src="/images/rnn_time.svg" alt=""></p><p><br></p><p><img src="/images/rnn_expand_time.svg" alt=""></p><p><br></p><p>在 RNN 中一樣含有內在狀態，但不同的是 RNN 可以在每個時間點上給輸入向量（$\mathbf{x^{(t)}}$），所以可以根據前一個時間點的內在狀態（$\mathbf{h^{(t)}}$）跟輸入向量去計算輸出，或是外在狀態（$\mathbf{y^{(t)}}$）。</p><p>所以大家會在一些論文上看到模型的狀態關係式長下面這個樣子：</p><p>$$<br>\mathbf{h^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}}) = \mathbf{x^{(t)}} W_x + \mathbf{h^{(t-1)}} W_h + \mathbf{b}<br>$$</p><p>$$<br>\mathbf{y^{(t)}} = g(\mathbf{h^{(t)}}) = sigm(\mathbf{h^{(t)}} W_y)<br>$$</p><p>這邊特別引入了非線性的轉換（$sigm$）來讓模型更強大。</p><p>隨著從一開始的馬可夫模型到這邊應該對這幾個模型有點感覺，其實 RNN 可以說是很大的突破，在假設上放了很多元素讓模型變得更強大。</p><h2 id="Long-short-term-memory"><a href="#Long-short-term-memory" class="headerlink" title="Long short-term memory"></a>Long short-term memory</h2><p>人們為了改進 RNN這個模型的記憶性，希望他可以記住更遠以前的東西，所以設計了 LSTM 來替換他的 hidden layer 的運作模式，後期更有 GRU，還有人說只需要 forget gate 就有很強大的效能的 MGU。這些都是對於記憶性做的改進，個人覺得這些在工程上的貢獻比較大，真正學術上的突破其實還好。</p><p>今天的整理就先到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;接續上一篇。&lt;/p&gt;
&lt;h2 id=&quot;Recurrent-neural-network&quot;&gt;&lt;a href=&quot;#Recurrent-neural-network&quot; class=&quot;headerlink&quot; title=&quot;Recurrent neural network&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>23 Markov chain 及 HMM</title>
    <link href="https://yuehhua.github.io/2018/10/23/23-markov-chain-and-hmm/"/>
    <id>https://yuehhua.github.io/2018/10/23/23-markov-chain-and-hmm/</id>
    <published>2018-10-23T01:21:38.000Z</published>
    <updated>2018-10-23T01:21:38.594Z</updated>
    
    <content type="html"><![CDATA[<p>上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。</p><p>這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。</p><p>在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。</p><a id="more"></a><h2 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h2><p>馬可夫模型是這樣的，他假設一個變數有不同種的狀態，例如下圖：</p><p><br></p><p><img src="/images/markov_model.svg" alt=""></p><p><br></p><p>在這邊有4個狀態，一個圓圈代表一個狀態，狀態跟狀態之間會隨著時間改變，每個狀態會有一定機率變成其他狀態，或是維持原本的狀態不變。</p><p>我們可以把目前的狀態用一個向量來表達：</p><p>$$<br>\mathbf{y} =<br>\begin{bmatrix}<br>y_1&amp;y_2&amp;y_3&amp;y_4 \\<br>\end{bmatrix}<br>$$</p><blockquote><p>注意：我們這邊使用的向量為列向量（row vector），一般在其他數學領域使用的則是行向量（column vector），兩者是可以藉由轉置來互換的，並不影響運算結果</p></blockquote><p>我們這邊用 $\mathbf{y}$ 代表他是可以被觀察到的。狀態變化我們可以用一個矩陣來表達：</p><p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p><p>其中 $a_{ij}$ 代表的是由狀態 $i$ 變成狀態 $j$ 的機率，這邊要注意的是，每一個列（row）的機率總和要是 1。</p><blockquote><p>補充：<br>由於我們的向量都是 row vector<br>$A$ 為一 right stochastic matrix，運算形式為 $\mathbf{y} A$<br>由狀態 $i$ 變成其他狀態之機率和為 1（$A \mathbf{1} = \mathbf{1}$）<br>感謝陳杰翰先生幫忙指正</p></blockquote><p>所以不同時間點的狀態變化關係可以寫成以下式子：</p><p>$$<br>\mathbf{y^{(t)}} = \mathbf{y^{(t-1)}} A<br>$$</p><p>$\mathbf{y^{(t)}}$ 的意思是第 t 次（或是時間為 t）的狀態，$\mathbf{y^{(t-1)}}$ 狀態會經過一次轉換或是運算轉變成 $\mathbf{y^{(t)}}$。</p><p>如果你把其中的第一項的運算拆開來看就會長這樣，可以自行檢驗狀態的變化：</p><p>$$<br>y_1^{(t)} = a_{11}y_1^{(t-1)} + a_{12}y_2^{(t-1)} + a_{13}y_3^{(t-1)} + a_{14}y_4^{(t-1)}<br>$$</p><p>從時間軸上來看，我們可以把狀態的轉變畫出來像是這樣：</p><p><br></p><p><img src="/images/markov_model_expand_time.svg" alt=""></p><p><br></p><p>每次的轉變我們都可以看成一個函數 $f$，他其實等同於上面提到的矩陣：</p><p>$$<br>\mathbf{y^{(t)}} = f(\mathbf{y^{(t-1)}}) = \mathbf{y^{(t-1)}} A<br>$$</p><p>所以他的意思是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$，所以這是單純的狀態變化。</p><p>上面的矩陣當中其實內容是機率，我們也可以把他轉成機率的寫法，但是解釋會變得不太一樣：</p><p>$$<br>p = f(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p><p>這邊的解釋是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$ <strong>的機率</strong>。</p><p>下句跟上句的不同在於，上句的描述是肯定的，他只描述了狀態的改變，但是下句多加描述了 <strong>這件事會發生的機率</strong>，所以應該要把 $\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}$ 理解成 <strong>這一件事</strong>，那麼 $f$ 的輸出就是機率了。</p><p>我們可以把上圖 <em>收起來</em>，所以看起來會像這樣：</p><p><br></p><p><img src="/images/markov_model_time.svg" alt=""></p><p><br></p><p>花了點時間把一些符號跟數學概念講完了，來談談他的假設，一般來說，馬可夫模型最大的假設在於：</p><p>$$<br>P(\mathbf{y^{(t)}} \mid \mathbf{y^{(1)}}, \mathbf{y^{(2)}}, \dots, \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p><p>也就是要預測第 t 單位時間的狀態，我們經歷了第 1~(t - 1) 單位時間，但是他只需要用前一個時間單位的狀態就可以預測下一個狀態，前面很多狀態都是不必要的，這我們稱為一階馬可夫模型（first-order Markov chain）。</p><p>當然可以推廣到 m 階馬可夫模型（m th-order Markov chain），那代表需要前 m 個狀態來預測下一個狀態，順帶一提，有零階馬可夫模型，那就跟我們一般的機率分佈模型（$P(\mathbf{y^{(t)}}）$）一樣。</p><p>沒有特別提的話，通常大家談的馬可夫模型都是一階馬可夫模型。一般來說，他有個非常重要的特性，就是 <strong>無記憶性</strong>，也就是他不會去記住他所經歷的狀態，他只需要用現在的狀態就可以預測下一個狀態。</p><p>不過我要特別提一下這個模型的一些其他假設：</p><ul><li>狀態是離散的。在馬可夫模型的狀態空間中是離散的，也就是你可以用一個正整數來數出有幾種狀態存在。</li><li>時間是離散的。我們剛剛有看到他計算的是第 t 單位時間，下一次就是乘上一個矩陣之後成為第 t+1 單位時間。</li><li>狀態是可被觀察的。</li><li>以一個隨機變數作為一個狀態。</li></ul><p>接下來我們來談談另一個模型。</p><p><br></p><h2 id="Hidden-Markov-model"><a href="#Hidden-Markov-model" class="headerlink" title="Hidden Markov model"></a>Hidden Markov model</h2><p>接下來是進階版的隱馬可夫模型（hidden Markov model），他的假設是這樣的，在一個系統中存在一些我們看不到的狀態，是系統的內在狀態，隨著系統的內在狀態不同，他所表現出來的外在狀態也不同，而外在狀態是我們可以觀測到的。</p><p><br></p><p><img src="/images/hmm.svg" alt=""></p><p><br></p><p>大家可以看到這個圖跟剛剛的很相似，帶是又多了一些東西。較大的圈圈是內在狀態，小的圈圈是外在狀態。隨著時間改變，內在狀態會隨著變動，內在狀態的變動我們可以用一個矩陣來表示：</p><p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p><p>裏面裝的一樣是機率。接下來，不同的內在狀態有不同的機率會噴出（emit）外在狀態，這也會用另一個矩陣表示：</p><p>$$<br>B = [b_{ij}] =<br>\begin{bmatrix}<br>b_{11}&amp; b_{12}&amp; b_{13}&amp; b_{14} \\<br>b_{21}&amp; b_{22}&amp; b_{23}&amp; b_{24} \\<br>b_{31}&amp; b_{32}&amp; b_{33}&amp; b_{34} \\<br>\end{bmatrix}<br>$$</p><p>寫成狀態轉移的關係式的話會變成：</p><p>$$<br>\mathbf{h^{(t)}} = \mathbf{h^{(t-1)}} A<br>$$</p><p>$\mathbf{h^{(t)}}$ 代表在第 t 單位時間的內在狀態。</p><p>$$<br>\mathbf{y^{(t)}} = \mathbf{h^{(t)}} B<br>$$</p><p>$\mathbf{y^{(t)}}$ 代表在第 t 單位時間根據內在狀態噴出的外在狀態。</p><p>如果在時間軸上表達的話是這個樣子：</p><p><br></p><p><img src="/images/hmm_expand_time.svg" alt=""></p><p><br></p><p><img src="/images/hmm_time.svg" alt=""></p><p><br></p><p>由於在這邊又多了一個內在狀態，所以在模型的表達力上遠遠超越馬可夫模型。舉個例子好了，假設小明很好奇在不同天氣的時候外面的人吃冰淇淋的狀況是如何，但是小明又很懶得出門看天氣，這時候他就假設天氣（晴天、陰天、雨天）是內在狀態（看不到），然後他觀察路上的人吃冰淇淋（外在狀態，吃、不吃）的多寡，這時候這麼模型就可以派上用場，他藉由持續觀察路人有沒有吃冰淇淋，可以推論外面天氣的變化狀況。</p><p>這時候我們也來總結一下，這個模型的假設：</p><ul><li>內在狀態跟外在狀態都是離散的。</li><li>時間是離散的。</li><li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li><li>以一個隨機變數作為一個狀態。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。&lt;/p&gt;
&lt;p&gt;這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。&lt;/p&gt;
&lt;p&gt;在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>22 Convolutional encoder-decoder 架構</title>
    <link href="https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/"/>
    <id>https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/</id>
    <published>2018-10-22T15:10:35.000Z</published>
    <updated>2018-10-23T00:53:57.844Z</updated>
    
    <content type="html"><![CDATA[<p>標題這不是一個專有名詞。</p><p>在電腦視覺的領域中有幾個有名的問題：</p><ol><li>影像辨識（Image recognition）</li><li>物件辨識（Object detection）</li><li>語意分割（Semantic segmentation）</li></ol><a id="more"></a><p><img src="/images/cv-tasks.jpg" alt=""></p><p>影像辨識是給一張影像，希望模型可以辨識出當中的東西是什麼。輸入模型的會是影像向量，輸出的會是類別向量。</p><p>物件辨識給的同樣是一張影像，除了需要辨識出當中的物件以外，還要給出這個物體所在的位置，輸出的除了類別向量以外，還有座標。</p><p>語意分割可能無法一眼看出當中的含意，對一個句子來說，詞本身帶有一些含意，對比到影像上，句子就是影像，而詞意就是影像中的物體。語意分割是給一張影像，需要將影像中的物件切割出來，所以必須對每個像素做分類。</p><p><img src="/images/semantic_seg.jpeg" alt=""></p><p>三者是各自不同的任務。</p><p>不同的任務有些共通性，這些共通性讓他們可能都可以適用 CNN 的架構。不過這麼說還是太過粗糙了。</p><p>對於影像辨識來說，一般性架構會是有 convolution layer 為主的 feature extractor，接著會是以 fully-connected layer 為主的 classifier。在不同階段有不同的目的，在輸入影像之後要先對影像進一步抽取特徵，有了足夠的特徵之後才進行分類。</p><p>語意分割也有類似的架構，在前面會有 convolution layer 為主的 feature extractor，但是為了將每個樣素做分類，必須對每一個像素做預測，預測像素的類別。在後半的部份，有人提出了 Fully convolution network，試圖做像素的類別預測。</p><p>像素的類別預測這件事從另一個角度切入，會很像是一種生成的過程。也就是，我們在前面要將影像的特徵萃取出來，是一種將資訊壓縮的過程，在後半我們希望將壓縮的資訊還原到某種程度，我們需要產生器（generator）。</p><p><img src="/images/segnet.svg" alt=""></p><blockquote><p><a href="http://mi.eng.cam.ac.uk/projects/segnet/" target="_blank" rel="noopener">SegNet [2015, University of Cambridge]</a></p></blockquote><p>在語意分割這個問題，後來就一路發展到了 encoder-decoder 架構，我們又回到類似 autoencoder 的樣子，讓 encoder 跟 decoder 一起訓練的模型架構。在這邊 encoder 就是 feature extractor，decoder 就是一種 generator。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;標題這不是一個專有名詞。&lt;/p&gt;
&lt;p&gt;在電腦視覺的領域中有幾個有名的問題：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;影像辨識（Image recognition）&lt;/li&gt;
&lt;li&gt;物件辨識（Object detection）&lt;/li&gt;
&lt;li&gt;語意分割（Semantic segmentation）&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>21 Activation functions and ReLU</title>
    <link href="https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/"/>
    <id>https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/</id>
    <published>2018-10-19T14:34:55.000Z</published>
    <updated>2018-10-19T14:34:55.385Z</updated>
    
    <content type="html"><![CDATA[<p>今天我們來談談 activation function 吧！</p><h2 id="先談談線性轉換"><a href="#先談談線性轉換" class="headerlink" title="先談談線性轉換"></a>先談談線性轉換</h2><p>談 activation function 之前先要談談線性轉換。</p><p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。</p><p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p><p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p><a id="more"></a><p>$$<br>A = U \Sigma V^T<br>$$</p><p>這 3 個矩陣分別是有意義的，$U$、$\Sigma$、$V$ 的意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：</p><p><img src="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300" alt=""></p><blockquote><p>圖來自以上提到的文章</p></blockquote><p>所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。</p><p>在模型上，我們還會加上一個 bias 來達到平移的效果。</p><p>我們可以來造一些點：</p><p><img src="/images/activation1.png" alt=""></p><p>我們可以 random 造一個矩陣：</p><p>$$<br>A =<br>\begin{bmatrix}<br>0.35166263&amp; 0.1124966 \\<br>0.25249535&amp; 0.65481947 \\<br>\end{bmatrix}<br>$$</p><p>所以我們可以對每個點做運算 $Ax$：</p><p><img src="/images/activation2.png" alt=""></p><h2 id="去吧！Activation-function！"><a href="#去吧！Activation-function！" class="headerlink" title="去吧！Activation function！"></a>去吧！Activation function！</h2><p>接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？</p><p><img src="/images/activation3.png" alt=""></p><p>大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。</p><p>所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。</p><p>那如果是用 sigmoid 的效果呢？</p><p><img src="/images/activation4.png" alt=""></p><p>他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。</p><p>我們放大來看看他整體形狀有沒有什麼變化。</p><p><img src="/images/activation5.png" alt=""></p><p>整體形狀有些微被扭曲了，不知道大家有沒有發現呢？</p><p>所以在引進 activation function 之後，模型擁有了 <strong>扭曲</strong> 的能力！</p><p>那麼 activation function 到底實際上做了什麼事呢？</p><h2 id="雕塑的工匠"><a href="#雕塑的工匠" class="headerlink" title="雕塑的工匠"></a>雕塑的工匠</h2><p>以 ReLU 來說，他就像一個工匠正在雕塑一個作品。</p><p>ReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：</p><p><img src="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/image/a51d2-dsc_7226.jpg" alt=""></p><blockquote><p>圖來自 <a href="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/" target="_blank" rel="noopener">漫遊‧藝術網＠成大校園</a></p></blockquote><p>原諒我私心用成大的雕塑品當範例。XD</p><p>ReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。</p><h2 id="消失的梯度"><a href="#消失的梯度" class="headerlink" title="消失的梯度"></a>消失的梯度</h2><p>以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。</p><p>其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。</p><p>那如果放在 CNN 的話，就會發生梯度消失的問題。</p><p>在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。</p><p>讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。</p><p>這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。</p><h2 id="模型怎麼知道要從哪裡下刀？"><a href="#模型怎麼知道要從哪裡下刀？" class="headerlink" title="模型怎麼知道要從哪裡下刀？"></a>模型怎麼知道要從哪裡下刀？</h2><p>我想蠻多人應該會有跟我一樣的問題。</p><p>我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？</p><p>答案是 backpropagation (gradient descent method) 會告訴你！</p><p>藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。</p><h2 id="ReLU-與-softplus"><a href="#ReLU-與-softplus" class="headerlink" title="ReLU 與 softplus"></a>ReLU 與 softplus</h2><p>softplus 是一個跟 ReLU 非常像的 activation function。</p><p>$$<br>softplus(x) = log(1 + e^x)<br>$$</p><p>兩者的差別：</p><p><img src="/images/Rectifier_and_softplus_functions.svg" alt=""></p><p>你可以把 softplus 看成 ReLU 的可微分版本，或是將 ReLU 看成 softplus 的簡化版本。</p><p>他在負的區域看起來跟 sigmoid function 很像，另一邊正的區域就會非常接近 identity function。</p><p>在行為上也很符合神經的特性，就是有正向的訊號就會是正向的輸出，如果是負向的訊號就不輸出。</p><p>很有趣的特性吧！</p><h2 id="為什麼-activation-function-一定要長這樣？"><a href="#為什麼-activation-function-一定要長這樣？" class="headerlink" title="為什麼 activation function 一定要長這樣？"></a>為什麼 activation function 一定要長這樣？</h2><p>其實沒有規定 activation function 一定要長怎樣。</p><p>但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。</p><p>所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>不同的模型跟問題其實適用不同的 activation function。</p><p>像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。</p><p>所以在 feature extraction 的階段就需要找適用的 activation function。</p><p>那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。</p><p>這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD</p><p>今天就到這邊告一個段落囉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天我們來談談 activation function 吧！&lt;/p&gt;
&lt;h2 id=&quot;先談談線性轉換&quot;&gt;&lt;a href=&quot;#先談談線性轉換&quot; class=&quot;headerlink&quot; title=&quot;先談談線性轉換&quot;&gt;&lt;/a&gt;先談談線性轉換&lt;/h2&gt;&lt;p&gt;談 activation function 之前先要談談線性轉換。&lt;/p&gt;
&lt;p&gt;有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。&lt;/p&gt;
&lt;p&gt;推薦可以看周老師的線代啟示錄 &lt;a href=&quot;https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;奇異值分解 (SVD)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>20 Convolutional neural network</title>
    <link href="https://yuehhua.github.io/2018/10/17/20-convolutional-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/17/20-convolutional-neural-network/</id>
    <published>2018-10-17T14:58:40.000Z</published>
    <updated>2018-10-17T14:58:40.101Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Convolution-layer"><a href="#Convolution-layer" class="headerlink" title="Convolution layer"></a>Convolution layer</h2><p>這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？</p><p>我們先來看二維的 cross-correlation 長什麼樣子。</p><a id="more"></a><p><img src="/images/ccor2d.svg" alt=""></p><p>然後是反序的 convolution。</p><p><img src="/images/conv2d.svg" alt=""></p><p>這樣有沒有搞懂一點 convolution 的運算了呢？</p><p>接著，想像在右手邊比較小的方框就是你的 filter （或是 kernel），然後他會沿著兩個軸去移動，去掃描看看有沒有跟 filter 的 pattern 很像的，當他偵測到很像的 pattern 的時候，輸出的 feature map 的值就會很高，所以這樣就可以做到上面講的第二點，也就是位移的不變性。不過說起來也不是真的有什麼位移不變性啦！他只是沿著軸去做掃描可以減少訓練的參數，這樣 filter 還是有在位移阿！只是對於要偵測的 pattern 看起來好像有”位移不變性”一樣。到這邊我們第一點跟第二點都解決了，剩下第三點。</p><h2 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h2><p>跟第三點相關的就是 subsampling，也就是如果讓圖片變小，不只可以降低要辨識的區塊大小，還可以降低需要訓練的參數量。那要怎麼讓圖片變小？</p><p>Maxpooling 是目前主流的方法，也就是在一個 window 的範圍內去找最大值，只保留最大值。還有一種是 meanpooling，顧名思義，他取整個 window 的平均值作為保留值。</p><p>所以 subsampling 在一些應用場景是需要的，有些是不需要的，像是有些 pattern 的辨識是不能去除細節的，一旦去除細節就會造成辨識困難，那就代表他沒有辦法用 subsampling。有時候照片縮小到一定程度人類也會無法辨識當中的圖像是什麼，所以也不要用過頭。</p><h2 id="Feature-extractor-classifier-架構"><a href="#Feature-extractor-classifier-架構" class="headerlink" title="Feature extractor-classifier 架構"></a>Feature extractor-classifier 架構</h2><p>經過以上介紹後，我們可以把 convolution layer 跟 subsampling 結合起來，成為所謂的 feature extractor。</p><p>經由以上三點特性，這些 layer 的巧妙運用可以是非常棒的 feature extractor。不同種的資料特性需要不同設計的 feature extractor，接下來就是 classifier 上場了。</p><p>典型的 classifier 可以是 SVM，或是要用比較潮的 deep learning 也可以，最單純的就是前一篇提到的 MLP 了。</p><p>這樣前後組合好就是個 CNN 的雛型了！</p><h2 id="MLP-做不好的事情"><a href="#MLP-做不好的事情" class="headerlink" title="MLP 做不好的事情"></a>MLP 做不好的事情</h2><p>前一篇我們有提到 MLP 因為會從整體特徵去做內積，所以整體的模式會優先被考慮，如果有區域性的特徵並不一定會被凸顯出來。在 MLP 相對上會比較注重整體性而不是區域性，所以使用 MLP 在影像處理上就比 CNN 不是那麼有利。</p><h2 id="關鍵在哪裡？"><a href="#關鍵在哪裡？" class="headerlink" title="關鍵在哪裡？"></a>關鍵在哪裡？</h2><p>我個人認為關鍵在資料的 <strong>區域性</strong>，也就是你想做的事情其實是跟資料的區域性有關係，或是你的資料是週期性資料（週期性出現的模式也可以視為是一種區域性模式重複出現），這樣你就可以用 convolution layer！（注意！不是 CNN！）</p><p>像是音樂當中有週期性的模式就可以用，在生物領域，蛋白質會去辨認 DNA 序列，有被辨認到的部份就會有蛋白質黏附上去，生物學家會想知道到底哪些地方有蛋白質黏附，黏附的序列是區域性的，所以也有應用 CNN 技術在這方面上。</p><p>最重要的還是去了解你的資料的特性，然後了解模型當中各元件的性質跟數學特性，這樣才能正確地使用這些技術解決問題，走到這邊需要同時是領域知識的專家，也同時是 DL 的專家才行阿！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Convolution-layer&quot;&gt;&lt;a href=&quot;#Convolution-layer&quot; class=&quot;headerlink&quot; title=&quot;Convolution layer&quot;&gt;&lt;/a&gt;Convolution layer&lt;/h2&gt;&lt;p&gt;這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？&lt;/p&gt;
&lt;p&gt;我們先來看二維的 cross-correlation 長什麼樣子。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>19 Convolution 運算</title>
    <link href="https://yuehhua.github.io/2018/10/17/19-convolution-operation/"/>
    <id>https://yuehhua.github.io/2018/10/17/19-convolution-operation/</id>
    <published>2018-10-17T14:58:29.000Z</published>
    <updated>2018-10-18T13:45:31.881Z</updated>
    
    <content type="html"><![CDATA[<p>熱身運動都做好了，接下來我們就一路往影像處理上的重要技術 CNN 前進啦！</p><p>Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。</p><p>那為什麼 convolution 這麼重要，重要到要放在名稱上呢？</p><a id="more"></a><p>我先推荐大家去看台大李宏毅老師的介紹，看完再繼續往下看文章。</p><iframe width="560" height="315" src="https://www.youtube.com/embed/FrKWiRv254g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><hr><h2 id="影像處理中的祕密"><a href="#影像處理中的祕密" class="headerlink" title="影像處理中的祕密"></a>影像處理中的祕密</h2><p>我們希望從資料當中找到某種規律，這種規律我們叫作模式（pattern），模式是會重複出現的特徵，像是你可以辨識出蘋果，因為一顆蘋果有他特定的特徵，像是顏色、形狀、大小等等，當這些組合起來，並且他們都會一起出現而且重複出現，我們就可以稱他為模式。</p><p>在影片當中有提到在影像處理上有幾個特點：</p><ol><li>一些模式比整張圖小</li><li>同樣的模式可能出現在圖片的不同地方</li><li>縮放圖片不影響圖片中物件的辨識</li></ol><p>第 1 點講的是，要去辨識一個模式並不需要整張圖，也就是，<strong>local 的資訊比 global 的資訊重要</strong>。在影片中有舉例，一隻鳥的特徵有鳥喙、羽毛、翅膀等等特徵，你並不會去在意他背景的圖片長什麼樣子。鳥喙這樣的特徵他是 <strong>區域性的</strong>，你不需要整張圖片的資訊去判斷這張圖是不是鳥喙，所以在設計模型的原則上需要去擷取區域性的資訊。</p><p>第 2 點講的是，同樣的模式可能會出現在不同圖片的不同地方，這邊其實隱含了一個概念，就是 <strong>位移不變性（translation invariance）</strong>。由於同樣模式可以在不同地方上被找到，所以我們只需要一個 node 去偵測他就好了 ，這樣的話可以節省非常多的 node（或是 weight），這稱為 shared weight。</p><p>第 3 點，如果圖片縮放不影響圖片辨識，那麼。這時候我們可以做 subsampling，除了可以減少資料處理的量，也不會影響圖片的辨識結果。</p><hr><p>在我們講 CNN 之前先來幫大家惡補一下 convolution 這個運算。</p><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>在 CNN 中會用到的重要運算就是 convolution，而在 CNN 中有所謂的 convolution layer，也就是這一層的運算就是做 convolution 而不是直接內積。</p><p>我們說到 convolution layer，他真正的作用是用來做什麼的呢？他其實是用來 <strong>擷取 local 的資訊</strong> 的。</p><p>承接前面第一點提到的，在圖片當中，pattern 是 local 的資訊而不是 global 的，而 pattern 是我們想抓的資訊，所以我們要的資訊只有 local 的而已。</p><p>那麼 convolution 要如何擷取 local 的資訊呢？</p><h2 id="Convolution-運算"><a href="#Convolution-運算" class="headerlink" title="Convolution 運算"></a>Convolution 運算</h2><p>我們先來看 convolution 的原始定義，這邊假設兩個函數 $f$、$g$，則兩個函數的 convolution 為：</p><p>$$<br>(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d\tau<br>$$</p><p>以上我們看到他是一個積分式，當中引入了另一個變數 $\tau$，他代表的是<em>一個時間區間</em>，那接下來是兩個函數的<em>相乘</em>，然後將他們對變數 $\tau$ <em>積分</em>。我們來想想看，先不管變數 $\tau$，相乘後積分的運算跟什麼樣的運算很像？</p><p>是的！內積！我們來看看函數的內積長什麼樣子。</p><p>$$<br>\lt f, g \gt = \int_{-\infty}^{\infty} f(t) g(t) dt<br>$$</p><p>什麼？你跟我說這不是你認識的內積？不不不，你認識的內積其實是這個內積的離散版本。</p><p>$$<br>\lt f, g \gt = \sum_{i=1}^{n} f_i g_i<br>$$</p><p>$$<br>&lt;a, b&gt; = \sum_{i=1}^{n} a_i b_i = \mathbf{a}^T\mathbf{b}<br>$$</p><p>這樣是不是比較清楚一點了？我們來比較一下，因為積分是在 <strong>連續空間的加總</strong>，相對應的加總就是在 <strong>離散空間</strong> 的版本，那麼在連續空間上需要一個 $d\tau$ 來把連續空間切成一片一片的，但是在離散空間上，他很自然的就是 $1$ 了。這樣是不是又發覺他們根本是一樣的呢？</p><p>那你知道函數其實是一種向量嗎？不知道的同學表示沒有讀過或是沒有認真讀線性代數。</p><p>那這樣大家應該接受了函數的內積以及向量的內積其實是一樣的。接下來我們來討論一下那個神奇的 $\tau$。</p><p>$\tau$ 是一個時間區間，而積分其實是在對這個時間區間做切片然後加總，他其實跟我們在做訊號處理上的 window 的概念是一樣的，所以他其實是在某個 window 中做內積的意思。我們先來看看有 window 的內積長什麼樣子。</p><p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n + m]<br>$$</p><p>在下圖我們可以假想左邊的向量是 $b$，右邊的是 $a$，而向量 $a$ 是有被 window 給限定範圍的（m = 1…k），所以在下面這張圖就是當 n = 1、m = 1…4 的時候的情境。箭頭則是向量元素相乘的對象，每次內積完，n 就會往下移動一個元素。</p><p><img src="/images/ccor1d.svg" alt=""></p><p>計算完之後就變成一個新的向量，這就是 window 版本的內積的運作原理了！他其實有一個正式的名字，稱為 cross-correlation。</p><p>我們來看看把 convolution 離散化來看看是長什麼樣子。剛剛我們看到的 convolution 是連續的版本，是函數的版本，那我們實際上的運算是以向量去操作的，那麼離散版本的 convolution 是：</p><p>$$<br>(a * b)[n] = \sum_{m=-\infty}^{\infty} a[m] b[n - m]<br>$$</p><p>這邊的 window 就是 $m$ 這個參數，其實我們可以給他一個區間，不要是負無限大到正無限大。</p><p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n - m]<br>$$</p><p>所以這邊的 window 大小調成是 $k$ 了！</p><p><img src="/images/conv1d.svg" alt=""></p><p>你會發現，convolution 會跟 cross-correlation 很像，差別在於順序，也就是 convolution 內積的順序是相反的，所以他在數學式上的表達是用相減的，這邊的情境是 n = 6、m = 1…4。</p><p>我們來總結一下 convolution 這個運算，他其實是 local 版本的內積運算，而且他的內積方向是反序的。</p><p>補充一點，convolution 其實也是一種線性的運算喔！是不是跟前面談到的線性模型 $\sigma(W^T\mathbf{x} + \mathbf{b})$ 有點異曲同工的感覺呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;熱身運動都做好了，接下來我們就一路往影像處理上的重要技術 CNN 前進啦！&lt;/p&gt;
&lt;p&gt;Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。&lt;/p&gt;
&lt;p&gt;那為什麼 convolution 這麼重要，重要到要放在名稱上呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>18 Multi-layer preceptron</title>
    <link href="https://yuehhua.github.io/2018/10/17/18-multi-layer-preceptron/"/>
    <id>https://yuehhua.github.io/2018/10/17/18-multi-layer-preceptron/</id>
    <published>2018-10-17T14:58:18.000Z</published>
    <updated>2018-10-17T14:58:18.099Z</updated>
    
    <content type="html"><![CDATA[<p>我們來更具體一點講 multi-layer perceptron (MLP)。</p><p>最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。</p><a id="more"></a><hr><p>一般線性模型：$f(\mathbf{x}) = W^{\prime T}\mathbf{x} + b = W^T\mathbf{x}$</p><p>Linear MLP：</p><p>$f_1(\mathbf{x}) = W_1^T\mathbf{x}$</p><p>$f_2(\mathbf{x}) = W_2^Tf_1(\mathbf{x})$</p><p>$f_3(\mathbf{x}) = W_3^Tf_2(\mathbf{x})$</p><p>…</p><p>$f_n(\mathbf{x}) = W_n^Tf_{n-1}(\mathbf{x})$</p><hr><p>那這樣這個有什麼好講的呢？</p><p>大家應該有看到在這邊唯一的運算：內積（inner product）</p><h2 id="內積的意義"><a href="#內積的意義" class="headerlink" title="內積的意義"></a>內積的意義</h2><p>有念過線性代數的人應該對內積這個運算還算熟悉（在這邊都假設大家有一定線性代數基礎）。</p><p>$$<br>&lt;\mathbf{x}, \mathbf{y}&gt; = \mathbf{x}^T \mathbf{y}<br>$$</p><p>$$<br>= \begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}^T</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}</p><p>=<br>\begin{bmatrix}<br>x_1, x_2, \cdots, x_n<br>\end{bmatrix}</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p><p>內積，要先定義矩陣相乘的運算，而矩陣的相乘其實是一種線性轉換。</p><p>$$<br>f(\mathbf{x}) = A\mathbf{x}<br>$$</p><p>我們來觀察一下內積這個運算，這兩個向量會先把相對應的分量相乘。</p><p>$$<br>\begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}</p><p>\leftrightarrow</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p><p>接著，再相加。</p><p>$$<br>x_1y_1 + x_2y_2 + \cdots + x_ny_n<br>$$</p><p>這時候我們可以想想看，如果當一邊是權重另一邊是資料的時候所代表的意義是什麼？</p><p>當兩個分量的大小都很大的時候，相乘會讓整個值變很大，相對，如果兩個都很接近零的話，結果值就不大。如果很多分量乘積結果都很大，相加會讓整體結果變得很大。</p><p>內積，其實隱含了 <strong>相似性</strong> 的概念在裡面，也就是說，如果你的權重跟資料很匹配的話，計算出來的值會很大。大家有沒有從裏面看出些端倪呢？</p><p>我們再由另一個角度切入看內積，內積我們可以把他寫成另一種形式，這個應該在大家的高中數學課本當中都有：</p><p>$$<br>\mathbf{x}^T \mathbf{y} = ||\mathbf{x}|| ||\mathbf{y}|| cos \theta<br>$$</p><p>這時候我們就可以看到內積可以被拆成3個部份：分別是兩個向量的大小跟向量夾角的 $cos \theta$ 值。</p><p>而當中 $cos \theta$ 就隱含著相似性在裡頭，也就是說，當兩個向量的夾角愈小，$cos \theta$ 會愈接近 1。相反，如果兩個向量夾角愈接近 180 度，那 $cos \theta$ 會愈接近 -1。剛好呈現 90 度就代表這兩個向量是 <strong>沒有關係的</strong>。</p><p>這時候可能有人會說內積又不是完全反應相似性而已，沒錯！因為他也考慮了兩個向量的長度，當一組向量夾角與另一組向量夾角相同，但是第1組的向量長度都比較長，那內積的結果第1組向量就會比較大。</p><p>所以內積是沒有去除掉向量長度因素的運算，如果單純想要用向量夾角來當成相似性的度量的話可以考慮用 cos similarity。</p><p>$$<br>cos \theta = \frac{\mathbf{x}^T \mathbf{y}}{||\mathbf{x}|| ||\mathbf{y}||}<br>$$</p><h2 id="內積與-MLP"><a href="#內積與-MLP" class="headerlink" title="內積與 MLP"></a>內積與 MLP</h2><p>那 MLP 當中內積扮演了什麼樣的角色呢？</p><p>在純粹線性的 MLP 當中，多層的 $f(\mathbf{x})$ 疊起來，我們可以把他看做是做非常多次的線性轉換或是座標轉換（change of basis），但是這是在 inference 階段的解釋。</p><p>那在 training 階段內積扮演了什麼樣的角色呢？</p><p>這邊提供一個新的想法：在 training 的過程中，我們的 dataset 是不變的，會變動的是 weight ，而內積則是在衡量這兩者之間的 feature norm 及向量夾角，所以 weight 會調整成匹配這樣特性的樣子。換句話說，內積考慮了 data 與 weight 之間的相似性與大小，並且藉由 training 去調整 weight 讓他與資料匹配。</p><p>在 inference 階段，你就可以把他看成是，weight 正在幫你做出某種程度的篩選，跟 weight 匹配的資料，內積值就會比較大，相對的是，weight 不匹配的資料，內積值就會比較小，藉由這樣將內積結果遞進到下一層的運算。</p><h2 id="機率與內積"><a href="#機率與內積" class="headerlink" title="機率與內積"></a>機率與內積</h2><p>其實還有一個觀點，就是機率觀點，機率要求一個 distribution 的長度為 1，$\int_{-\infty}^{\infty} P(X) = 1$。在這邊我們的 distribution 常常以一個 vector（或是 random variable）的形式呈現。事實上就是把一個計算好的向量去除以他的長度。如此一來，我們就去除了長度影響的因素，以符合機率的要求。</p><p>那機率當中的內積指的是什麼呢？</p><p>你如果動動手 google 一下就會發現在機率當中的內積就是這個運算</p><p>$$<br>\mathbb{E}[XY] = \int XY dP<br>$$</p><p>如果有念過統計的人，是不是覺得這東西很眼熟呢？</p><p>$$<br>cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]<br>$$</p><p>是的！他跟共變異數是有相關的，共變異數還是跟我們要去度量兩個隨機變數之間的 <strong>相似性</strong> 有關係。</p><p>$$<br>\rho = \frac{cov(X, Y)}{\sigma_X \sigma_Y}<br>$$</p><p>只要把他除以隨機變數的標準差就可以得到相關係數了呢！</p><h2 id="加入非線性"><a href="#加入非線性" class="headerlink" title="加入非線性"></a>加入非線性</h2><p>事實上，在我們生活中遇到的事物都是非線性的居多，線性模型可以施展手腳的範疇就不大了。</p><p>這時我們就希望在 MLP 中加入非線性的元素以增加模型的表達力。這時候模型的每一層就變成了：</p><p>$$<br>f(\mathbf{x}) = \sigma (W^T \mathbf{x})<br>$$</p><p>而當中的 $\sigma$ 就成了我們的 activation function 了，也就是非線性的來源！</p><h2 id="Fully-connected-layer"><a href="#Fully-connected-layer" class="headerlink" title="Fully connected layer"></a>Fully connected layer</h2><p>當這些層的 node 都互相連接，就代表了所有 node 都參與了計算，這個計算所考慮的資料是 <strong>global</strong> 的。</p><p>這些層所做的運算是相對 <strong>簡單</strong> 的（相對 convolution 來說）。</p><p>每個 node 對每一層運算所做的貢獻是 <strong>弱</strong> 的。當一層的 node 數很多，e.g. 上千個 node，每個 node 的運算結果就會被稀釋掉了。即便內積運算有包含個別值的大小的成份在裡頭，當 node 數一多，這樣的影響也會被減弱，剩下的是整體向量與向量之間的相似性。但有一個情況例外，當有 node 的值極大，e.g. $x_i / x_j = 1000$，當有人是別人的千倍以上的話就要注意一下了，這也是很常在機器學習當中會遇到的問題，這時候就會需要做 normalization 來處理。</p><p>最後提醒，內積的運算中雖然有隱含相似性在其中，但是他 <em>不等同</em> 於 <strong>去計算相似性</strong>。</p><p>今天的討論就到這邊告一個段落，希望在大家思考 deep learning 模型的時候，這些東西有幫上一些忙。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們來更具體一點講 multi-layer perceptron (MLP)。&lt;/p&gt;
&lt;p&gt;最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>17 Autoencoder</title>
    <link href="https://yuehhua.github.io/2018/10/17/17-autoencoder/"/>
    <id>https://yuehhua.github.io/2018/10/17/17-autoencoder/</id>
    <published>2018-10-17T14:36:06.000Z</published>
    <updated>2018-10-19T15:26:36.709Z</updated>
    
    <content type="html"><![CDATA[<p>既然前一篇提到學習特徵是一件重要的事，那麼我們就來講講 autoencoder 吧！</p><p>Autoencoder 就是一個 unsupervised 方法，試圖學習出可以用的特徵。雖然不少人可能會說他是一個壓縮的方法或是一個降維的方法，其實他都是。</p><a id="more"></a><p>Autoencoder 就是希望給一個 input ，經過轉換之後會成為一個新的、維度比較低的向量，並且可以再由這個向量透過轉換還原成原本的 input。既然低維的向量可以還原成 input 的樣子，那代表他含有足夠的資訊可以還原，所以你可以把他看成是一種壓縮或是降維。我想強調的是，autoencoder 可以將一個 input 的樣子轉換成一個新的表示方式，我們就可以用新的視角來看待他。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>Autoencoder 算是一個非常簡單的模型，他就是將資料向量經過一個轉換過後成為新的向量，在讓新的向量經過轉換過後還原成原本的向量。</p><p>抽象上來說比較像是：</p><p>$$<br>X \rightarrow Z \rightarrow X<br>$$</p><p>Autoencoder 可以被拆解成兩個部份：一個是 encoder $X \rightarrow Z$，另一個是 decoder $Z \rightarrow X$ 的部份。</p><p>在這個架構上 encoder-decoder 是相接在一起做訓練的。</p><p>最簡單的 autoencoder 在 encoder 跟 decoder 上各自都只有一層，當然你可以不只一層，取決於你自己的狀況。</p><p>他是一個非常簡單的架構，所以有非常多的變體，像是 denoising autoencoder、sparse autoencoder、variational autoencoder…等等。</p><p>每個都有自己獨特的用法，所以這是一個非常基礎的模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;既然前一篇提到學習特徵是一件重要的事，那麼我們就來講講 autoencoder 吧！&lt;/p&gt;
&lt;p&gt;Autoencoder 就是一個 unsupervised 方法，試圖學習出可以用的特徵。雖然不少人可能會說他是一個壓縮的方法或是一個降維的方法，其實他都是。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>16 Representation learning</title>
    <link href="https://yuehhua.github.io/2018/10/16/16-representation-learning/"/>
    <id>https://yuehhua.github.io/2018/10/16/16-representation-learning/</id>
    <published>2018-10-16T15:28:04.000Z</published>
    <updated>2018-10-16T15:32:43.024Z</updated>
    
    <content type="html"><![CDATA[<p>機器學習的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。</p><p>當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個成效，像是你給的特徵不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些資料當中，那麼模型根本就預測不出來，所以玩過機器學習的人就會知道特徵工程的重要性。</p><a id="more"></a><p>以往特徵工程是需要人自己手動處理的，如今我們也希望由機器學習的模型中自動學出來。大家可以看到我們的技術進展：從以往的手寫程式進展到經典的機器學習技術，這是一個巨大的飛躍。</p><p><img src="/images/diagram-deep-learning.png" alt=""></p><blockquote><p>From <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?ie=UTF8&amp;qid=1472485235&amp;sr=8-1&amp;keywords=deep+learning+book" target="_blank" rel="noopener"><em>Deep Learning</em></a> by Ian Goodfellow and Yoshua Bengio and Aaron Courville</p></blockquote><hr><p><br><br></p><h2 id="他幫我們解決了什麼問題呢？"><a href="#他幫我們解決了什麼問題呢？" class="headerlink" title="他幫我們解決了什麼問題呢？"></a>他幫我們解決了什麼問題呢？</h2><p>以往的手寫程式需要工程師非常的聰明，他需要知道在 input 與 output 之間的所有規則，然後把這些規則化成可以執行的程式，這些實作的過程需要花非常大量的人力跟腦力。</p><p><br></p><p><img src="/images/before_ml.svg" alt=""></p><p><br></p><p>然而，我們進展到機器學習的技術，我們試圖去收集一些資料，這些資料符合我們預期的 input 與 output 之間的關係。</p><p><br></p><p><img src="/images/after_ml.svg" alt=""></p><p><br></p><p>他可以幫我們將中間的 <strong>過程</strong> 連接起來，我們不需要去 <em>手刻</em> 或是 <em>事先知道</em> 這些過程，更何況自然界很多過程都是 <strong><em>人類沒辦法理解的</em></strong> 或是 <strong><em>還不知道的</em></strong>。</p><p><br></p><p><img src="/images/mnist.svg" alt=""></p><p><br></p><p>這些過程在數學家的眼中就稱為 <strong>函數</strong>，對於機器學習專家來說，input 與 output 之間有無限多種函數的可能。哪一種可能才是最符合我們資料的長相的？我們希望挑出最有可能的那一種，就把那就把那一種當成是模型，並且輸出，這樣我們就能讓機器自動去學出 input 與 output 的對應關係，這是一個飛躍性的進展。</p><p><br></p><h2 id="特徵工程（feature-engineering）"><a href="#特徵工程（feature-engineering）" class="headerlink" title="特徵工程（feature engineering）"></a>特徵工程（feature engineering）</h2><p>接著我們意識到：我們還是需要手動去處理特徵。經典的機器學習模型只幫我們處理了 <strong>將特徵對應到輸出的關係</strong>，我們還是得藉由特徵萃取的技術來轉換，而我們很難知道什麼樣的特徵萃取才真正能夠把資料中我們想要的資訊萃取出來，這部分就進到 representation Learning 的範疇。</p><p><br></p><h2 id="自動化特徵萃取（Automatic-feature-extraction）"><a href="#自動化特徵萃取（Automatic-feature-extraction）" class="headerlink" title="自動化特徵萃取（Automatic feature extraction）"></a>自動化特徵萃取（Automatic feature extraction）</h2><p>在特徵萃取的過程中，常常我們面對的是高維度的向量，由於我們很難去理解高維度的向量之間的轉換，導致我們在轉換的時候會遇上困難，我們根本不知道需要轉換成什麼樣維度的向量，我們也不知道中間需要什麼樣的轉換函數。在數學領域當中，有相關的領域稱為微分幾何，所以常常我們會討論在數學上的流形（manifold），representation learning 就是希望連同特徵萃取以及模型可以一併處理，也就是藉由模型的過程會到回饋（從 gradient descent 等等方法），去引導特徵萃取的過程，進而去學到 <strong>特徵-特徵</strong> 之間轉換的模式。</p><p><br></p><h2 id="深度學習"><a href="#深度學習" class="headerlink" title="深度學習"></a>深度學習</h2><p>深度學習就是一種 representation Learning。他希望資料在高維度的轉換當中，可以去萃取出足夠而抽象的資訊，去進行預測。而深度學習只是將特徵-特徵之間的轉換模式以 <strong>層-層</strong> 之間的轉換實現，而高維的特徵向量以 <strong>層</strong> 的形式呈現。所以越深的網路代表著經過多次的函數處理跟萃取，所萃取的資訊的抽象程度越高，抽象程度越高，就越接近人類所想像的。</p><p><br></p><h2 id="Representation-learning"><a href="#Representation-learning" class="headerlink" title="Representation learning"></a>Representation learning</h2><p>如同前面描述到的，我們需要更少對於特徵工程的依賴，增加自動化特徵萃取的使用。所以我們用”學習”的方式讓模型自動去學到他要的特徵，自動去做特徵萃取。那麼 representation learning 更深層的意思是什麼呢？</p><blockquote><p>你需要的不是一個答案，而是一個表示方式。</p></blockquote><p>以上是我在工研院的課程對學員們講過的話，一句話解釋 representation learning 大概是這個意思。</p><p>舉個例子好了，人類在照片中可以辨識出當中的狗狗，人們在交談的時候可以以語言的”狗”來描述同一個概念，基本上他們都擁有相同的資訊量。對於狗的概念來說，照片中的圖像只是這個概念的一種表示方式，語言中也有對應的表現方式。讓機器學會狗的概念，就是要讓他可以從圖像或是語言中可以萃取出有相同資訊量的東西，這樣的東西可以是以資料結構的方式表示，或是以一個向量表示，所以你需要的不是一個答案，而是一種表示方式，讓你可以看的懂的表示方式。</p><p>最終極的情況來說，在人類腦中很多既定的概念都已經存在，並沒有什麼新的概念出現了。出現的只有新的概念以不同的形式或是姿態出現，這互相之間都是可以轉換的，當然，轉換伴隨著資訊量的流失。</p><p>記得有個實驗在測試在發展成熟的社會是不是比原始社會的人們更聰明，實驗者設計了類似配對記憶遊戲，在卡牌上畫上各種現代日常生活中會看到的物件，分別測試了都市的人們以及原始部落的人們，果然在都市的人們有比較好的成績。不過這個實驗有個為人詬病的地方，都市人當然熟悉日常生活周遭的物件，原始部落的人們卻從來也沒有看過這些東西。所以又有另一組人馬將實驗換成在原始部落中常常看到的植物的葉子，對都市的人們來說，那些葉子根本無從分辨，但是原始部落的人們的測驗結果卻跟都市的人們對日常生活物件有一樣高的分數。代表人的腦袋並沒有差別，只是認得的東西不同。</p><p>或許在人們的腦袋中，有某些概念是一樣的，但是有不同的表現形式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;機器學習的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。&lt;/p&gt;
&lt;p&gt;當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個成效，像是你給的特徵不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些資料當中，那麼模型根本就預測不出來，所以玩過機器學習的人就會知道特徵工程的重要性。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>15 為什麼要深？</title>
    <link href="https://yuehhua.github.io/2018/10/15/15-why-deep/"/>
    <id>https://yuehhua.github.io/2018/10/15/15-why-deep/</id>
    <published>2018-10-15T06:56:39.000Z</published>
    <updated>2018-10-15T07:03:37.605Z</updated>
    
    <content type="html"><![CDATA[<p>接著我們就來到了蠻重要的問題，既然一個 hidden layer 的網路架構就可以逼近任何連續函數，那麼為什麼要深度學習？</p><a id="more"></a><p>對於這個問題，台大李宏毅老師有非常詳細的講解：</p><iframe width="560" height="315" src="https://www.youtube.com/embed/FN8jclCrqY0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>影片裏面實驗很多，所以我還是總結一下：</p><ol><li>用邏輯電路類比神經網路的話，你一樣可以用單層的並聯將所有邏輯閘都起來成為一個電路，一樣可以達到相同的效果，但是用多層的串聯可以將所需要的邏輯閘數目減少（類比神經網路的神經元），所以可以達到減少參數的效果。</li><li>使用 ReLU 作為 activation function 就是要用分段線性的方式來逼近一個函數，在網路參數相進的情況下，單層網路所能產生的”段”比較少，多層網路所產生的”段”比較多，產生的線段較多就可以去逼近一個更複雜的函數，所以模型就比較強大。</li><li>計算分段線性的數量，當你有 $N$ 個神經元，單層網路裡最多只能產生 $N - 1$ 個線段，多層網路，每層安排兩個神經元，可以產生 $2^{\frac{N}{2}}$ 個線段。</li></ol><p>更有文獻提到，計算線段的數量，如果你的網路每層有 $K$ 個神經元，而且有 $H$ 層，那麼至少會有 $K^H$ 個線段。</p><p>由於深度是放在指數上面，所以增加深度就可以簡單地提高模型的複雜度，也就可以讓模型變得比較強大。</p><h2 id="計算複雜度"><a href="#計算複雜度" class="headerlink" title="計算複雜度"></a>計算複雜度</h2><p>在一些電腦的問題上，我們常常形容問題是 NP 或者不是 NP，來描述一個問題的複雜度有多高。</p><p>一個問題的時間複雜度可以在多項式時間內的，我們稱為 P，如果不是，那我們稱為 NP。</p><p>NP-complete 問題是 NP 問題當中最難的了，計算複雜度大概會是指數級成長，這種成長速度應該使用神經網路有辦法克服。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接著我們就來到了蠻重要的問題，既然一個 hidden layer 的網路架構就可以逼近任何連續函數，那麼為什麼要深度學習？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>14 淺層神經網路</title>
    <link href="https://yuehhua.github.io/2018/10/14/14-shallow-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/14/14-shallow-neural-network/</id>
    <published>2018-10-14T15:16:02.000Z</published>
    <updated>2018-10-15T07:03:50.798Z</updated>
    
    <content type="html"><![CDATA[<p>為什麼大家到現在都這麼迷神經網路模型？</p><p>我想主因不是因為他是模擬生物而來，他有一些更扎實的數學特性。</p><p>我們前面講過各種線性模型，然後將他過渡到神經網路。</p><p>今天要告訴大家，即便是淺層的神經網路也是很厲害的。</p><a id="more"></a><h2 id="Universal-approximation-theorem"><a href="#Universal-approximation-theorem" class="headerlink" title="Universal approximation theorem"></a>Universal approximation theorem</h2><p>Universal approximation theorem 是個淺層的神經網路的數學定理。</p><p>他說：一個簡單的 feedforward network，只包含了一個 hidden layer，並且有適切的 activation function，包含有限個神經元的情況下，可以去逼近任何連續函數。</p><p>在 1989 就以經由 George Cybenko 先生第一次證實了這個定理，他使用了 sigmoid activation function。</p><p>如果大家對細節有興趣，請參閱 <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" target="_blank" rel="noopener">維基百科的條目</a>。</p><p>這代表什麼呢？</p><p>到目前為止，我們都會將一個現實中的問題化成一個數學問題，一個數學問題基本上都是包含函數的。</p><p>像是影像辨識，我們就可以看成一個可以輸入影像的函數，這個函數會輸出辨識結果，也就是分類。</p><p>我們可以這樣寫 $f: \text{images} \rightarrow \text{classes}$。</p><p>所以語音辨識就是 $f: \text{speech} \rightarrow \text{text}$。</p><p>聊天機器人就是 $f: \text{text} \rightarrow \text{text}$。</p><p>…</p><p>到這邊你可以想想，幾乎人類的問題都可以化成一個函數來解答。</p><p>所以可以逼近任何連續函數的模型根本就可以解答任何問題的意思阿！</p><p>所以大家才拼了命的用這個模型去解決很多問題。</p><h2 id="待解問題"><a href="#待解問題" class="headerlink" title="待解問題"></a>待解問題</h2><p>如果這個模型這麼萬能，那麼他就真的沒有缺點嗎？</p><p>我們需要從這個定理切入，定理中描述的有限個神經元，但至少不是無限，他並沒有說需要幾個。</p><p>這理所當然，因為沒有人知道你的問題有多複雜，需要用多難的方法解嘛！</p><p>然後 activation function 也是沒有提的。</p><p>所以這就是留給現代的大家去決定的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;為什麼大家到現在都這麼迷神經網路模型？&lt;/p&gt;
&lt;p&gt;我想主因不是因為他是模擬生物而來，他有一些更扎實的數學特性。&lt;/p&gt;
&lt;p&gt;我們前面講過各種線性模型，然後將他過渡到神經網路。&lt;/p&gt;
&lt;p&gt;今天要告訴大家，即便是淺層的神經網路也是很厲害的。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>13 Kernel SVM 與 RBF network</title>
    <link href="https://yuehhua.github.io/2018/10/14/13-kernel-svm-and-rbf-network/"/>
    <id>https://yuehhua.github.io/2018/10/14/13-kernel-svm-and-rbf-network/</id>
    <published>2018-10-13T16:18:55.000Z</published>
    <updated>2018-10-15T07:08:15.501Z</updated>
    
    <content type="html"><![CDATA[<p>我們前面介紹了線性模型跟基本的神經網路模型。</p><p>可能有的人會覺得我怎麼不放神經網路的圖，看數學式子看的很痛苦。</p><p>是的，我的確沒打算放圖。一來神經網路的圖在各大網站或是 google 上遍地都是我實在沒有必要再放一張，二來因為這個模型的核心根本不是哪些圖，那些圖只是幫助理解，理解之後就都是看數學式了，再回去看圖就太小兒科了。</p><p>神經網路的概念在於將多個模型串接起來，也就是前面提到的堆疊的概念。</p><a id="more"></a><h2 id="處理流程"><a href="#處理流程" class="headerlink" title="處理流程"></a>處理流程</h2><p>堆疊的概念其實跟傳統的機器學習處理流程有點像。</p><p>機器學習的處理流程大概就跟做料理很像。</p><p>首先，你需要先買菜（蒐集資料），然後是備料（資料前處理）。備料的動作其實很不一樣，依據你要煮的料理（機器學習模型）是什麼而有所區別，該是切塊、切條、切絲，還是切丁（特徵離散化）？肉該不該先醃過（特徵轉換）？</p><p>如果你想呈現的是一道味道一體呈現的料理，而不是肉是肉、菜是菜，味道都各自獨立，你是不是該在一些烹調方式或是前處理上讓味道融為一體？（特徵正規化）（謎：又不是在吃沙拉，還味道各自分離。）</p><p>等料都備好了之後，就是重點的料理部份。料理方式要用煎煮炒炸哪一種（定義監督式、非監督式學習），然後整體的食譜（機器學習模型）是什麼？像是典型的紅酒燉牛肉就是經典食譜（很多人用的模型），當然你可以根據自己的喜好修改成自己的版本（改模型架構），不過大多數人怕失敗，所以都去找了電視上的或是名主廚的食譜（市面上常看到的套件，像 scikit-learn）。</p><p>料理好了之後就是要排盤（成果展現）啦！你總不可能紅酒燉牛肉做好之後整鍋端到餐桌上去，一定是要做些裝飾跟點綴（資料視覺化）。那些就是主廚（資料科學家或 AI 工程師）想要呈現給你的客人（通常是老闆）的東西，那除了嗅覺跟味覺，還有視覺上的效果。整體說來，需要營造的是一個氛圍或是體驗（老闆的感覺）。</p><p>身為一個主廚必須在各個小地方或是細節用心，才能拿到米其林指南推荐的殊榮（KDD 或 kaggle 競賽冠軍）。</p><p>好像有點離題了……</p><p>總之，這些步驟都是環環相扣的，而且需要從最前端串接到最後的。模型的堆疊也是做類似的事情，希望可以把前處理、料理等等步驟都串接起來成為一個模型，所以以往的機器學習 pipeline 就演化成神經網路模型了。</p><h2 id="RBF-network"><a href="#RBF-network" class="headerlink" title="RBF network"></a>RBF network</h2><p>我們回到今天的主題來，像前面我們談過 SVM 是個很厲害的分類器，主要是引進了 kernel 讓這個模型可以做非線性的處理。</p><p>那麼 kernel 能不能被放到神經網路裡呢？</p><p>其實是可以的，應該說，有一種網路模型稱為 Radial basis function network（RBF network），他其實就很像是 Gaussian kernel SVM。</p><p>我們來看看前面的 kernel SVM 模型：</p><p>$$<br>SVM(\mathbf{x}) = sign(\sum_{SV} \alpha_n y_n K(\mathbf{x}, \mathbf{x_n}) + b)<br>$$</p><p>Gaussian kernel，或是稱 RBF function：</p><p>$$<br>K(\mathbf{x}, \mathbf{x_n}) = exp(\gamma ||\mathbf{x} - \mathbf{x_n}||^2) = RBF(\mathbf{x}, \mathbf{x_n})<br>$$</p><p>那麼 RBF network 是長什麼樣子呢？</p><p>$$<br>y = \sum_{n} w_n RBF(\mathbf{x}, \boldsymbol{\mu_n})<br>$$</p><p>示意圖的話是這樣：</p><p><img src="/images/rbf_network.svg" alt=""></p><p>你可以將 $RBF(\mathbf{x}, \boldsymbol{\mu_n})$ 看成第一層，就是在計算兩個向量之間的距離，或是反過來說叫作相似度。</p><p>第二層則是做一個線性組合，這邊就很像神經網路的一層。</p><p>我們其實可以將上面的式子擴展成：</p><p>$$<br>y = \sigma(\sum_{n} w_n RBF(\mathbf{x}, \boldsymbol{\mu_n}) + b)<br>$$</p><p>或是寫成：</p><p>$$<br>y = \sigma(\mathbf{w}^T RBF(\mathbf{x}, \boldsymbol{\mu}) + b)<br>$$</p><p>我們可以將第二層變成一個神經網路的層，還包含有 activation function，所以這樣 kernel 也變成一層了。</p><p>RBF network 一開始是設計用來做函數內插的，就是有一些資料點 $\mathbf{x}$，我希望可以由這些資料來幫我們找到一條平滑的函數，愈密集的地方是愈有可能是線經過的。</p><p>今天的部份就到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們前面介紹了線性模型跟基本的神經網路模型。&lt;/p&gt;
&lt;p&gt;可能有的人會覺得我怎麼不放神經網路的圖，看數學式子看的很痛苦。&lt;/p&gt;
&lt;p&gt;是的，我的確沒打算放圖。一來神經網路的圖在各大網站或是 google 上遍地都是我實在沒有必要再放一張，二來因為這個模型的核心根本不是哪些圖，那些圖只是幫助理解，理解之後就都是看數學式了，再回去看圖就太小兒科了。&lt;/p&gt;
&lt;p&gt;神經網路的概念在於將多個模型串接起來，也就是前面提到的堆疊的概念。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>12 從線性模型到神經網路</title>
    <link href="https://yuehhua.github.io/2018/10/12/12-from-linear-model-to-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/12/12-from-linear-model-to-neural-network/</id>
    <published>2018-10-12T15:46:06.000Z</published>
    <updated>2018-10-15T07:04:53.018Z</updated>
    
    <content type="html"><![CDATA[<p>我們把線性模型們都大統一了。</p><p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y] \leftrightarrow \eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>接下來就要進入到令人興奮的神經網路模型了！</p><a id="more"></a><p>首先，我們先來介紹著名的感知器…嗯…前面不是介紹過了？</p><p>喔喔！對喔！他長這個樣子：</p><p>$$<br>y = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p><p>其中 $\mathbf{w}^T\mathbf{x} + b$ 是我們熟悉的線性模型，然後 $\sigma$ 就是所謂的 activation function。</p><p>不覺得這看起來跟上面的很相似嗎？</p><p>我們動點手腳：</p><p>$$<br>\sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>是的！發現了嗎？其實 $\sigma^{-1}$ 就是在廣義線性模型裡的鏈結函數阿！他會是 activation function 的反函數！</p><p>這樣是不是又了結了一樁心事了呢？</p><h2 id="堆疊"><a href="#堆疊" class="headerlink" title="堆疊"></a>堆疊</h2><p>在神經網路當中，我們會把一個一個神經元並排起來，數學上看起來就是把預測單一個 y 擴張成多個維度：</p><p>$$<br>\mathbf{y} = \sigma(W^T\mathbf{x} + \mathbf{b})<br>$$</p><p>所以在權重 W 的部份也隨之從一個向量擴張成一個矩陣，b 的部份也是，可以自己驗算看看。</p><p>但是預測多維向量並不是讓模型強大的地方，讓模型強大是因為把很多個這樣的模型頭尾接起來。</p><p>$$<br>\mathbf{x} \overset{f^{(0)}}{\longrightarrow} \mathbf{h}^{(1)} \overset{f^{(1)}}{\longrightarrow} \dots \overset{f^{(k-1)}}{\longrightarrow} \mathbf{h}^{(k)} \overset{f^{(k)}}{\longrightarrow} \mathbf{y}<br>$$</p><p>當中的這些函數們 $f^{(k))}$ 就是我們說的層。</p><p>$$<br>\mathbf{h}^{(k)} = f^{(k))}(\mathbf{h}^{(k-1)}) = \sigma(W^T\mathbf{h}^{(k-1)} + \mathbf{b})<br>$$</p><p>神經網路模型之所以強大的原因是因為將模型頭尾相接，並不是因為他是模擬生物系統，只是靈感是從生物系統上得來的而已。</p><p>搭配上 activation function 的非線性轉換，就可以模擬很多非線性的現象。</p><table><thead><tr><th style="text-align:center">model</th><th style="text-align:center">link function</th><th style="text-align:center">activation function</th></tr></thead><tbody><tr><td style="text-align:center">linear regression</td><td style="text-align:center">identity function: $y = x$</td><td style="text-align:center">identity function: $y = x$</td></tr><tr><td style="text-align:center">logistic regression</td><td style="text-align:center">logit function: $y = \frac{x}{1-x}$</td><td style="text-align:center">sigmoid function: $y = \frac{1}{1 + e^{-x}}$</td></tr><tr><td style="text-align:center">Poisson regression</td><td style="text-align:center">log function: $y = ln(x)$</td><td style="text-align:center">exponential function: $y = exp(x)$</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們把線性模型們都大統一了。&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;y \overset{f}{\longleftrightarrow} \mathbb{E}[y] \leftrightarrow \eta = \mathbf{w}^T\mathbf{x} + b&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;接下來就要進入到令人興奮的神經網路模型了！&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>11 廣義線性模型</title>
    <link href="https://yuehhua.github.io/2018/10/11/11-generalized-linear-model/"/>
    <id>https://yuehhua.github.io/2018/10/11/11-generalized-linear-model/</id>
    <published>2018-10-11T05:16:28.000Z</published>
    <updated>2018-10-15T07:04:40.296Z</updated>
    
    <content type="html"><![CDATA[<p>我們前面探討了不同的資料型態可以對應不同的迴歸模型。</p><p>不覺得每個迴歸模型都有那麼點相似的地方嗎？</p><a id="more"></a><p>線性迴歸：</p><p>$$<br>\mathbb{E}[y] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>羅吉斯迴歸：</p><p>$$<br>ln(\frac{\mathbb{E}[y]}{1 - \mathbb{E}[y]}) = ln(\frac{p}{1 - p}) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>Poisson 迴歸：</p><p>$$<br>ln(\mathbb{E}[y]) = ln(\lambda) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>在右手邊的部份都是一樣的，是一樣的線性組合加上一個常數。</p><p>差別在於預測出來的數值是怎麼連結到目標變量的平均值上 $\mathbb{E}[y]$。</p><p>是的，我們在預測的都是目標變量的平均值。</p><h2 id="鏈結函數（link-function）"><a href="#鏈結函數（link-function）" class="headerlink" title="鏈結函數（link function）"></a>鏈結函數（link function）</h2><p>要連結目標變量的平均值 $\mathbb{E}[y]$ 跟線性組合加上一個常數…..，姑且叫他 $\eta$ 好了。</p><p>$$<br>\mathbb{E}[y] \leftrightarrow \eta<br>$$</p><p>統計學家發展出使用鏈結函數來連結這兩者，所以不同的資料型態會對應不同的鏈結函數。</p><p>線性迴歸使用 identity function $y = x$：</p><p>$$<br>\mathbb{E}[y] = \eta<br>$$</p><p>羅吉斯迴歸使用 logit function $y = ln(\frac{x}{1 - x})$：</p><p>$$<br>ln(\frac{\mathbb{E}[y]}{1 - \mathbb{E}[y]}) = \eta<br>$$</p><p>Poisson 迴歸使用 log function $y = ln(x)$：</p><p>$$<br>ln(\mathbb{E}[y]) = \eta<br>$$</p><h2 id="廣義線性模型（generalized-linear-model）"><a href="#廣義線性模型（generalized-linear-model）" class="headerlink" title="廣義線性模型（generalized linear model）"></a>廣義線性模型（generalized linear model）</h2><p>這麼一來我們就可以把三個模型搓一搓做成 <del>撒尿牛丸</del> 廣義線性模型啦！</p><p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y] \leftrightarrow \eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>對應不同的目標變量，我們就有了萬用的模型，就像物理的大一統理論一樣。</p><p>廣義線性模型其實包含了三個部份：</p><ol><li>鏈結函數</li><li>線性預測子</li><li>指數族</li></ol><h2 id="線性預測子（linear-predictor）"><a href="#線性預測子（linear-predictor）" class="headerlink" title="線性預測子（linear predictor）"></a>線性預測子（linear predictor）</h2><p>統計學家特別給了一個線性預測子這樣的名字。</p><p>$$<br>\eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>這代表要從預測變量 $\mathbf{x}$ 去預測我們的目標變量，其中 $\mathbf{x}$ 的變數之間都是 <strong>互相獨立</strong> 的。</p><p>互相獨立的變數之間，要以 <strong>線性組合</strong> 來預測我們的目標變量。</p><h2 id="指數族（exponential-family）"><a href="#指數族（exponential-family）" class="headerlink" title="指數族（exponential family）"></a>指數族（exponential family）</h2><p>可是每一種資料的機率分佈都可以接上廣義線性模型嗎？答案是否定的。</p><p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y]<br>$$</p><p>統計學家研究了一下這個模型，發現只有符合指數族的條件才能夠用。</p><p>指數族長成這樣：</p><p>$$<br>f(\mathbf{y} \mid \boldsymbol{\theta}) = \frac{1}{Z(\boldsymbol{\theta})} h(\mathbf{y}) exp(\boldsymbol{\theta}^T \phi(\mathbf{y}))<br>$$</p><p>$\boldsymbol{\theta}$ 是機率分佈的期望值，或是稱為 natural parameter。</p><p>$\phi(\mathbf{y})$ 是 sufficient statisitcs，這邊有非常多有趣的東西，不過也有點理論。</p><p>$Z(\boldsymbol{\theta})$ 稱為 partition function，是機率分佈的分母，常常會在不同的領域見到他，像是物理。</p><p>$h(\mathbf{y})$ 就是個縮放因子，沒什麼重要性，常常是 1。</p><p>我知道大家可能會有很多疑問，但是礙於篇幅，我就不再繼續介紹下去了，這邊下去又是統計所一門課了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們前面探討了不同的資料型態可以對應不同的迴歸模型。&lt;/p&gt;
&lt;p&gt;不覺得每個迴歸模型都有那麼點相似的地方嗎？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>10 從線性迴歸到 Poisson 迴歸</title>
    <link href="https://yuehhua.github.io/2018/10/10/10-from-linear-regression-to-poisson-regression/"/>
    <id>https://yuehhua.github.io/2018/10/10/10-from-linear-regression-to-poisson-regression/</id>
    <published>2018-10-10T14:56:34.000Z</published>
    <updated>2018-10-15T07:04:28.010Z</updated>
    
    <content type="html"><![CDATA[<p>上次我們講完了線性迴歸跟羅吉斯迴歸的差異。</p><p>可是並不是每一種資料都是連續型的或是類別型的。</p><p>這次要來介紹 Poisson 迴歸，當你要預測的是計數型資料（count data）就可以用他。</p><a id="more"></a><h2 id="Poisson-distribution"><a href="#Poisson-distribution" class="headerlink" title="Poisson distribution"></a>Poisson distribution</h2><p>在講 Poisson 迴歸之前要先來講講 Poisson 分佈，他的公式大概是長的像這樣：</p><p>$$<br>P(Y=y; \lambda) = \frac{e^{- \lambda} \lambda^y}{y!}<br>$$</p><p>圖形的話看起來是這樣。</p><p><img src="/images/Poisson_distribution.svg" alt=""></p><blockquote><p>圖片來自維基百科</p></blockquote><p>要怎麼看懂這個分佈呢？</p><p>我們先想像一個情境好了，假設我們經營一家便利商店，在一天之中來光臨這家商店的人數不同時段不一樣。即使是同一個時段，你也很難準確預測會有多少人進到店裡來。這時候我們就會用機率的描述方式，在這邊 k 指的是當我們觀察每段時間區間內進入店裡的客人數量，那麼 $\lambda$ 就是平均來說，每個時間區間的來客人數。你可以看到在 $\lambda = 1$ 的分佈上，來客人數是 0 或是 1 的機率其實很高，但是大於 1 的情形並不是沒有，只是機率比較低罷了。</p><p>因此，我們可以用這樣的分佈來估算計數型的資料</p><h2 id="Poisson-迴歸"><a href="#Poisson-迴歸" class="headerlink" title="Poisson 迴歸"></a>Poisson 迴歸</h2><p>計數型的資料難道不能用一般的線性迴歸嗎？</p><p>其實這兩者有非常大的差別：</p><ol><li>計數型的資料不會有負值</li><li>計數型的資料不會有小數點</li></ol><p>基於以上兩點資料性質上的差異，我們必須把不同資料分別看待。</p><p>但是也不是完全不能用線性迴歸，只是需要動點手腳，就是對資料取 log。</p><p>如果你對上面的 Poisson 分佈取 log 的話會發生什麼事呢？</p><p>$$<br>ln(P(Y=y; \lambda)) = - \lambda + y ln(\lambda) - \sum_{y}^{j=1} j<br>$$</p><p>看到了吧！$\lambda$ 跳出來了！而平均數 $\lambda$ 是連續型的數值，可以作為線性迴歸要預測的對象的。</p><blockquote><p>注意：以上並非正式的證明，請勿用於正式推導</p></blockquote><p>其實我們的 Poisson 迴歸是長成這樣的：</p><p>$$<br>ln(\mathbb{E}[y]) = ln(\mathbb{E}[P(Y=y; \lambda)]) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>是不是跟我們前面提到的羅吉斯迴歸有 87% 像呢？</p><p>而且我們在上面有提到 $\lambda$ 是平均數，所以呢…</p><p>$$<br>ln(\mathbb{E}[y]) = ln(\lambda) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>Poisson 迴歸在預測的根本是 $ln(\lambda)$ 嘛！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上次我們講完了線性迴歸跟羅吉斯迴歸的差異。&lt;/p&gt;
&lt;p&gt;可是並不是每一種資料都是連續型的或是類別型的。&lt;/p&gt;
&lt;p&gt;這次要來介紹 Poisson 迴歸，當你要預測的是計數型資料（count data）就可以用他。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
</feed>
