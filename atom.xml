<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dream Maker</title>
  
  <subtitle>Love Math, Science, Biology, Computer science</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yuehhua.github.io/"/>
  <updated>2019-02-24T07:47:10.209Z</updated>
  <id>https://yuehhua.github.io/</id>
  
  <author>
    <name>Yueh-Hua Tu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>神經微分方程（Neural Ordinary Differential Equations）</title>
    <link href="https://yuehhua.github.io/2019/02/24/neural-ode/"/>
    <id>https://yuehhua.github.io/2019/02/24/neural-ode/</id>
    <published>2019-02-24T07:47:10.000Z</published>
    <updated>2019-02-24T07:47:10.209Z</updated>
    
    <content type="html"><![CDATA[<p>這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。</p><h2 id="核心觀念"><a href="#核心觀念" class="headerlink" title="核心觀念"></a>核心觀念</h2><p>概念上來說，就是將神經網路<strong>離散的層</strong>觀念打破，將他貫通成為<strong>連續的層</strong>的網路架構。</p><p>連續和離散的差別來自於倒傳遞的過程：</p><p>$$<br>\mathbb{y}_{t+1} = \mathbb{y}_t - \eta \nabla \mathcal{L}<br>$$</p><p>其中 $\nabla \mathcal{L}$ 就是梯度的部份，是向量的，然而我們把他簡化成純量來看的話，他不過就是</p><p>$$<br>\frac{d \mathcal{L}}{dt}<br>$$</p><p>廣義上來說，一個函數的微分，如果是離散的版本就會是</p><p>$$<br>\frac{dy}{dt} = \frac{y(t + \Delta) - y(t)}{\Delta}<br>$$</p><p>如此一來，所形成的方程式就會是差分方程，然而連續的版本就是</p><p>$$<br>\frac{dy}{dt} = \lim_{\Delta \rightarrow 0} \frac{y(t + \Delta) - y(t)}{\Delta}<br>$$</p><p>這個所形成的會是微分方程。</p><h2 id="從離散到連續"><a href="#從離散到連續" class="headerlink" title="從離散到連續"></a>從離散到連續</h2><p>我們可以從離散的版本</p><p>$$<br>\frac{dy}{dt} = \frac{y(t + \Delta) - y(t)}{\Delta}<br>$$</p><p>把他轉成以下的樣貌</p><p>$$<br>y(t + \Delta) = y(t) + \Delta \frac{dy}{dt}<br>$$</p><p>要將他貫通的話，我們就得由從神經網路的基礎開始，如果是一般的前回饋網路（feed-forward network）當中的隱藏層是像下列這個樣子：</p><p>$$<br>h_{t+1} = f(h_t, \theta)<br>$$</p><p>我們可以發現像是 ResNet 這類的網路有 skip connection 的設置，所以跟一般的前回饋網路不同</p><p>$$<br>h_{t+1} = h_t + f(h_t, \theta)<br>$$</p><p>而 RNN 等等有序列概念的模型也有類似的結構，就是會是前一層的結果加上通過 $f$ 運算後的結果，成為下一層的結果。</p><p>這樣的形式跟我們前面提到的形式不謀而合</p><p>$$<br>y(t + \Delta) = y(t) + \Delta \frac{dy}{dt}<br>$$</p><p>只要我們把 $\Delta = 1$ 代入，就成了</p><p>$$<br>y(t+1) = y(t) + \frac{dy}{dt}<br>$$</p><p>以下給大家比對一下</p><p>$$<br>h_{t+1} = h_t + f(h_t, \theta) \\<br>y(t+1) = y(t) + \frac{dy}{dt}<br>$$</p><p>也就是，我們可以讓</p><p>$$<br>\frac{dy}{dt} = f(h_t, \theta)<br>$$</p><p>神奇的事情就發生了！神經網路 $f$ 就可以被我們拿來計算微分 $\frac{dy}{dt}$！</p><p>比較精確的說法是，把神經網路的層 $f$ 拿來逼近微分項，或是說梯度。這樣我們後面就可以用數值方法來逼近解。</p><p>$$<br>y(t + \Delta) = y(t) + \Delta \frac{dy}{dt} \\<br>\downarrow \\<br>y(t + \Delta) = y(t) + \Delta f(t, h(t), \theta_t)<br>$$</p><p>要拉成連續的還有一個重要的手段，就是將不同的層 $t$ 從離散的變成連續的，所以作者將 $t$ 做了參數化，將他變成 $f$ 的參數之一，如此一來，就可以在任意的層中放入資料做運算。</p><p>最重要的概念導出了這樣的式子</p><p>$$<br>h(t) \rightarrow \frac{dy(t)}{dt} = f(h(t), t, \theta) \rightarrow y(t)<br>$$</p><h2 id="神經網路作為一個系統的微分形式"><a href="#神經網路作為一個系統的微分形式" class="headerlink" title="神經網路作為一個系統的微分形式"></a>神經網路作為一個系統的微分形式</h2><p>在傳統科學或是工程領域，我們會以微分式來表達以及建構一個系統。</p><p>$$<br>\nu = \frac{dx}{dt} = t + 1<br>$$</p><p>其實在這邊是一樣的道理，整體來說，我們是換成用神經網路去描述一個微分式，其實本質上就是這樣。</p><p>原本的層的概念就是用數學函數來建立的，而層與層之間傳遞著計算的結果。</p><p>$$<br>\mathbb{h_1} = \sigma(W_1 \mathbb{x} + \mathbb{b_1}) \\<br>\mathbb{y} = \sigma(W_2 \mathbb{h_1} + \mathbb{b_2})<br>$$</p><p>然而變成連續之後，我們等於是用神經網路中的層去建立跟描繪微分形式。</p><p>$$<br>\frac{d h_1(t)}{dt} = \sigma(W_1 \mathbb{x}(t) + \mathbb{b_1}) \\<br>\frac{d y(t)}{dt} = \sigma(W_2 \mathbb{h_1}(t) + \mathbb{b_2})<br>$$</p><p>是不是跟如出一轍呢？</p><p>$$<br>\frac{dy(t)}{dt} = f(h(t), t, \theta)<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。&lt;/p&gt;
&lt;h2 id=&quot;核心觀念&quot;&gt;&lt;a href=&quot;#核心觀念&quot; 
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>筆記 - 從向量空間到函數空間</title>
    <link href="https://yuehhua.github.io/2019/02/24/note-of-vector-space-to-function-space/"/>
    <id>https://yuehhua.github.io/2019/02/24/note-of-vector-space-to-function-space/</id>
    <published>2019-02-24T04:54:02.000Z</published>
    <updated>2019-02-24T04:54:02.908Z</updated>
    
    <content type="html"><![CDATA[<p>參考<a href="https://ccjou.wordpress.com/2009/08/18/%E5%BE%9E%E5%B9%BE%E4%BD%95%E5%90%91%E9%87%8F%E7%A9%BA%E9%96%93%E5%88%B0%E5%87%BD%E6%95%B8%E7%A9%BA%E9%96%93/" target="_blank" rel="noopener">從幾何向量空間到函數空間| 線代啟示錄</a>。</p><ol><li>由 $\mathbb{R}^n$ 拓展到 $\mathbb{R}^{\infty}$ 所需俱備的條件是什麼？</li></ol><p>由於一個向量 $\mathbb{v} \in \mathbb{R}^{\infty}$，在無限維度下我們需要考慮一個問題，就是 norm。</p><p>如果這個空間有定義 norm 的話，我們就要考慮他有沒有收斂，也就是 $||\mathbb{v}||^2$ 要存在。</p><p>所以條件就是</p><p>$$<br>||\mathbb{v}||^2 = \sum_{i=1}^{\infty} v_i^2<br>$$</p><p>要收斂。</p><ol start="2"><li>從 $\mathbb{R}^{\infty}$ 無限維度的向量空間再拓展到 $C^{\omega}$ 函數空間，所需要俱備的條件是什麼？</li></ol><p>一個無限維度的向量是一個離散的版本，由剛剛的式子可以看的出來</p><p>$$<br>||\mathbb{v}||^2 = \sum_{i=1}^{\infty} v_i^2<br>$$</p><p>而一個（解析）函數則是連續的</p><p>$$<br>||f||^2 = \int f^2(x) dx<br>$$</p><p>除了以上的 norm 要收斂外，從離散到連續應該有些假設或是條件才是。</p><ol start="3"><li>函數的基底</li></ol><p>Fourier series</p><p>$$<br>f(x) = a_0 + a_1 \cos x + b_1 \sin x + a_2 \cos 2x + b_2 \sin 2x + \cdots<br>$$</p><p>所以基底就是</p><p>$$<br>&lt;\beta&gt; = &lt;1, \cos x, \sin x, \cos 2x, \sin 2x, \cdots&gt;<br>$$</p><ol start="4"><li>非週期性函數基底</li></ol><p>Legendre polynomial</p><ol start="5"><li>Least square problem</li></ol><p>$$<br>(A^TA)\hat{y} = A^Tb<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;參考&lt;a href=&quot;https://ccjou.wordpress.com/2009/08/18/%E5%BE%9E%E5%B9%BE%E4%BD%95%E5%90%91%E9%87%8F%E7%A9%BA%E9%96%93%E5%88%B0%E5%87%BD%E6%95
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>在參數化型別中使用值的效能分析</title>
    <link href="https://yuehhua.github.io/2019/02/22/performance-analysis-of-value-in-parametric-type/"/>
    <id>https://yuehhua.github.io/2019/02/22/performance-analysis-of-value-in-parametric-type/</id>
    <published>2019-02-22T04:24:37.000Z</published>
    <updated>2019-02-22T14:12:14.663Z</updated>
    
    <content type="html"><![CDATA[<p>我在使用的時候有注意到<em>在參數化型別中使用值</em>的方式與<em>傳統封裝</em>的方式有效能上的差異。</p><p>所以我就做了一些測試。</p><p>在參數化型別中使用值：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> A&#123;T&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>傳統型別封裝：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> B</span><br><span class="line">    x::<span class="built_in">Int64</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><em>全文出現的程式碼為實際測試程式碼</em></p><p>因為 Julia 有提供好用的 <code>@code_llvm</code> 及 <code>@code_native</code> 來觀察一行程式碼實際被轉換成 LLVM 或是組合語言的時候會產生多少行的程式碼，藉此我們可以用低階程式碼來評估是否有效率。程式碼的行數愈少是越有效率的。</p><h2 id="建立"><a href="#建立" class="headerlink" title="建立"></a>建立</h2><p>我們來測試一個物件被建立需要多少行的程式碼。</p><h3 id="A-LLVM"><a href="#A-LLVM" class="headerlink" title="A - LLVM"></a>A - LLVM</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_llvm</span> A&#123;<span class="number">5</span>&#125;()</span><br></pre></td></tr></table></figure><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">;  @ REPL[1]:3 within `Type'</span></span><br><span class="line"><span class="comment">define nonnull %jl_value_t addrspace(10)* @japi1_Type_12238(%jl_value_t addrspace(10)*, %jl_value_t addrspace(10)**, i32) #0 &#123;</span></span><br><span class="line"><span class="comment">top:</span></span><br><span class="line"><span class="comment">  %3 = alloca %jl_value_t addrspace(10)**, align 8</span></span><br><span class="line"><span class="comment">  store volatile %jl_value_t addrspace(10)** %1, %jl_value_t addrspace(10)*** %3, align 8</span></span><br><span class="line"><span class="comment">  ret %jl_value_t addrspace(10)* addrspacecast (%jl_value_t* inttoptr (i64 140407726014496 to %jl_value_t*) to %jl_value_t addrspace(10)*)</span></span><br><span class="line"><span class="comment">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="B-LLVM"><a href="#B-LLVM" class="headerlink" title="B - LLVM"></a>B - LLVM</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_llvm</span> B(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">;  @ REPL[1]:2 within `Type'</span></span><br><span class="line"><span class="comment">define &#123; i64 &#125; @julia_Type_12221(%jl_value_t addrspace(10)*, i64) &#123;</span></span><br><span class="line"><span class="comment">top:</span></span><br><span class="line"><span class="comment">  %.fca.0.insert = insertvalue &#123; i64 &#125; undef, i64 %1, 0</span></span><br><span class="line"><span class="comment">  ret &#123; i64 &#125; %.fca.0.insert</span></span><br><span class="line"><span class="comment">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="A-Assembly"><a href="#A-Assembly" class="headerlink" title="A - Assembly"></a>A - Assembly</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_native</span> A&#123;<span class="number">5</span>&#125;()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">; ┌ @ REPL[1]:3 within `Type&apos;</span><br><span class="line">movq%rsi, -8(%rsp)</span><br><span class="line">movabsq$140407726014496, %rax  # imm = 0x7FB338A20020</span><br><span class="line">retq</span><br><span class="line">; └</span><br></pre></td></tr></table></figure><h3 id="B-Assembly"><a href="#B-Assembly" class="headerlink" title="B - Assembly"></a>B - Assembly</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_native</span> B(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">; ┌ @ REPL[1]:2 within `Type&apos;</span><br><span class="line">movq%rsi, %rax</span><br><span class="line">retq</span><br><span class="line">nopw%cs:(%rax,%rax)</span><br><span class="line">; └</span><br></pre></td></tr></table></figure><h2 id="取值"><a href="#取值" class="headerlink" title="取值"></a>取值</h2><p>接著測試從物件當中取值出來的效能。</p><p>定義取值的方法：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get_value(::A&#123;T&#125;) <span class="keyword">where</span> &#123;T&#125; = T</span><br><span class="line">get_value(b::B) = b.x</span><br></pre></td></tr></table></figure><p>事先建立好物件：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = A&#123;<span class="number">5</span>&#125;()</span><br><span class="line">b = B(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="A-LLVM-1"><a href="#A-LLVM-1" class="headerlink" title="A - LLVM"></a>A - LLVM</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_llvm</span> get_value(a)</span><br></pre></td></tr></table></figure><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">;  @ REPL[8]:1 within `get_value'</span></span><br><span class="line"><span class="comment">define i64 @julia_get_value_12274() &#123;</span></span><br><span class="line"><span class="comment">top:</span></span><br><span class="line"><span class="comment">  ret i64 5</span></span><br><span class="line"><span class="comment">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="B-LLVM-1"><a href="#B-LLVM-1" class="headerlink" title="B - LLVM"></a>B - LLVM</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_llvm</span> get_value(b)</span><br></pre></td></tr></table></figure><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">;  @ REPL[5]:1 within `get_value'</span></span><br><span class="line"><span class="comment">define i64 @julia_get_value_12630(&#123; i64 &#125; addrspace(11)* nocapture nonnull readonly dereferenceable(8)) &#123;</span></span><br><span class="line"><span class="comment">top:</span></span><br><span class="line"><span class="comment">; ┌ @ sysimg.jl:18 within `getproperty'</span></span><br><span class="line"><span class="comment">   %1 = getelementptr inbounds &#123; i64 &#125;, &#123; i64 &#125; addrspace(11)* %0, i64 0, i32 0</span></span><br><span class="line"><span class="comment">; └</span></span><br><span class="line"><span class="comment">  %2 = load i64, i64 addrspace(11)* %1, align 8</span></span><br><span class="line"><span class="comment">  ret i64 %2</span></span><br><span class="line"><span class="comment">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="A-Assembly-1"><a href="#A-Assembly-1" class="headerlink" title="A - Assembly"></a>A - Assembly</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_native</span> get_value(a)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">; ┌ @ REPL[8]:1 within `get_value&apos;</span><br><span class="line">movl$5, %eax</span><br><span class="line">retq</span><br><span class="line">nopw%cs:(%rax,%rax)</span><br><span class="line">; └</span><br></pre></td></tr></table></figure><h3 id="B-Assembly-1"><a href="#B-Assembly-1" class="headerlink" title="B - Assembly"></a>B - Assembly</h3><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@code_native</span> get_value(b)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">; ┌ @ REPL[5]:1 within `get_value&apos;</span><br><span class="line">movq(%rdi), %rax</span><br><span class="line">retq</span><br><span class="line">nopw%cs:(%rax,%rax)</span><br><span class="line">; └</span><br></pre></td></tr></table></figure><p>給大家參考。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我在使用的時候有注意到&lt;em&gt;在參數化型別中使用值&lt;/em&gt;的方式與&lt;em&gt;傳統封裝&lt;/em&gt;的方式有效能上的差異。&lt;/p&gt;
&lt;p&gt;所以我就做了一些測試。&lt;/p&gt;
&lt;p&gt;在參數化型別中使用值：&lt;/p&gt;
&lt;figure class=&quot;highlight julia&quot;&gt;&lt;tab
      
    
    </summary>
    
      <category term="Computer Science" scheme="https://yuehhua.github.io/categories/Computer-Science/"/>
    
      <category term="Julialang" scheme="https://yuehhua.github.io/categories/Computer-Science/Julialang/"/>
    
    
  </entry>
  
  <entry>
    <title>在參數化型別中定義值</title>
    <link href="https://yuehhua.github.io/2019/02/22/define-values-in-parametric-type/"/>
    <id>https://yuehhua.github.io/2019/02/22/define-values-in-parametric-type/</id>
    <published>2019-02-21T17:22:42.000Z</published>
    <updated>2019-02-21T17:22:42.393Z</updated>
    
    <content type="html"><![CDATA[<p>應該不少人看到這個標題會摸不著頭緒到底要做什麼，但是看完 Julia 中常見的程式碼你就會了解了。</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Array</span>&#123;<span class="built_in">Any</span>, <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><p>有沒有曾經納悶過那個數字 2 到底是怎麼進到參數的位置上的呢？</p><p>參數的位置不是只能放型別（type）嗎？</p><p>這同時也是我困惑已久的問題，就搜尋了一下，果不其然被我找到了方法：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> A&#123;T&#125;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A&#123;<span class="number">5</span>&#125;()</span><br></pre></td></tr></table></figure><p>原來這麼簡單就可以完成了！語法上並沒有限定一定要是<em>型別</em>，要放型別以外的東西似乎是可以的。</p><p>我目前測試了可以的有：Int64、Float64、Complex、Char、Bool、Symbol，所以估計數字應該都是可以的。</p><p>不行的有：String、Array，估計物件或是陣列都是不行的。</p><h2 id="定義範圍"><a href="#定義範圍" class="headerlink" title="定義範圍"></a>定義範圍</h2><p>不過使用上並沒有任何限制會有點危險，所以還是定義一下範圍會比較好，像是：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> A&#123;<span class="literal">I</span>&#125;</span><br><span class="line">    <span class="keyword">function</span> A&#123;<span class="literal">I</span>&#125;() <span class="keyword">where</span> &#123;<span class="literal">I</span>&#125;</span><br><span class="line">        <span class="keyword">isa</span>(<span class="literal">I</span>,<span class="built_in">Integer</span>) || error(<span class="string">"bad parameter"</span>)</span><br><span class="line">        new()</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>這樣就可以限制參數要是整數的範圍。</p><h2 id="從參數取值"><a href="#從參數取值" class="headerlink" title="從參數取值"></a>從參數取值</h2><p>那我們能不能從型別的參數當中取值呢？</p><p>可以。</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_value(::A&#123;<span class="literal">I</span>&#125;) <span class="keyword">where</span> A&#123;<span class="literal">I</span>&#125; = <span class="literal">I</span></span><br></pre></td></tr></table></figure><p>如此一來，我們就可以從型別中拿到值了。</p><h2 id="好處？"><a href="#好處？" class="headerlink" title="好處？"></a>好處？</h2><p>這麼做有什麼好處？</p><p>當你把值的資訊放到型別當中，型別就多了一些資訊可以提供編譯器處理，這對於要自己設計型別階層可是非常好用的。</p><p>例如像是你可以將陣列的長度資訊儲存到型別上，這樣編譯器就可以處理陣列的長度資訊了。</p><p>這樣的程式風格會跟 dependent type language 有些相似了。</p><p>大家可以玩玩看。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;應該不少人看到這個標題會摸不著頭緒到底要做什麼，但是看完 Julia 中常見的程式碼你就會了解了。&lt;/p&gt;
&lt;figure class=&quot;highlight julia&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;lin
      
    
    </summary>
    
      <category term="Computer Science" scheme="https://yuehhua.github.io/categories/Computer-Science/"/>
    
      <category term="Julialang" scheme="https://yuehhua.github.io/categories/Computer-Science/Julialang/"/>
    
    
  </entry>
  
  <entry>
    <title>Note - Mathematical objects</title>
    <link href="https://yuehhua.github.io/2019/02/06/mathematical-objects/"/>
    <id>https://yuehhua.github.io/2019/02/06/mathematical-objects/</id>
    <published>2019-02-05T16:02:53.000Z</published>
    <updated>2019-02-05T16:02:53.088Z</updated>
    
    <content type="html"><![CDATA[<p>20th century Cantor:</p><blockquote><p>All mathematical objects can be defined as sets.</p></blockquote><p>Fundamentals:</p><ul><li>numbers</li><li>permutations</li><li>partitions</li><li>matrices</li><li>sets</li><li>functions</li><li>relations</li></ul><p>Geometry:</p><ul><li>hexagons</li><li>points</li><li>lines</li><li>triangles</li><li>circles</li><li>spheres</li><li>polyhedra</li><li>topological space</li><li>manifolds</li></ul><p>Algebra:</p><ul><li>groups</li><li>rings</li><li>fields</li><li>lattices</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;20th century Cantor:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All mathematical objects can be defined as sets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Fundamentals:&lt;/p&gt;
&lt;ul&gt;
&lt;li
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>生物資訊的初衷</title>
    <link href="https://yuehhua.github.io/2019/02/01/original-intention-to-bioinformatics/"/>
    <id>https://yuehhua.github.io/2019/02/01/original-intention-to-bioinformatics/</id>
    <published>2019-01-31T16:25:47.000Z</published>
    <updated>2019-01-31T16:25:47.679Z</updated>
    
    <content type="html"><![CDATA[<p>受到其他文章的啟發，我也來寫一篇為什麼我踏入生物資訊領域好了。</p><p>受到啟發應該算是從高中的時候說起，高中的時候喜歡數學、物理跟生物。對於數學，喜歡他的抽象及純粹，而物理可以解釋這個世界的法則，對於生物則是一直以來隱隱約約有些感覺的。小時候對於生命現象一直很好奇，對於生物的多樣性感到驚奇，但到了高中卻成了考卷上的考題，我不認為那是我要的。</p><p>還記得高中生物上到下視丘的時候會講到很多不同種的激素調控，我突然覺得這一切的背後似乎有著什麼，我對「調控」產生了興趣。接著到了高中快結束，終於上到近代的生物技術以及 DNA 分子的轉錄轉譯，雖然對當時的我來說有點複雜，但是我喜歡挑戰理解這種複雜的事物，我將他轉化成比較好理解的「設計圖」解釋。DNA 就像是一台車子的整體設計圖，RNA 就是將設計圖的一部份零件複製一份出來，並且製造出蛋白質，也就是真實的零件。理解了這些讓我非常開心。</p><p>大學念了醫學檢驗生物技術，但卻不是我的第一志願，不過我確定我對生物技術是有興趣的，我也非常認真對待我的選擇。在傳統的生物醫學研究都是花了十幾年的時間在研究一個蛋白或是一個基因的功能或是交互作用。</p><p>我大三的某天在逛維基百科（你沒看錯，我會去逛維基百科）被我發現了系統生物學這個領域，看到頁面的當下非常震驚，可以以一個系統的觀點切入生物的議題，那麼就可以不用那麼辛苦的一個基因一個基因研究了。而且系統的概念直接串起了在生化中學到的調控，他不只是 pathway，而是一個複雜的網路，可以藉由網路的調控或是反應機制，讓生物體做出特定的行為。生物體就是個巨大的機械，但是複雜度卻遠高於機械，也不像機構那樣那麼容易理解，很多事情是人類目前還不知道的。</p><p>因為這樣的未知，這樣的複雜，這樣的調控系統，讓我決心研究所要往這個方向走。</p><p>大四的時候有進階生物技術，接觸到定序技術、生物資訊、序列處理的議題。同時雙主修資訊工程，我更享受在資工系的課程當中，雖然他講的是程式、作業系統等等，但是對於（建造）系統的概念始終是保留的。我最有興趣的大概是離散數學、演算法跟訊號與系統了，離散數學中的圖論可以說是非常神妙，而圖論的用途也超級廣，可以拿來 model 很多不同的事物。演算法則是去證明一件事情可以被如何的完成是最快的，這些魔法都來自於數學。訊號與系統講述了如何去探知或是解析一個系統，我們怎麼從一個系統的行為當中去反推這個系統的架構。</p><p>到研究所真正接觸了生物資訊與我的認知相去不遠，不過還是少了點什麼，看了看課程發現了機器學習的課程，也詢問了學長關於這個領域，聽說還蠻推薦的，但是受限於開課時間，就乾脆自己去找了 coursera 上林軒田老師的機器學習課程看，大概一個月左右就把他看完了。看的當下非常開心，學到了跟演算法非常相似的技術，而當時大數據剛開始紅，所以就以這個技術為主軸開始了我的研究。</p><p>殊不知，後來的深度學習的崛起，AI 的爆紅，讓機器學習變得異常的熱門。不過我還是希望繼續做系統生物學，應該說是計算生物學。來把這迷樣的生物系統 model 出來吧！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;受到其他文章的啟發，我也來寫一篇為什麼我踏入生物資訊領域好了。&lt;/p&gt;
&lt;p&gt;受到啟發應該算是從高中的時候說起，高中的時候喜歡數學、物理跟生物。對於數學，喜歡他的抽象及純粹，而物理可以解釋這個世界的法則，對於生物則是一直以來隱隱約約有些感覺的。小時候對於生命現象一直很好奇，
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>溫柔而強大</title>
    <link href="https://yuehhua.github.io/2019/01/19/kindness-and-powerful/"/>
    <id>https://yuehhua.github.io/2019/01/19/kindness-and-powerful/</id>
    <published>2019-01-19T15:07:41.000Z</published>
    <updated>2019-01-19T16:10:25.149Z</updated>
    
    <content type="html"><![CDATA[<p>已經好一段時間沒有用文章紀錄下自己的心情跟想法了。</p><p>這段期間都在專注寫技術文章。</p><a id="more"></a><p>在 2018 年 5 月剛好興趣參與了 Pytorch Taichung 的社群聚會，當時其實只是想去看看 Deep learning 技術發展到什麼階段了，自己當時也讀了一些進階的論文，像是 Attention model。</p><p>第一場是我給的開場演講，介紹了機器學習的概論。記得當時的主持人是生澀的副社長敬宇，以國際演討會的規格介紹整個流程，看來不是怎麼熟練，卻很有精神。</p><p>在經營社群來說，我也在群眾前演講一段時間了。一般社群小聚互動其實沒有那麼拘謹，我也就輕鬆開始我的演講。</p><p>記得演講結束之後，大家不太發言，是由副社問了 dropout 機制的問題，展開了一系列的討論。</p><p>往後一直到我出了我第一本書，這本書成為一個契機。敬宇跟我敲碗很久了，也就剛好拿到書的那個禮拜就有機會拿給他，就一起吃了午餐。</p><p>兩個人聊的很開心，也不顧時間到了，依依不捨。記得他下午還要趕去做實驗呢。</p><p>以往社群的人很欣賞我論文的報告，不流於形式，好理解，敬宇更是成為了迷弟。</p><p>後來就成為了很要好的朋友，無所不聊，從數學物理電腦到經濟哲學價值觀。</p><p>慢慢了解到敬宇的一些事情，發現他是需要幫助的孩子。</p><p>表面上沈默，私底下非常努力的孩子，慢熟而堅強。</p><p>身為好友應該可以給一些支援，雖然他很少主動開口。</p><p>對我而言，有能力幫助別人也是一種成功，成為一個 Giver。</p><p>一個溫柔而強大的 Giver。</p><p>大概是我這一年目標想做的的事情吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;已經好一段時間沒有用文章紀錄下自己的心情跟想法了。&lt;/p&gt;
&lt;p&gt;這段期間都在專注寫技術文章。&lt;/p&gt;
    
    </summary>
    
      <category term="My Style" scheme="https://yuehhua.github.io/categories/My-Style/"/>
    
      <category term="Friends" scheme="https://yuehhua.github.io/categories/My-Style/Friends/"/>
    
    
  </entry>
  
  <entry>
    <title>The Deterministic Information Bottleneck</title>
    <link href="https://yuehhua.github.io/2019/01/18/deterministic-information-bottleneck/"/>
    <id>https://yuehhua.github.io/2019/01/18/deterministic-information-bottleneck/</id>
    <published>2019-01-18T03:39:06.000Z</published>
    <updated>2019-01-19T16:10:31.676Z</updated>
    
    <content type="html"><![CDATA[<p>在 Information Bottleneck 之後出現了不少驚豔的呼喊，也出現了指出這個方法的缺點及詮釋錯誤。</p><p>在這之後有人專注在確定性這件事上。</p><a id="more"></a><iframe width="560" height="315" src="https://www.youtube.com/embed/-D5b_zCJxrs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><pre class="mermaid">graph LR    X -->|"p(x, y)"|Y    X -->|"q(t|x)"|T    T --- Y    X((X))    Y((Y))    T((T))</pre><p>$T$:</p><ul><li>soft sufficient statistics (for statistics)</li><li>lossy compression (for signal)</li><li>maximally informative clustering (for machine learning)</li></ul><h2 id="IB"><a href="#IB" class="headerlink" title="IB"></a>IB</h2><p>$$<br>min\ \mathcal{L} [q(t|x)] = I(T; X) - \beta I(T; Y), \beta &gt; 0<br>$$</p><p>$I(T; X)$: compression<br>$I(T; Y)$: relevance</p><p>Markov constraint: $T \leftarrow X \leftrightarrow Y$</p><p>$$<br>q(t|x) = \frac{q(t)}{Z(x, \beta)} exp(- \beta D_{KL} [p(y|x) || q(y|t)])) \\<br>q(t) = \sum_x p(x)q(t|x) \\<br>q(y|t) = \frac{1}{q(t)} \sum_x p(y|x)q(t|x)p(x)<br>$$</p><p>$I(T; X)$ from channel coding, rate distortion theory</p><h2 id="DIB"><a href="#DIB" class="headerlink" title="DIB"></a>DIB</h2><p>$$<br>min\ \mathcal{L} [q(t|x)] = H(T) - \beta I(T; Y)<br>$$</p><p>$H(T)$: penalize coding itself<br>$I(T; Y)$: lead to deterministic $\mathcal{L}_{IB}$</p><p>$$<br>\mathcal{L} _{IB} - \mathcal{L} _{DIB} = I(T; X) - H(T) = -H(T|X)<br>$$</p><p>$\mathcal{L}_{IB}$: implicit encourage of stochastic</p><h2 id="Generalized-IB"><a href="#Generalized-IB" class="headerlink" title="Generalized IB"></a>Generalized IB</h2><p>$$<br>\mathcal{L}_{\alpha} = H(T) - \alpha H(T|X) - \beta I(Y; T)<br>$$</p><p>$\alpha = 1 \Rightarrow \mathcal{L} _{IB}$: stochastic $\rightarrow$ soft clustering<br>$\alpha = 0 \Rightarrow \mathcal{L} _{DIB}$: deterministic $\rightarrow$ hard clustering</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Information Bottleneck 之後出現了不少驚豔的呼喊，也出現了指出這個方法的缺點及詮釋錯誤。&lt;/p&gt;
&lt;p&gt;在這之後有人專注在確定性這件事上。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="https://yuehhua.github.io/categories/Deep-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>共形映射</title>
    <link href="https://yuehhua.github.io/2019/01/16/conformal-map/"/>
    <id>https://yuehhua.github.io/2019/01/16/conformal-map/</id>
    <published>2019-01-16T06:22:27.000Z</published>
    <updated>2019-01-16T06:41:37.404Z</updated>
    
    <content type="html"><![CDATA[<p>$<br>\Omega \subset \mathbb{R}^2, f: \Omega \rightarrow \mathbb{R}^2<br>$</p><p>$$<br>f(x, y) = (u(x, y), v(x, y))<br>$$</p><p>$$<br>J(x, y) =<br>\begin{bmatrix}<br>\frac{\partial u}{\partial x}&amp; \frac{\partial u}{\partial y} \\<br>\frac{\partial v}{\partial x}&amp; \frac{\partial v}{\partial y}<br>\end{bmatrix}<br>$$</p><p>$<br>(x, y) \in \Omega, J(x, y) = s(x, y)R(x, y)<br>$</p><p>$s$ is a non-zero scalar.</p><p>$R$ is a $2 \times 2$ rotation matrix.</p><h5 id="mathcal-prop"><a href="#mathcal-prop" class="headerlink" title="$\mathcal{prop.}$"></a>$\mathcal{prop.}$</h5><ol><li>$f: \Omega \rightarrow \mathbb{R}^2 \text{ and } g: f(\Omega) \rightarrow \mathbb{R}^2$ are conformal map, then $g \circ f$ is conformal map</li><li>$f: \Omega \rightarrow \mathbb{R}^2$ is conformal map, $f^{-1}$ is conformal map</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$&lt;br&gt;\Omega \subset \mathbb{R}^2, f: \Omega \rightarrow \mathbb{R}^2&lt;br&gt;$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;f(x, y) = (u(x, y), v(x, y))&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;J(
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>保角映射</title>
    <link href="https://yuehhua.github.io/2019/01/16/angle-preserving-map/"/>
    <id>https://yuehhua.github.io/2019/01/16/angle-preserving-map/</id>
    <published>2019-01-16T03:47:36.000Z</published>
    <updated>2019-01-16T06:33:29.124Z</updated>
    
    <content type="html"><![CDATA[<h5 id="mathcal-Def"><a href="#mathcal-Def" class="headerlink" title="$\mathcal{Def.}$"></a>$\mathcal{Def.}$</h5><p>$A$ 為保角映射（angle-preserving map）</p><p>$$<br>\frac{(Ax)^T(Ay)}{||Ax|| \cdot ||Ay||} = \frac{x^Ty}{||x|| \cdot ||y||} \\<br>(\Rightarrow A\text{ is invertible})<br>$$</p><p>$$<br>\Rightarrow A = sQ, Q^TQ = I, s \ne 0<br>$$</p><p>$s$ 代表伸縮量</p><p>$det Q = 1$: 伸縮 + 旋轉<br>$det Q = -1$: 伸縮 + 鏡射</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;mathcal-Def&quot;&gt;&lt;a href=&quot;#mathcal-Def&quot; class=&quot;headerlink&quot; title=&quot;$\mathcal{Def.}$&quot;&gt;&lt;/a&gt;$\mathcal{Def.}$&lt;/h5&gt;&lt;p&gt;$A$ 為保角映射（angle-preservi
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>等距同構</title>
    <link href="https://yuehhua.github.io/2019/01/16/isometry/"/>
    <id>https://yuehhua.github.io/2019/01/16/isometry/</id>
    <published>2019-01-16T03:47:19.000Z</published>
    <updated>2019-01-16T06:07:41.143Z</updated>
    
    <content type="html"><![CDATA[<p>let $A \in \mathbb{R}^{n \times n}, f(x) = Ax$</p><p>$A$ 為保長映射（length-preserving map）或等距同構（isometry），以下為等價定義方式：</p><h5 id="mathcal-Def"><a href="#mathcal-Def" class="headerlink" title="$\mathcal{Def}$."></a>$\mathcal{Def}$.</h5><ol><li>$A$ is orthogonal matrix</li><li>$||Ax|| = ||x||$</li><li>$||Ax - Ay|| = ||x - y||$</li><li>$(Ax)^T(Ay) = x^Ty$</li></ol><p>$\Rightarrow$ 旋轉</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;let $A \in \mathbb{R}^{n \times n}, f(x) = Ax$&lt;/p&gt;
&lt;p&gt;$A$ 為保長映射（length-preserving map）或等距同構（isometry），以下為等價定義方式：&lt;/p&gt;
&lt;h5 id=&quot;mathcal-Def&quot;
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>Functional</title>
    <link href="https://yuehhua.github.io/2019/01/15/functional/"/>
    <id>https://yuehhua.github.io/2019/01/15/functional/</id>
    <published>2019-01-15T12:34:54.000Z</published>
    <updated>2019-02-05T15:36:16.818Z</updated>
    
    <content type="html"><![CDATA[<p>一般我們數學上稱 $f: \mathbb{R} \rightarrow \mathbb{R}$ 為函數 function。</p><p>然而，如果一個函數可以接受另一個函數作為他的輸入變數，而輸出是一個純量，$F: S \rightarrow \mathbb{R}$ 泛函 functional，其中 $S$ 是一個向量空間，函數是一種廣義的向量。</p><p>在最佳化理論或是機器學習當中最常遇到的就是損失函數 $\mathcal{L}$，他其實是一個泛函。</p><p>$$<br>\mathcal{L}[f] = \frac{1}{N} \sum_{i=1}^{N} (y_i - f(x_i))^2<br>$$</p><p>當我們有不同的資料，需要計算這些資料的 mean square error 的時候就會寫成像上面這個樣子。</p><p>$$<br>\mathcal{L}[f] = \lim_{n \rightarrow \infty} \int_b^a |f(x) - \sum_{i=1}^N a_i f_i(x)|^2 dx<br>$$</p><p>如果我們處理的不是資料，而是一段連續的空間，那我們就可以用以上這個連續的版本。</p><blockquote><p>ex.</p></blockquote><p>consider $X, Y \in V$</p><ol><li>$g: X \rightarrow \mathbb{R}$ is functional</li><li>$g: X \rightarrow \mathbb{R}^n$ is functional</li><li>$g: X \rightarrow Y$ is operator</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一般我們數學上稱 $f: \mathbb{R} \rightarrow \mathbb{R}$ 為函數 function。&lt;/p&gt;
&lt;p&gt;然而，如果一個函數可以接受另一個函數作為他的輸入變數，而輸出是一個純量，$F: S \rightarrow \mathbb{R}$ 泛函
      
    
    </summary>
    
      <category term="Math" scheme="https://yuehhua.github.io/categories/Math/"/>
    
    
  </entry>
  
  <entry>
    <title>Compare Clustering and Embedding</title>
    <link href="https://yuehhua.github.io/2019/01/15/compare-clustering-and-embedding/"/>
    <id>https://yuehhua.github.io/2019/01/15/compare-clustering-and-embedding/</id>
    <published>2019-01-15T01:10:33.000Z</published>
    <updated>2019-01-15T01:22:16.348Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th></th><th>Clustering</th><th>Embedding</th></tr></thead><tbody><tr><td>Target space</td><td>discrete</td><td>continuous</td></tr><tr><td>Target dimension</td><td>$d$</td><td>$\mathbb{R}^d$</td></tr><tr><td>Transformed result can be</td><td>composable</td><td>correlated</td></tr><tr><td>Assumption</td><td>globally static context (dataset)</td><td>globally dynamic context (dataset)</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Clustering&lt;/th&gt;
&lt;th&gt;Embedding&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Target space&lt;/td&gt;
&lt;td&gt;discrete&lt;/td&gt;
&lt;t
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>方均根、標準差、馬克士威-波茲曼分佈</title>
    <link href="https://yuehhua.github.io/2018/12/20/rms-and-variance/"/>
    <id>https://yuehhua.github.io/2018/12/20/rms-and-variance/</id>
    <published>2018-12-19T16:19:39.000Z</published>
    <updated>2019-01-01T13:24:51.817Z</updated>
    
    <content type="html"><![CDATA[<p>不久前跟好朋友聊天聊到 random walk 的問題，一個醉漢會在一維的空間上隨機往前或往後走一步。</p><p>這樣的話，他最後會走到哪裡去呢？</p><a id="more"></a><p>基本上如果有看過隨機過程的話，會知道當 $n \rightarrow \infty$ 的時候會收斂到原點。</p><h2 id="平均位移"><a href="#平均位移" class="headerlink" title="平均位移"></a>平均位移</h2><p>如果我們考慮走了 $N$ 步之後的位移 $S$</p><p>$$<br>S = \sum_{i=1}^{N} x_i<br>$$</p><p>那麼平均位移說起來就是</p><p>$$<br>\mathbb{E}[S] = \mathbb{E}[\sum_{i=1}^{N} x_i] = \sum_{i=1}^{N} \mathbb{E}[x_i]<br>$$</p><p>當中的 $\mathbb{E}[x_i]$，由於每次要不是往前走一步或是往後退一步，而且兩者發生的機率一樣，所以每步的位移平均是 0，所以整體平均位移也是 0。</p><p>$$<br>\mathbb{E}[S] = 0<br>$$</p><p>如果你真的用電腦跑模擬，去紀錄多次走 5000 步（或是更多）最終的位移會是多少。</p><p>如此一來，你會得到一個以 0 為平均的常態分佈曲線。</p><h2 id="有沒有更有意義的資訊？"><a href="#有沒有更有意義的資訊？" class="headerlink" title="有沒有更有意義的資訊？"></a>有沒有更有意義的資訊？</h2><p>我們除了可以看平均以外還可以看什麼？</p><p>我們或許可以看位移平方後的平均</p><p>$$<br>\mathbb{E}[S^2] = \mathbb{E}[(\sum_{i=1}^{N} x_i)^2] \\<br>= \mathbb{E}[(x_1 + x_2 + \dots + x_N)^2] \\<br>= \mathbb{E}[(x_1^2 + x_2^2 + \dots + x_N^2) + 2(x_1x_2 + x_1x_3 + \dots + x_{N-1}x_N)] \\<br>= \mathbb{E}[x_1^2] + \mathbb{E}[x_2^2] + \dots + \mathbb{E}[x_N^2] + 2(\mathbb{E}[x_1x_2] + \mathbb{E}[x_1x_3] + \dots + \mathbb{E}[x_{N-1}x_N)]) \\<br>$$</p><p>如同前面的假設，如果我們將位移給平方了，那我們會得到每一項都是 1。至於相乘項的部份，可以自己動手試試看計算比較小的組合，不過理論上會是 0。</p><p>$$<br>= 1 + 1 + \dots + 1 + 2(0 + 0 + \dots + 0) \\<br>= N<br>$$</p><p>我們得到了位移平方後的平均是 $N$！</p><h2 id="方均根"><a href="#方均根" class="headerlink" title="方均根"></a>方均根</h2><p>大家可能在高中物理中聽到方均根（root-mean-square）這個計算方式，我們也可以求得方均根位移。</p><p>只要再開個根號就可以了，$\sqrt{\mathbb{E}[S^2]} = \sqrt{N}$。</p><p>這東西是不是看起來跟統計上的標準差很像呢？</p><p>$$<br>\sigma = \sqrt{Var[X]} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x - \mu)^2}<br>$$</p><p>大概差別只會在於裏面有沒有把 $\mu$ 給減掉而已，不過也可以藉由將 $\mu$ 設定成 0 來達到同樣的效果。</p><p>意思也就是說，當醉漢走了 $N$ 步之後，會呈現一個常態分佈，而平均值是 0，標準差則是 $\sqrt{N}$。</p><p>也就是當醉漢走愈多步，終究會回歸原點，但是也會有機率距離原點一段距離，而這段距離會隨著步數的增加而變長。</p><p>在隨機過程中，這是個很經典的問題。</p><p>既然談到了方均根，就不難聯想到高中物理中講到的方均根速度。</p><h2 id="氣體動力論"><a href="#氣體動力論" class="headerlink" title="氣體動力論"></a>氣體動力論</h2><p>在氣體動力論當中，我們可以去計算一個空間中的氣體分子運動速度，以方均根的形式表示</p><p>$$<br>v_{rms} = \sqrt{\frac{3kT}{m}}<br>$$</p><p>而這個氣體速度會呈現一個分佈情形，稱為馬克士威-波茲曼速率分佈。</p><h2 id="馬克士威-波茲曼速率分佈"><a href="#馬克士威-波茲曼速率分佈" class="headerlink" title="馬克士威-波茲曼速率分佈"></a>馬克士威-波茲曼速率分佈</h2><p>我們可以從維基百科上找到以下的速度分佈。</p><blockquote><p>Maxwell–Boltzmann velocity distribution</p></blockquote><p>$$<br>f(\nu_x, \nu_y, \nu_z) = (\frac{m}{2 \pi kT})^{3/2} exp \big[ - \frac{m(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2kT} \big]<br>$$</p><p>他描述了一個氣體分子在三維空間上有 $\nu_x, \nu_y, \nu_z$ 三種不同的速度分量，可以利用這些分量來計算出整體的速度分佈情形。</p><p>不覺得上式跟常態分佈有點相似嗎？</p><p>來呼叫一下常態分佈。</p><p>$$<br>f(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} exp \big[ - \frac{(x - \mu)^2}{2 \sigma^2} \big]<br>$$</p><p>那我們來動動手，做點簡單的驗證吧！</p><p>$$<br>f(\nu_x, \nu_y, \nu_z) = (\frac{m}{2 \pi kT})^{3/2} exp \big[ - \frac{m(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2kT} \big] \\<br>= (\frac{1}{\sqrt{2 \pi} \sqrt{\frac{kT}{m}} })^3 exp \big[ - \frac{(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2 (\sqrt{\frac{kT}{m}})^2 } \big]<br>$$</p><p>我們做點簡單的整理，然後將標準差抓出來。</p><p>let $\sigma = \sqrt{\frac{kT}{m}}$</p><p>代入之後就會成為</p><p>$$<br>= (\frac{1}{\sqrt{2 \pi} \sigma })^3 exp \big[ - \frac{(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2 \sigma^2 } \big]<br>$$</p><p>是不是變得更像了呢？那麼指數項中的速度平方和怎麼處理？</p><p>當然是把指數拆開囉！</p><p>$$<br>= \big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_x^2}{2 \sigma^2 } ] \big)<br>\big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_y^2}{2 \sigma^2 } ] \big)<br>\big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_z^2}{2 \sigma^2 } ] \big)<br>$$</p><p>我們會發現馬克士威-波茲曼速率分佈其實是三個常態分佈的乘積，或是多元常態分佈（Multivariate normal distribution）！</p><p>$$<br>= f_x \cdot f_y \cdot f_z<br>$$</p><p>三個常態分佈各是對映三維空間中的三個速度分量，也就是不同速度分量之間是各自獨立，不互相影響的。</p><p>跟真正的常態分佈的差異仍舊是有沒有將平均值減掉。</p><p>$$<br>\nu_x = v_x - \mu_x<br>$$</p><p>$\nu_x$：相對速度</p><p>$v_x$：絕對速度</p><p>或許我們可以這樣解釋，在這整個空間中，整個氣體是靜止不動的，所以他的整體平均速度是 0，而氣體的微觀速度 $\nu_x$ 就是他真正的速度。如果整體氣體是有一個速度在移動的，那麼你可以透過相對速度及平均速度來推得氣體的絕對速度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不久前跟好朋友聊天聊到 random walk 的問題，一個醉漢會在一維的空間上隨機往前或往後走一步。&lt;/p&gt;
&lt;p&gt;這樣的話，他最後會走到哪裡去呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Physics" scheme="https://yuehhua.github.io/categories/Physics/"/>
    
    
  </entry>
  
  <entry>
    <title>紀念 - 風超大的高美濕地</title>
    <link href="https://yuehhua.github.io/2018/12/15/ning/"/>
    <id>https://yuehhua.github.io/2018/12/15/ning/</id>
    <published>2018-12-15T06:26:27.000Z</published>
    <updated>2018-12-15T06:29:57.428Z</updated>
    
    <content type="html"><![CDATA[<p>很久沒有寫文章了。</p><p>一個陽光耀眼愜意的下午，舒適的溫度。</p><p>下午約了好朋友一起去逛逛，從綠園道一路逛到科博館。</p><a id="more"></a><p>一個可以談天說地的朋友，常常可以聊整個晚上到半夜的朋友。</p><p>沿途談論科學的精確性，測量的誤差永遠是有的，但取決於想要看到多麼精細的結果。</p><p>雖然不同的學科背景，數學是很有感觸跟交集的領域。</p><p>果不其然的走入了科博館的數學館，在第一個展區把玩裝置，期待肥皂泡可以形成四維的超正方體結構。</p><p>陸續看了不少展區，可以從解析幾何通往丘成桐先生的研究。談談物理，也聊聊數學。</p><p>數學是發現吧？朋友一臉驚訝的地看著我。我輕輕的搖頭，表示自然數也可以被定義，而邏輯與語言正是用來定義自然數的工具。</p><p>這些工具本身卻是人們「定義」出來的。那是發明或是發現呢？</p><p>時間差不多就前往高美濕地，愈往濱海就感受到風力的強勁，方向盤緊握著。</p><p>誤以為這幾天天氣晴朗，不過到了之後發現雲很厚，沒辦法看到夕陽。</p><p>兩個人坐在棧道上吃著零食，隨意的聊著，儘管冷風颼颼，有好朋友一起比較不顯得冷，不孤獨。</p><p>對於孤獨，朋友似乎習以為常，畢竟能一起走的實在是太少了。</p><p>陪伴，讓集結的力量可以更強大，也可以走得比較長遠。</p><p>我也在尋找可以一起往前的伙伴，共享知識，一起成長，共享喜悅。</p><p>覺得很開心，也很幸運能遇到這樣的好朋友。</p><p>之後的路還很長，可以的話就一起冒險吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很久沒有寫文章了。&lt;/p&gt;
&lt;p&gt;一個陽光耀眼愜意的下午，舒適的溫度。&lt;/p&gt;
&lt;p&gt;下午約了好朋友一起去逛逛，從綠園道一路逛到科博館。&lt;/p&gt;
    
    </summary>
    
      <category term="My Style" scheme="https://yuehhua.github.io/categories/My-Style/"/>
    
      <category term="Friends" scheme="https://yuehhua.github.io/categories/My-Style/Friends/"/>
    
    
  </entry>
  
  <entry>
    <title>31 Variational autoencoder</title>
    <link href="https://yuehhua.github.io/2018/11/14/31-variational-autoencoder/"/>
    <id>https://yuehhua.github.io/2018/11/14/31-variational-autoencoder/</id>
    <published>2018-11-14T08:34:28.000Z</published>
    <updated>2019-01-01T14:04:26.947Z</updated>
    
    <content type="html"><![CDATA[<p>在 autoencoder 的模型裏面，會希望以一個 unsupervised 方法來做到特徵萃取的目的。</p><p>你也可以說他是一種降維的方法或是有損壓縮的方法。</p><p>基本上就是透過一個線性轉換將原來的特徵，映射到比較低維度的特徵空間上。</p><p><img src="/images/autoencoder1.svg" alt=""></p><a id="more"></a><p>圖中就是一個基本的 autoencoder 的樣子，而中間的 hidden layer 就是我們希望的特徵萃取結果。</p><p>我們希望所萃取到的特徵，可以被 <strong>還原</strong> 成原本圖形的樣子。他就會是類似壓縮跟解壓縮的概念。</p><p><img src="/images/autoencoder2.svg" alt=""></p><p>如果我們想看看他會壓縮成什麼樣子，我們可以將中間的 hidden layer 換成兩個 node 就好，如此一來，我們就可以將他視覺化。</p><p><img src="/images/autoencoder3.svg" alt=""></p><h2 id="問題"><a href="#問題" class="headerlink" title="問題"></a>問題</h2><p>常常在訓練 autoencoder 之後，我們希望他的 decoder，也就是後半的部份，可以被拿出來作為一個獨立的 generative model 使用。例如，我可以把 MNIST 資料集當中的數字 1 的圖片輸入 encoder 會得到一個特徵向量 $\mathbf{z}$，相對，這個特徵向量 $\mathbf{z}$ 可以被放到 decoder 中還原回原本的數字 1 圖片，我們希望如果日後知道某個特徵向量 $\mathbf{z}$ 也可以用同樣的作法還原出他原本的樣子。</p><p><img src="/images/autoencoder3.svg" alt=""></p><p>但往往行不通，在 $\mathbf{z}$ 上有些許的差異，就有可能產生很奇怪的結果。原因在於訓練資料通過 encoder 之後所產生的特徵向量的（流形）空間分佈，只有在訓練資料相對應的特徵向量分佈的附近才能產生出好的結果，離這些特徵向量太遠的是沒辦法產生好的結果，所以模型沒有看到過的資料就無法產生好的結果。</p><p>Variational autoencoder（VAE） 就是為了解決這樣的問題引進了 evidence lower bound（ELOB）的方法，並且將他改進成可用在 gradient-based method 上的模型。接下來會以兩種觀點切入講解，先講比較直觀的觀點。</p><h2 id="直觀觀點"><a href="#直觀觀點" class="headerlink" title="直觀觀點"></a>直觀觀點</h2><p>如果不在點附近的區域就無法產生合理的結果，我們可以去找出這些點是不是會呈現什麼樣的分佈，並且去得到這些分佈的參數。</p><p>如果是使用機率分佈的話，就可以減少空間上有 <strong>洞</strong> 的情形發生，也就是比較不會有模型沒看過的資料的地方，導致產生的圖不合理的狀況。</p><p>在 VAE 中，假設特徵向量會呈現常態分佈，所以我們會去計算這個分佈的平均值 $\mu$ 與標準差 $\sigma$，每個 hidden layer 的 node 都有一個相對應的平均值與標準差向量。</p><p><img src="/images/VAE1.svg" alt=""></p><p>相似的特徵向量會在空間分佈上在比較相近的位置，不同的特徵向量就各自形成各自的分佈情形。舉例來說，數字 1 的特徵向量會在空間分佈上比較接近，所以會形成一個分佈，跟數字 2 的分佈是不同的。如此一來，就可以用機率分佈的方式去涵蓋一些沒有被模型看過的資料。</p><p><img src="/images/VAE2.svg" alt=""></p><p>但是這要怎麼接續後面的 decoder 呢？當我們知道分佈之後，我們可以從分佈中做抽樣阿！</p><p>既然是數字 1 的分佈，我就可以從這個分佈當中做抽樣，抽樣出來的向量應該要可以還原回原本的樣子。</p><p>我們就可以將 encoder 跟 decoder 一起做訓練了！</p><h2 id="機率圖模型觀點"><a href="#機率圖模型觀點" class="headerlink" title="機率圖模型觀點"></a>機率圖模型觀點</h2><p>接下來會以比較抽象的觀點來說明。</p><h3 id="隱含因素"><a href="#隱含因素" class="headerlink" title="隱含因素"></a>隱含因素</h3><p>我們的目標是想造出一個 generative model，這個 generative model 需要產出不同的結果（$\mathbf{x}$），例如 MNIST 數字，而且我們會給一個 input（$\mathbf{z}$）來決定要產生出什麼數字，這個 input 就是決定要產生出什麼數字的 <strong>因素</strong>，我們希望從模型自己去學出來。整體來說，他是一個 unsupervised 問題，我們只能從結果（$\mathbf{x}$）去做回推這個 generative model（$p(\mathbf{x}, \mathbf{z})$）的長相，並且試圖猜測 input（$\mathbf{z}$）。</p><p>作者從貝氏定理出發，我們手上的資料只有 $\mathbf{x}$，想去求 $\mathbf{z}$，我們會假設資料產生的過程是個隨機過程，他牽涉到一個無法觀察的變數 $\mathbf{z}$。</p><p>$$<br>p_{\theta}(\mathbf{z} | \mathbf{x}) = \frac{p_{\theta}(\mathbf{x}, \mathbf{z})}{p_{\theta}(\mathbf{x})} = \frac{p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z})}{p_{\theta}(\mathbf{x})}<br>$$</p><p>這個過程包含了兩個部份：</p><ol><li>$p(\mathbf{z})$，$\mathbf{z}$ 是怎麼被產生的？</li><li>$p(\mathbf{x} | \mathbf{z})$，$\mathbf{x}$ 是如何從 $\mathbf{z}$ 產生的？</li></ol><h3 id="難題"><a href="#難題" class="headerlink" title="難題"></a>難題</h3><p>一般來說，使用貝氏定理會遇到一個難題，也就是需要知道分母怎麼估算，其中需要對 $\mathbf{z}$ 積分，那麼就需要知道所有的 $\mathbf{z}$ 排列組合，但是這是不可能的。</p><p>$$<br>p_{\theta}(\mathbf{x}) = \int p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z}) d\mathbf{z}<br>$$</p><p>目前貝氏的方法是用抽樣的方法（Markov chain Monte Calro）去估 $p_{\theta}(\mathbf{x})$，避開了直接去積分他。而且這樣的方法也太慢了，所以作者希望用 SGD 的方法來取代抽樣的方法。</p><h3 id="Evidence-lower-bound"><a href="#Evidence-lower-bound" class="headerlink" title="Evidence lower bound"></a>Evidence lower bound</h3><p>在這邊作者使用了 evidence lower bound（ELOB）的方法，既然要估 $p_{\theta}(\mathbf{x})$ 很困難，那麼我們直接去估 $p_{\theta}(\mathbf{z} | \mathbf{x})$ 如何？</p><p>那要怎麼估 $p_{\theta}(\mathbf{z} | \mathbf{x})$ 呢？如果直接算的話就是回到上面的方法，所以 ELOB 方法假設了另一個類似的 $q_{\phi}(\mathbf{z} | \mathbf{x})$ 來逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$。</p><p>要逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$，也就是要求兩個機率分佈的 KL divergence（$D_{KL}[q_{\phi}(\mathbf{z} | \mathbf{x}) || p_{\theta}(\mathbf{z} | \mathbf{x})]$）愈小愈好。</p><h3 id="架構"><a href="#架構" class="headerlink" title="架構"></a>架構</h3><p><img src="/images/VAE3.svg" alt=""></p><p>原本的問題就從左圖變成右圖，也就是以 $q_{\phi}(\mathbf{z} | \mathbf{x})$ 來逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$。$q_{\phi}(\mathbf{z} | \mathbf{x})$ 就是虛線箭頭的部份。</p><p><img src="/images/VAE4.svg" alt=""></p><p>我們再把右圖當中的兩個箭頭拆開，$q_{\phi}(\mathbf{z} | \mathbf{x})$ 就是 encoder，而在</p><p>$$<br>p_{\theta}(\mathbf{z} | \mathbf{x}) = \frac{p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z})}{p_{\theta}(\mathbf{x})}<br>$$</p><p>當中的 $p_{\theta}(\mathbf{x} | \mathbf{z})$ 就是 decoder 的部份。Encoder 對應到實際神經網路模型中，則是一個 $\mathbf{z} = f(\mathbf{x})$ 函數，decoder 對應的是另一個函數 $\mathbf{x} = g(\mathbf{z})$。</p><p><img src="/images/VAE5.svg" alt=""></p><p>我們可以把他轉換成這個樣子，這樣就形成了 autoencoder 的架構雛型了。</p><h3 id="抽樣"><a href="#抽樣" class="headerlink" title="抽樣"></a>抽樣</h3><p>回到前面的老問題，我們用一些資料通過 encoder 之後可以得到被壓縮的資料，而我們去估計出這些資料的機率分佈，我們要如何從這些機率分佈繼續往下計算呢？</p><p>是的！就是抽樣！我們會從這些機率分佈當中重新做抽樣，把他作為 decoder 的輸入，並且繼續做訓練。</p><p>但是在模型中間卡一個抽樣的動作，這個動作引進了機率這個不確定因素，這樣要怎麼做 gradient descent 呢？</p><h3 id="Reparametrization-trick"><a href="#Reparametrization-trick" class="headerlink" title="Reparametrization trick"></a>Reparametrization trick</h3><p><img src="/images/1606.05908.svg" alt=""></p><blockquote><p>圖片來自 <a href="https://arxiv.org/abs/1606.05908" target="_blank" rel="noopener">Tutorial on Variational Autoencoders</a></p></blockquote><p>左圖就是原本的抽樣的模型。要讓模型可以進行 gradient descent，這時候作者做了重新參數化的技巧，這樣的技巧就如同將抽樣的動作抽離出來，就如同右圖那樣。</p><p>重新參數化技巧是從標準常態分佈當中去隨機抽樣，抽樣的結果將他乘上標準差，並且加上平均值，這樣就可以模擬從隱藏層的分佈抽樣的動作。但是這並非重新參數化技巧的巧妙之處，巧妙之處是在於重新參數化之後，就可以將抽樣這個動作從反向傳遞（backpropagation）的路徑上移除，這樣子我們就可以輕鬆的做訓練了！</p><p>將抽樣的動作從反向傳遞的路徑上移除，你可以將這樣的抽樣動作看成資料的一部份，也就是原本的資料上會多加一筆從標準常態分佈的抽樣數值。而資料不也是抽樣而來的嗎？其實兩者的概念是等價的。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>這個模型結合了 Variational inference 的技巧，並且以 reparametrization trick 來讓模型可以用 gradient descent。這是非常重大的突破，他也開啟了生成型模型的一條路。真的非常推荐大家讀讀這個模型！不過原論文有點難懂就是了…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 autoencoder 的模型裏面，會希望以一個 unsupervised 方法來做到特徵萃取的目的。&lt;/p&gt;
&lt;p&gt;你也可以說他是一種降維的方法或是有損壓縮的方法。&lt;/p&gt;
&lt;p&gt;基本上就是透過一個線性轉換將原來的特徵，映射到比較低維度的特徵空間上。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/autoencoder1.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>30 結語</title>
    <link href="https://yuehhua.github.io/2018/10/31/30-conclusions/"/>
    <id>https://yuehhua.github.io/2018/10/31/30-conclusions/</id>
    <published>2018-10-30T16:14:33.000Z</published>
    <updated>2018-10-30T16:14:33.301Z</updated>
    
    <content type="html"><![CDATA[<p>原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。</p><p>我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。</p><p>這次的系列文章是在基礎之上，讓大家得以理解不同模型之間的來龍去脈以及變化性，這樣才有辦法更進一步進展到深度學習的領域。</p><p>完成這次鐵人賽的意義在於讓大家理解模型的來龍去脈，以及為什麼要用什麼樣的數學元件去兜一個模型。</p><p>當你遇到問題的時候，不可能會有一個 ready-to-use 的模型等在那邊給你用，對於人工智慧的技術應用，你必須要為自己所面對的問題和狀況自己去量身訂作自己的模型。</p><p>是的！你沒看錯，必須要由領域專家去理解自己要的是什麼，然後自己做出專門給這個情境的模型。</p><p>當你的情境非常特殊的時候，讓深度學習專家來深入其他知識領域是非常花時間的。如果以領域專家及深度學習專家之間以合作模式進行，那將會花費更高的成本在溝通上，因為人工智慧的技術應用需要對特定領域非常敏感。我個人認為只有領域專家繼續鑽研成為深度學習專家才有辦法徹底解決特定領域的問題。當然這條路非常的漫長，等於是需要一個特定領域的博士，以及深度學習的博士的等級。這些問題，只有當領域專家自己 <strong>理解</strong> 之後，不是只有 <strong>解決</strong> 問題，才能夠算是真正的 <strong>解決</strong> 了。</p><p>這系列文獻給擁有機器學習基礎，想繼續晉升到深度學習領域的朋友們。</p><p>感謝大家的支持！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。&lt;/p&gt;
&lt;p&gt;我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。&lt;/p&gt;
&lt;p&gt;這次的系列文章是在基礎之上，讓大家得以理解不同模型之間
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>29 Autoregressive generative model</title>
    <link href="https://yuehhua.github.io/2018/10/29/29-autoregressive-generative-model/"/>
    <id>https://yuehhua.github.io/2018/10/29/29-autoregressive-generative-model/</id>
    <published>2018-10-29T15:07:42.000Z</published>
    <updated>2018-10-31T07:44:57.360Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。</p><p>在 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a> 這篇文章以及他的論文當中又在重述了這件事。</p><p>他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？</p><a id="more"></a><p>答案幾乎是肯定的。為什麼說幾乎是肯定的呢？因為他需要滿足一些條件，才能達成訓練上模型的穩定性要求。</p><h2 id="Stable-recurrent-models"><a href="#Stable-recurrent-models" class="headerlink" title="Stable recurrent models"></a>Stable recurrent models</h2><p>問題描述是這樣的：</p><blockquote><p>理論上來說，一個有 <strong>好行為（well-behaved）</strong> 的 recurrent neural network 是否總是可以被差不多大小的 feed-forward network 取代，在不損失效能的情況下？</p></blockquote><p>這時候我們就需要知道什麼樣的 RNN 是有好行為（well-behaved）的？</p><p>當然你可以設計一個非線性的 RNN 讓 feed-forward network 無法取代，只要讓他無法用 gradient descent 訓練起來就可以了。</p><p>也就是說，<strong>好行為</strong> 的 RNN 就是，有辦法用 gradient descent 訓練起來，而不會讓梯度爆炸或是消失的模型。這樣穩定（stable）的模型就有辦法用 feed-forward network 去逼近。</p><p>論文中證明了一個重要的定理（論文中有正式的版本），我先寫他的原始描述，然後解釋：</p><blockquote><p>Thm.</p></blockquote><p>Assume the system $\phi$ is <em>$\lambda$-contractive</em>. Under <em>additional smoothness</em> and <em>Lipschitz assumptions</em> on the system $\phi$, the prediction function $f$, and the loss $p$, if</p><p>$$<br>k \ge O(log(N^{1/(1-\lambda)^3} / (\epsilon (1-\lambda)^2)))<br>$$</p><p>then after N steps of projected gradient descent with decaying step size $\alpha_t = O(1/t)$, $||w_{recurr} - w_{trunc}|| \le \epsilon$, which in turn implies $||y_t(w_{recurr}) - y_t^k(w_{trunc})|| \le O(\epsilon)$.</p><p>當你把以上定理認真看完之後你就會昏了。基本上是說，一個模型本身會要滿足幾個條件：</p><ol><li>$\lambda$-contractive</li><li>additional smoothness</li><li>Lipschitz assumptions</li></ol><p>這幾個條件簡單來說，就是你的 loss function 需要是平滑的，那麼你的梯度就不會起伏太大，導致梯度爆炸或是消失的狀況。在這樣的狀況下，就可以用 feed-forward network 去逼近。</p><h2 id="Feed-forward-network-逼近"><a href="#Feed-forward-network-逼近" class="headerlink" title="Feed-forward network 逼近"></a>Feed-forward network 逼近</h2><p><img src="/images/wavenet.gif" alt=""></p><blockquote><p>動畫取自 <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">WaveNet 官網</a></p></blockquote><p>那用 feed-forward network 去逼近有什麼好處呢？</p><p>文章中提到三大好處：</p><ol><li>平行化：你可以善用 GPU 加速</li><li>可訓練：以往 RNN 都不好訓練，但是如果可以換成 feed-forward network 就會比較容易訓練起來</li><li>推論速度：在速度上更快</li></ol><p>近年來非常多的模型都採用了 auto-regressive 的架構，從前面提到的 Transformer，到新版的 Google 小姐 - WaveNet 都用了這樣的架構。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。&lt;/p&gt;
&lt;p&gt;在 &lt;a href=&quot;http://www.offconvex.org/2018/07/27/approximating-recurrent/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;When Recurrent Models Don’t Need to be Recurrent&lt;/a&gt; 這篇文章以及他的論文當中又在重述了這件事。&lt;/p&gt;
&lt;p&gt;他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>28 Transformer</title>
    <link href="https://yuehhua.github.io/2018/10/28/28-transformer/"/>
    <id>https://yuehhua.github.io/2018/10/28/28-transformer/</id>
    <published>2018-10-28T14:58:09.000Z</published>
    <updated>2018-10-31T07:44:47.883Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。</p><p>這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。</p><p>Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。</p><p>（跟變形金剛一樣的名字耶！帥吧！）</p><a id="more"></a><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>這個架構上延續了 encoder-decoder 的架構，encoder 會將輸入的序列 $(x_1, x_2, …, x_n)$ 轉換成 $\mathbf{z} = (z_1, z_2, …, z_n)$，而 decoder 會將 $\mathbf{z}$ 轉換成 $(y_1, y_2, …, y_m)$，一次轉換一個。在每一步當中，模型都是 auto-regressive 的，也就是說，前一次產生的結果會被當成下一次的輸入進行運算。</p><p>整個 Transformer 的架構就是在 encoder 及 decoder 上使用了 stacked self-attention 以及全連接的網路。我們來看圖，在圖的左半邊就是 encoder，右半邊就是 decoder：</p><p><img src="/images/transformer1.svg" alt=""></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder 由 N 個完全一樣的層堆疊（stack）起來（$N = 6$）。每層包含兩個子層，第一個是一個 multi-head self-attention 的機制，第二個是簡單的全連接層網路。每個子層外都包了 residual connection 以及 layer normalization，看起來就像 $LayerNorm(x + Sublayer(x))$。</p><p>Residual connection 主要可以將前層的資訊繞過一層，直接與下一層做運算。Layer normalization 有穩定學習效果的作用。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder 大致上與 encoder 相同，差別是在用了兩個 multi-head self-attention 的機制，所以總共有3個子層。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>在 attention 的機制上，我們在上一篇講過了。這邊我們要進一步探討這個模型用到的 scaled dot product attention。在這邊就是分成 query、key 跟 value 三者，首先要先將 query 跟所有 key 做過一次的內積，並且除以 $\sqrt{d_k}$，然後過一次 softmax 函數。計算到這邊就是權重的部份，最後權重再跟 value 去計算結果。其中 $d_k$ 是 key 向量的維度。公式在這邊：</p><p>$$<br>Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><p><img src="/images/transformer2.svg" alt=""></p><p>圖的左邊是 scaled dot product attention。為什麼要除以 $\sqrt{d_k}$ 呢？文章中有提到，內積會讓整個結果變很大，會讓梯度變得極小，這會不利於訓練，所以需要除以 $\sqrt{d_k}$。</p><p>在圖的右邊，是 multi-head self-attention，核心就是平行計算多個 scaled dot product attention 的結果，並把這些結果都串接起來。有了這樣的機制就可以不只注意一個地方，可以有多個關注點。</p><p>在 self-attention 的機制，意味著所有的 query、key 跟 value 都來自於自己。不像之前的 attention 橫跨 encoder 跟 decoder，所以資訊會從雙方而來。</p><p>在 Transformer 模型當中，有一個是 encoder-decoder attention layer，然後 encoder 跟 decoder 各有一個 self-attention layer，就是各自的第一個子層。</p><p>如此構成了整個 Transformer 模型，如果各位想知道這個模型的應用跟效能的話，請移駕去看論文，論文寫的還蠻簡單易懂的。</p><p>當然這麼模型當中有不少巧思在裡頭，有需要說明的話就提問囉！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。&lt;/p&gt;
&lt;p&gt;這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。&lt;/p&gt;
&lt;p&gt;Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。&lt;/p&gt;
&lt;p&gt;（跟變形金剛一樣的名字耶！帥吧！）&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>27 Attention model</title>
    <link href="https://yuehhua.github.io/2018/10/27/27-attention-model/"/>
    <id>https://yuehhua.github.io/2018/10/27/27-attention-model/</id>
    <published>2018-10-27T15:10:46.000Z</published>
    <updated>2018-10-31T07:44:40.087Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。</p><p>Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。</p><p>Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。</p><a id="more"></a><p>有幾篇論文算是開始用這樣的機制：</p><ol><li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li><li><a href="https://arxiv.org/abs/1509.00685" target="_blank" rel="noopener">A Neural Attention Model for Abstractive Sentence Summarization</a></li><li><a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noopener">Reasoning about Entailment with Neural Attention</a></li></ol><p>Attention model 在很多模型當中都是做為 encoder-decoder 之間的橋樑，原本的 encoder 跟 decoder 之間是只有一個 vector 來傳遞所有的訊息，但是多了 attention mechanism 就不一樣了。</p><p><img src="/images/seq2seq2.svg" alt=""></p><p>Attention mechanism 主要可以動態的去抓到 encoder 中傳遞的訊息，並且將這些訊息與 decoder 輸出的前一個訊息互相比對之後，透過線性組合之後輸出。這樣的輸出有什麼效果呢？他可以動態地去找到兩邊最相符的資訊，並且將他重要的部份以權重的方式凸顯出來，所以這部份是做線性組合。</p><p><img src="/images/attention.svg" alt=""></p><p>我看到一個廣義的描述方法，他是這樣說的，我們可以把 attention model 想成是一個函數，這個函數會吃兩種東西，一種是 query，另一種是 key-value 的資料結構，讓 query 去比對所有的 key 找到吻合的，會透過一個 compatibility function 去計算吻合的程度，並且作為權重，最後將權重與相對應的 value 做內積。在這邊 query、key、value 三者都是向量。</p><p>在這邊 query 會是 decoder 的 $z_0$，而 key 就是 encoder 的 $h^1, h^2, …$。每一個 key 都會去跟 query 個別通過 compatibility function 算一次吻合程度 $\alpha_0^1, \alpha_0^2, …$。這些 $\alpha_0^1, \alpha_0^2, …$ 就是權重，會去跟 value $h^1, h^2, …$ 做線性組合。在這邊為了簡單所以讓 value 跟 key 是一樣的，其實可以是不同的東西。計算出來的結果 $c^1$ 就是 context vector，會作為 decoder 的輸入，與 $z_0$ 一起計算出 $z_1$。</p><p>這樣就算是完成一輪 attention mechanism 了。下一次再繼續用 $z_1$ 當成 query 進行比對。</p><p>如此一來，就可以以動態的方式去產生序列了。Encoder 負責的是將輸入的序列轉成固定大小的向量，decoder 將這樣的向量轉換回序列，而中間需要動態調整的部份就像人的注意力一樣，會去掃視跟比對哪個部份的翻譯是最吻合的，然後將他做一個線性組合的調整。</p><p>今天的解析就到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。&lt;/p&gt;
&lt;p&gt;Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。&lt;/p&gt;
&lt;p&gt;Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
</feed>
