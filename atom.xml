<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dream Maker</title>
  
  <subtitle>Love Math, Science, Biology, Computer science</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yuehhua.github.io/"/>
  <updated>2018-10-23T00:53:57.844Z</updated>
  <id>https://yuehhua.github.io/</id>
  
  <author>
    <name>Yueh-Hua Tu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>22 Convolutional encoder-decoder 架構</title>
    <link href="https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/"/>
    <id>https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/</id>
    <published>2018-10-22T15:10:35.000Z</published>
    <updated>2018-10-23T00:53:57.844Z</updated>
    
    <content type="html"><![CDATA[<p>標題這不是一個專有名詞。</p><p>在電腦視覺的領域中有幾個有名的問題：</p><ol><li>影像辨識（Image recognition）</li><li>物件辨識（Object detection）</li><li>語意分割（Semantic segmentation）</li></ol><a id="more"></a><p><img src="/images/cv-tasks.jpg" alt=""></p><p>影像辨識是給一張影像，希望模型可以辨識出當中的東西是什麼。輸入模型的會是影像向量，輸出的會是類別向量。</p><p>物件辨識給的同樣是一張影像，除了需要辨識出當中的物件以外，還要給出這個物體所在的位置，輸出的除了類別向量以外，還有座標。</p><p>語意分割可能無法一眼看出當中的含意，對一個句子來說，詞本身帶有一些含意，對比到影像上，句子就是影像，而詞意就是影像中的物體。語意分割是給一張影像，需要將影像中的物件切割出來，所以必須對每個像素做分類。</p><p><img src="/images/semantic_seg.jpeg" alt=""></p><p>三者是各自不同的任務。</p><p>不同的任務有些共通性，這些共通性讓他們可能都可以適用 CNN 的架構。不過這麼說還是太過粗糙了。</p><p>對於影像辨識來說，一般性架構會是有 convolution layer 為主的 feature extractor，接著會是以 fully-connected layer 為主的 classifier。在不同階段有不同的目的，在輸入影像之後要先對影像進一步抽取特徵，有了足夠的特徵之後才進行分類。</p><p>語意分割也有類似的架構，在前面會有 convolution layer 為主的 feature extractor，但是為了將每個樣素做分類，必須對每一個像素做預測，預測像素的類別。在後半的部份，有人提出了 Fully convolution network，試圖做像素的類別預測。</p><p>像素的類別預測這件事從另一個角度切入，會很像是一種生成的過程。也就是，我們在前面要將影像的特徵萃取出來，是一種將資訊壓縮的過程，在後半我們希望將壓縮的資訊還原到某種程度，我們需要產生器（generator）。</p><p><img src="/images/segnet.svg" alt=""></p><blockquote><p><a href="http://mi.eng.cam.ac.uk/projects/segnet/" target="_blank" rel="noopener">SegNet [2015, University of Cambridge]</a></p></blockquote><p>在語意分割這個問題，後來就一路發展到了 encoder-decoder 架構，我們又回到類似 autoencoder 的樣子，讓 encoder 跟 decoder 一起訓練的模型架構。在這邊 encoder 就是 feature extractor，decoder 就是一種 generator。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;標題這不是一個專有名詞。&lt;/p&gt;
&lt;p&gt;在電腦視覺的領域中有幾個有名的問題：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;影像辨識（Image recognition）&lt;/li&gt;
&lt;li&gt;物件辨識（Object detection）&lt;/li&gt;
&lt;li&gt;語意分割（Semantic segmentation）&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>21 Activation functions and ReLU</title>
    <link href="https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/"/>
    <id>https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/</id>
    <published>2018-10-19T14:34:55.000Z</published>
    <updated>2018-10-19T14:34:55.385Z</updated>
    
    <content type="html"><![CDATA[<p>今天我們來談談 activation function 吧！</p><h2 id="先談談線性轉換"><a href="#先談談線性轉換" class="headerlink" title="先談談線性轉換"></a>先談談線性轉換</h2><p>談 activation function 之前先要談談線性轉換。</p><p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。</p><p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p><p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p><a id="more"></a><p>$$<br>A = U \Sigma V^T<br>$$</p><p>這 3 個矩陣分別是有意義的，$U$、$\Sigma$、$V$ 的意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：</p><p><img src="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300" alt=""></p><blockquote><p>圖來自以上提到的文章</p></blockquote><p>所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。</p><p>在模型上，我們還會加上一個 bias 來達到平移的效果。</p><p>我們可以來造一些點：</p><p><img src="/images/activation1.png" alt=""></p><p>我們可以 random 造一個矩陣：</p><p>$$<br>A =<br>\begin{bmatrix}<br>0.35166263&amp; 0.1124966 \\<br>0.25249535&amp; 0.65481947 \\<br>\end{bmatrix}<br>$$</p><p>所以我們可以對每個點做運算 $Ax$：</p><p><img src="/images/activation2.png" alt=""></p><h2 id="去吧！Activation-function！"><a href="#去吧！Activation-function！" class="headerlink" title="去吧！Activation function！"></a>去吧！Activation function！</h2><p>接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？</p><p><img src="/images/activation3.png" alt=""></p><p>大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。</p><p>所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。</p><p>那如果是用 sigmoid 的效果呢？</p><p><img src="/images/activation4.png" alt=""></p><p>他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。</p><p>我們放大來看看他整體形狀有沒有什麼變化。</p><p><img src="/images/activation5.png" alt=""></p><p>整體形狀有些微被扭曲了，不知道大家有沒有發現呢？</p><p>所以在引進 activation function 之後，模型擁有了 <strong>扭曲</strong> 的能力！</p><p>那麼 activation function 到底實際上做了什麼事呢？</p><h2 id="雕塑的工匠"><a href="#雕塑的工匠" class="headerlink" title="雕塑的工匠"></a>雕塑的工匠</h2><p>以 ReLU 來說，他就像一個工匠正在雕塑一個作品。</p><p>ReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：</p><p><img src="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/image/a51d2-dsc_7226.jpg" alt=""></p><blockquote><p>圖來自 <a href="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/" target="_blank" rel="noopener">漫遊‧藝術網＠成大校園</a></p></blockquote><p>原諒我私心用成大的雕塑品當範例。XD</p><p>ReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。</p><h2 id="消失的梯度"><a href="#消失的梯度" class="headerlink" title="消失的梯度"></a>消失的梯度</h2><p>以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。</p><p>其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。</p><p>那如果放在 CNN 的話，就會發生梯度消失的問題。</p><p>在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。</p><p>讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。</p><p>這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。</p><h2 id="模型怎麼知道要從哪裡下刀？"><a href="#模型怎麼知道要從哪裡下刀？" class="headerlink" title="模型怎麼知道要從哪裡下刀？"></a>模型怎麼知道要從哪裡下刀？</h2><p>我想蠻多人應該會有跟我一樣的問題。</p><p>我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？</p><p>答案是 backpropagation (gradient descent method) 會告訴你！</p><p>藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。</p><h2 id="ReLU-與-softplus"><a href="#ReLU-與-softplus" class="headerlink" title="ReLU 與 softplus"></a>ReLU 與 softplus</h2><p>softplus 是一個跟 ReLU 非常像的 activation function。</p><p>$$<br>softplus(x) = log(1 + e^x)<br>$$</p><p>兩者的差別：</p><p><img src="/images/Rectifier_and_softplus_functions.svg" alt=""></p><p>你可以把 softplus 看成 ReLU 的可微分版本，或是將 ReLU 看成 softplus 的簡化版本。</p><p>他在負的區域看起來跟 sigmoid function 很像，另一邊正的區域就會非常接近 identity function。</p><p>在行為上也很符合神經的特性，就是有正向的訊號就會是正向的輸出，如果是負向的訊號就不輸出。</p><p>很有趣的特性吧！</p><h2 id="為什麼-activation-function-一定要長這樣？"><a href="#為什麼-activation-function-一定要長這樣？" class="headerlink" title="為什麼 activation function 一定要長這樣？"></a>為什麼 activation function 一定要長這樣？</h2><p>其實沒有規定 activation function 一定要長怎樣。</p><p>但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。</p><p>所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>不同的模型跟問題其實適用不同的 activation function。</p><p>像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。</p><p>所以在 feature extraction 的階段就需要找適用的 activation function。</p><p>那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。</p><p>這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD</p><p>今天就到這邊告一個段落囉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天我們來談談 activation function 吧！&lt;/p&gt;
&lt;h2 id=&quot;先談談線性轉換&quot;&gt;&lt;a href=&quot;#先談談線性轉換&quot; class=&quot;headerlink&quot; title=&quot;先談談線性轉換&quot;&gt;&lt;/a&gt;先談談線性轉換&lt;/h2&gt;&lt;p&gt;談 activation function 之前先要談談線性轉換。&lt;/p&gt;
&lt;p&gt;有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。&lt;/p&gt;
&lt;p&gt;推薦可以看周老師的線代啟示錄 &lt;a href=&quot;https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;奇異值分解 (SVD)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>20 Convolutional neural network</title>
    <link href="https://yuehhua.github.io/2018/10/17/20-convolutional-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/17/20-convolutional-neural-network/</id>
    <published>2018-10-17T14:58:40.000Z</published>
    <updated>2018-10-17T14:58:40.101Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Convolution-layer"><a href="#Convolution-layer" class="headerlink" title="Convolution layer"></a>Convolution layer</h2><p>這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？</p><p>我們先來看二維的 cross-correlation 長什麼樣子。</p><a id="more"></a><p><img src="/images/ccor2d.svg" alt=""></p><p>然後是反序的 convolution。</p><p><img src="/images/conv2d.svg" alt=""></p><p>這樣有沒有搞懂一點 convolution 的運算了呢？</p><p>接著，想像在右手邊比較小的方框就是你的 filter （或是 kernel），然後他會沿著兩個軸去移動，去掃描看看有沒有跟 filter 的 pattern 很像的，當他偵測到很像的 pattern 的時候，輸出的 feature map 的值就會很高，所以這樣就可以做到上面講的第二點，也就是位移的不變性。不過說起來也不是真的有什麼位移不變性啦！他只是沿著軸去做掃描可以減少訓練的參數，這樣 filter 還是有在位移阿！只是對於要偵測的 pattern 看起來好像有”位移不變性”一樣。到這邊我們第一點跟第二點都解決了，剩下第三點。</p><h2 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h2><p>跟第三點相關的就是 subsampling，也就是如果讓圖片變小，不只可以降低要辨識的區塊大小，還可以降低需要訓練的參數量。那要怎麼讓圖片變小？</p><p>Maxpooling 是目前主流的方法，也就是在一個 window 的範圍內去找最大值，只保留最大值。還有一種是 meanpooling，顧名思義，他取整個 window 的平均值作為保留值。</p><p>所以 subsampling 在一些應用場景是需要的，有些是不需要的，像是有些 pattern 的辨識是不能去除細節的，一旦去除細節就會造成辨識困難，那就代表他沒有辦法用 subsampling。有時候照片縮小到一定程度人類也會無法辨識當中的圖像是什麼，所以也不要用過頭。</p><h2 id="Feature-extractor-classifier-架構"><a href="#Feature-extractor-classifier-架構" class="headerlink" title="Feature extractor-classifier 架構"></a>Feature extractor-classifier 架構</h2><p>經過以上介紹後，我們可以把 convolution layer 跟 subsampling 結合起來，成為所謂的 feature extractor。</p><p>經由以上三點特性，這些 layer 的巧妙運用可以是非常棒的 feature extractor。不同種的資料特性需要不同設計的 feature extractor，接下來就是 classifier 上場了。</p><p>典型的 classifier 可以是 SVM，或是要用比較潮的 deep learning 也可以，最單純的就是前一篇提到的 MLP 了。</p><p>這樣前後組合好就是個 CNN 的雛型了！</p><h2 id="MLP-做不好的事情"><a href="#MLP-做不好的事情" class="headerlink" title="MLP 做不好的事情"></a>MLP 做不好的事情</h2><p>前一篇我們有提到 MLP 因為會從整體特徵去做內積，所以整體的模式會優先被考慮，如果有區域性的特徵並不一定會被凸顯出來。在 MLP 相對上會比較注重整體性而不是區域性，所以使用 MLP 在影像處理上就比 CNN 不是那麼有利。</p><h2 id="關鍵在哪裡？"><a href="#關鍵在哪裡？" class="headerlink" title="關鍵在哪裡？"></a>關鍵在哪裡？</h2><p>我個人認為關鍵在資料的 <strong>區域性</strong>，也就是你想做的事情其實是跟資料的區域性有關係，或是你的資料是週期性資料（週期性出現的模式也可以視為是一種區域性模式重複出現），這樣你就可以用 convolution layer！（注意！不是 CNN！）</p><p>像是音樂當中有週期性的模式就可以用，在生物領域，蛋白質會去辨認 DNA 序列，有被辨認到的部份就會有蛋白質黏附上去，生物學家會想知道到底哪些地方有蛋白質黏附，黏附的序列是區域性的，所以也有應用 CNN 技術在這方面上。</p><p>最重要的還是去了解你的資料的特性，然後了解模型當中各元件的性質跟數學特性，這樣才能正確地使用這些技術解決問題，走到這邊需要同時是領域知識的專家，也同時是 DL 的專家才行阿！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Convolution-layer&quot;&gt;&lt;a href=&quot;#Convolution-layer&quot; class=&quot;headerlink&quot; title=&quot;Convolution layer&quot;&gt;&lt;/a&gt;Convolution layer&lt;/h2&gt;&lt;p&gt;這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？&lt;/p&gt;
&lt;p&gt;我們先來看二維的 cross-correlation 長什麼樣子。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>19 Convolution 運算</title>
    <link href="https://yuehhua.github.io/2018/10/17/19-convolution-operation/"/>
    <id>https://yuehhua.github.io/2018/10/17/19-convolution-operation/</id>
    <published>2018-10-17T14:58:29.000Z</published>
    <updated>2018-10-18T13:45:31.881Z</updated>
    
    <content type="html"><![CDATA[<p>熱身運動都做好了，接下來我們就一路往影像處理上的重要技術 CNN 前進啦！</p><p>Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。</p><p>那為什麼 convolution 這麼重要，重要到要放在名稱上呢？</p><a id="more"></a><p>我先推荐大家去看台大李宏毅老師的介紹，看完再繼續往下看文章。</p><iframe width="560" height="315" src="https://www.youtube.com/embed/FrKWiRv254g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><hr><h2 id="影像處理中的祕密"><a href="#影像處理中的祕密" class="headerlink" title="影像處理中的祕密"></a>影像處理中的祕密</h2><p>我們希望從資料當中找到某種規律，這種規律我們叫作模式（pattern），模式是會重複出現的特徵，像是你可以辨識出蘋果，因為一顆蘋果有他特定的特徵，像是顏色、形狀、大小等等，當這些組合起來，並且他們都會一起出現而且重複出現，我們就可以稱他為模式。</p><p>在影片當中有提到在影像處理上有幾個特點：</p><ol><li>一些模式比整張圖小</li><li>同樣的模式可能出現在圖片的不同地方</li><li>縮放圖片不影響圖片中物件的辨識</li></ol><p>第 1 點講的是，要去辨識一個模式並不需要整張圖，也就是，<strong>local 的資訊比 global 的資訊重要</strong>。在影片中有舉例，一隻鳥的特徵有鳥喙、羽毛、翅膀等等特徵，你並不會去在意他背景的圖片長什麼樣子。鳥喙這樣的特徵他是 <strong>區域性的</strong>，你不需要整張圖片的資訊去判斷這張圖是不是鳥喙，所以在設計模型的原則上需要去擷取區域性的資訊。</p><p>第 2 點講的是，同樣的模式可能會出現在不同圖片的不同地方，這邊其實隱含了一個概念，就是 <strong>位移不變性（translation invariance）</strong>。由於同樣模式可以在不同地方上被找到，所以我們只需要一個 node 去偵測他就好了 ，這樣的話可以節省非常多的 node（或是 weight），這稱為 shared weight。</p><p>第 3 點，如果圖片縮放不影響圖片辨識，那麼。這時候我們可以做 subsampling，除了可以減少資料處理的量，也不會影響圖片的辨識結果。</p><hr><p>在我們講 CNN 之前先來幫大家惡補一下 convolution 這個運算。</p><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>在 CNN 中會用到的重要運算就是 convolution，而在 CNN 中有所謂的 convolution layer，也就是這一層的運算就是做 convolution 而不是直接內積。</p><p>我們說到 convolution layer，他真正的作用是用來做什麼的呢？他其實是用來 <strong>擷取 local 的資訊</strong> 的。</p><p>承接前面第一點提到的，在圖片當中，pattern 是 local 的資訊而不是 global 的，而 pattern 是我們想抓的資訊，所以我們要的資訊只有 local 的而已。</p><p>那麼 convolution 要如何擷取 local 的資訊呢？</p><h2 id="Convolution-運算"><a href="#Convolution-運算" class="headerlink" title="Convolution 運算"></a>Convolution 運算</h2><p>我們先來看 convolution 的原始定義，這邊假設兩個函數 $f$、$g$，則兩個函數的 convolution 為：</p><p>$$<br>(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d\tau<br>$$</p><p>以上我們看到他是一個積分式，當中引入了另一個變數 $\tau$，他代表的是<em>一個時間區間</em>，那接下來是兩個函數的<em>相乘</em>，然後將他們對變數 $\tau$ <em>積分</em>。我們來想想看，先不管變數 $\tau$，相乘後積分的運算跟什麼樣的運算很像？</p><p>是的！內積！我們來看看函數的內積長什麼樣子。</p><p>$$<br>\lt f, g \gt = \int_{-\infty}^{\infty} f(t) g(t) dt<br>$$</p><p>什麼？你跟我說這不是你認識的內積？不不不，你認識的內積其實是這個內積的離散版本。</p><p>$$<br>\lt f, g \gt = \sum_{i=1}^{n} f_i g_i<br>$$</p><p>$$<br>&lt;a, b&gt; = \sum_{i=1}^{n} a_i b_i = \mathbf{a}^T\mathbf{b}<br>$$</p><p>這樣是不是比較清楚一點了？我們來比較一下，因為積分是在 <strong>連續空間的加總</strong>，相對應的加總就是在 <strong>離散空間</strong> 的版本，那麼在連續空間上需要一個 $d\tau$ 來把連續空間切成一片一片的，但是在離散空間上，他很自然的就是 $1$ 了。這樣是不是又發覺他們根本是一樣的呢？</p><p>那你知道函數其實是一種向量嗎？不知道的同學表示沒有讀過或是沒有認真讀線性代數。</p><p>那這樣大家應該接受了函數的內積以及向量的內積其實是一樣的。接下來我們來討論一下那個神奇的 $\tau$。</p><p>$\tau$ 是一個時間區間，而積分其實是在對這個時間區間做切片然後加總，他其實跟我們在做訊號處理上的 window 的概念是一樣的，所以他其實是在某個 window 中做內積的意思。我們先來看看有 window 的內積長什麼樣子。</p><p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n + m]<br>$$</p><p>在下圖我們可以假想左邊的向量是 $b$，右邊的是 $a$，而向量 $a$ 是有被 window 給限定範圍的（m = 1…k），所以在下面這張圖就是當 n = 1、m = 1…4 的時候的情境。箭頭則是向量元素相乘的對象，每次內積完，n 就會往下移動一個元素。</p><p><img src="/images/ccor1d.svg" alt=""></p><p>計算完之後就變成一個新的向量，這就是 window 版本的內積的運作原理了！他其實有一個正式的名字，稱為 cross-correlation。</p><p>我們來看看把 convolution 離散化來看看是長什麼樣子。剛剛我們看到的 convolution 是連續的版本，是函數的版本，那我們實際上的運算是以向量去操作的，那麼離散版本的 convolution 是：</p><p>$$<br>(a * b)[n] = \sum_{m=-\infty}^{\infty} a[m] b[n - m]<br>$$</p><p>這邊的 window 就是 $m$ 這個參數，其實我們可以給他一個區間，不要是負無限大到正無限大。</p><p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n - m]<br>$$</p><p>所以這邊的 window 大小調成是 $k$ 了！</p><p><img src="/images/conv1d.svg" alt=""></p><p>你會發現，convolution 會跟 cross-correlation 很像，差別在於順序，也就是 convolution 內積的順序是相反的，所以他在數學式上的表達是用相減的，這邊的情境是 n = 6、m = 1…4。</p><p>我們來總結一下 convolution 這個運算，他其實是 local 版本的內積運算，而且他的內積方向是反序的。</p><p>補充一點，convolution 其實也是一種線性的運算喔！是不是跟前面談到的線性模型 $\sigma(W^T\mathbf{x} + \mathbf{b})$ 有點異曲同工的感覺呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;熱身運動都做好了，接下來我們就一路往影像處理上的重要技術 CNN 前進啦！&lt;/p&gt;
&lt;p&gt;Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。&lt;/p&gt;
&lt;p&gt;那為什麼 convolution 這麼重要，重要到要放在名稱上呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>18 Multi-layer preceptron</title>
    <link href="https://yuehhua.github.io/2018/10/17/18-multi-layer-preceptron/"/>
    <id>https://yuehhua.github.io/2018/10/17/18-multi-layer-preceptron/</id>
    <published>2018-10-17T14:58:18.000Z</published>
    <updated>2018-10-17T14:58:18.099Z</updated>
    
    <content type="html"><![CDATA[<p>我們來更具體一點講 multi-layer perceptron (MLP)。</p><p>最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。</p><a id="more"></a><hr><p>一般線性模型：$f(\mathbf{x}) = W^{\prime T}\mathbf{x} + b = W^T\mathbf{x}$</p><p>Linear MLP：</p><p>$f_1(\mathbf{x}) = W_1^T\mathbf{x}$</p><p>$f_2(\mathbf{x}) = W_2^Tf_1(\mathbf{x})$</p><p>$f_3(\mathbf{x}) = W_3^Tf_2(\mathbf{x})$</p><p>…</p><p>$f_n(\mathbf{x}) = W_n^Tf_{n-1}(\mathbf{x})$</p><hr><p>那這樣這個有什麼好講的呢？</p><p>大家應該有看到在這邊唯一的運算：內積（inner product）</p><h2 id="內積的意義"><a href="#內積的意義" class="headerlink" title="內積的意義"></a>內積的意義</h2><p>有念過線性代數的人應該對內積這個運算還算熟悉（在這邊都假設大家有一定線性代數基礎）。</p><p>$$<br>&lt;\mathbf{x}, \mathbf{y}&gt; = \mathbf{x}^T \mathbf{y}<br>$$</p><p>$$<br>= \begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}^T</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}</p><p>=<br>\begin{bmatrix}<br>x_1, x_2, \cdots, x_n<br>\end{bmatrix}</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p><p>內積，要先定義矩陣相乘的運算，而矩陣的相乘其實是一種線性轉換。</p><p>$$<br>f(\mathbf{x}) = A\mathbf{x}<br>$$</p><p>我們來觀察一下內積這個運算，這兩個向量會先把相對應的分量相乘。</p><p>$$<br>\begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}</p><p>\leftrightarrow</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p><p>接著，再相加。</p><p>$$<br>x_1y_1 + x_2y_2 + \cdots + x_ny_n<br>$$</p><p>這時候我們可以想想看，如果當一邊是權重另一邊是資料的時候所代表的意義是什麼？</p><p>當兩個分量的大小都很大的時候，相乘會讓整個值變很大，相對，如果兩個都很接近零的話，結果值就不大。如果很多分量乘積結果都很大，相加會讓整體結果變得很大。</p><p>內積，其實隱含了 <strong>相似性</strong> 的概念在裡面，也就是說，如果你的權重跟資料很匹配的話，計算出來的值會很大。大家有沒有從裏面看出些端倪呢？</p><p>我們再由另一個角度切入看內積，內積我們可以把他寫成另一種形式，這個應該在大家的高中數學課本當中都有：</p><p>$$<br>\mathbf{x}^T \mathbf{y} = ||\mathbf{x}|| ||\mathbf{y}|| cos \theta<br>$$</p><p>這時候我們就可以看到內積可以被拆成3個部份：分別是兩個向量的大小跟向量夾角的 $cos \theta$ 值。</p><p>而當中 $cos \theta$ 就隱含著相似性在裡頭，也就是說，當兩個向量的夾角愈小，$cos \theta$ 會愈接近 1。相反，如果兩個向量夾角愈接近 180 度，那 $cos \theta$ 會愈接近 -1。剛好呈現 90 度就代表這兩個向量是 <strong>沒有關係的</strong>。</p><p>這時候可能有人會說內積又不是完全反應相似性而已，沒錯！因為他也考慮了兩個向量的長度，當一組向量夾角與另一組向量夾角相同，但是第1組的向量長度都比較長，那內積的結果第1組向量就會比較大。</p><p>所以內積是沒有去除掉向量長度因素的運算，如果單純想要用向量夾角來當成相似性的度量的話可以考慮用 cos similarity。</p><p>$$<br>cos \theta = \frac{\mathbf{x}^T \mathbf{y}}{||\mathbf{x}|| ||\mathbf{y}||}<br>$$</p><h2 id="內積與-MLP"><a href="#內積與-MLP" class="headerlink" title="內積與 MLP"></a>內積與 MLP</h2><p>那 MLP 當中內積扮演了什麼樣的角色呢？</p><p>在純粹線性的 MLP 當中，多層的 $f(\mathbf{x})$ 疊起來，我們可以把他看做是做非常多次的線性轉換或是座標轉換（change of basis），但是這是在 inference 階段的解釋。</p><p>那在 training 階段內積扮演了什麼樣的角色呢？</p><p>這邊提供一個新的想法：在 training 的過程中，我們的 dataset 是不變的，會變動的是 weight ，而內積則是在衡量這兩者之間的 feature norm 及向量夾角，所以 weight 會調整成匹配這樣特性的樣子。換句話說，內積考慮了 data 與 weight 之間的相似性與大小，並且藉由 training 去調整 weight 讓他與資料匹配。</p><p>在 inference 階段，你就可以把他看成是，weight 正在幫你做出某種程度的篩選，跟 weight 匹配的資料，內積值就會比較大，相對的是，weight 不匹配的資料，內積值就會比較小，藉由這樣將內積結果遞進到下一層的運算。</p><h2 id="機率與內積"><a href="#機率與內積" class="headerlink" title="機率與內積"></a>機率與內積</h2><p>其實還有一個觀點，就是機率觀點，機率要求一個 distribution 的長度為 1，$\int_{-\infty}^{\infty} P(X) = 1$。在這邊我們的 distribution 常常以一個 vector（或是 random variable）的形式呈現。事實上就是把一個計算好的向量去除以他的長度。如此一來，我們就去除了長度影響的因素，以符合機率的要求。</p><p>那機率當中的內積指的是什麼呢？</p><p>你如果動動手 google 一下就會發現在機率當中的內積就是這個運算</p><p>$$<br>\mathbb{E}[XY] = \int XY dP<br>$$</p><p>如果有念過統計的人，是不是覺得這東西很眼熟呢？</p><p>$$<br>cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]<br>$$</p><p>是的！他跟共變異數是有相關的，共變異數還是跟我們要去度量兩個隨機變數之間的 <strong>相似性</strong> 有關係。</p><p>$$<br>\rho = \frac{cov(X, Y)}{\sigma_X \sigma_Y}<br>$$</p><p>只要把他除以隨機變數的標準差就可以得到相關係數了呢！</p><h2 id="加入非線性"><a href="#加入非線性" class="headerlink" title="加入非線性"></a>加入非線性</h2><p>事實上，在我們生活中遇到的事物都是非線性的居多，線性模型可以施展手腳的範疇就不大了。</p><p>這時我們就希望在 MLP 中加入非線性的元素以增加模型的表達力。這時候模型的每一層就變成了：</p><p>$$<br>f(\mathbf{x}) = \sigma (W^T \mathbf{x})<br>$$</p><p>而當中的 $\sigma$ 就成了我們的 activation function 了，也就是非線性的來源！</p><h2 id="Fully-connected-layer"><a href="#Fully-connected-layer" class="headerlink" title="Fully connected layer"></a>Fully connected layer</h2><p>當這些層的 node 都互相連接，就代表了所有 node 都參與了計算，這個計算所考慮的資料是 <strong>global</strong> 的。</p><p>這些層所做的運算是相對 <strong>簡單</strong> 的（相對 convolution 來說）。</p><p>每個 node 對每一層運算所做的貢獻是 <strong>弱</strong> 的。當一層的 node 數很多，e.g. 上千個 node，每個 node 的運算結果就會被稀釋掉了。即便內積運算有包含個別值的大小的成份在裡頭，當 node 數一多，這樣的影響也會被減弱，剩下的是整體向量與向量之間的相似性。但有一個情況例外，當有 node 的值極大，e.g. $x_i / x_j = 1000$，當有人是別人的千倍以上的話就要注意一下了，這也是很常在機器學習當中會遇到的問題，這時候就會需要做 normalization 來處理。</p><p>最後提醒，內積的運算中雖然有隱含相似性在其中，但是他 <em>不等同</em> 於 <strong>去計算相似性</strong>。</p><p>今天的討論就到這邊告一個段落，希望在大家思考 deep learning 模型的時候，這些東西有幫上一些忙。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們來更具體一點講 multi-layer perceptron (MLP)。&lt;/p&gt;
&lt;p&gt;最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>17 Autoencoder</title>
    <link href="https://yuehhua.github.io/2018/10/17/17-autoencoder/"/>
    <id>https://yuehhua.github.io/2018/10/17/17-autoencoder/</id>
    <published>2018-10-17T14:36:06.000Z</published>
    <updated>2018-10-19T15:26:36.709Z</updated>
    
    <content type="html"><![CDATA[<p>既然前一篇提到學習特徵是一件重要的事，那麼我們就來講講 autoencoder 吧！</p><p>Autoencoder 就是一個 unsupervised 方法，試圖學習出可以用的特徵。雖然不少人可能會說他是一個壓縮的方法或是一個降維的方法，其實他都是。</p><a id="more"></a><p>Autoencoder 就是希望給一個 input ，經過轉換之後會成為一個新的、維度比較低的向量，並且可以再由這個向量透過轉換還原成原本的 input。既然低維的向量可以還原成 input 的樣子，那代表他含有足夠的資訊可以還原，所以你可以把他看成是一種壓縮或是降維。我想強調的是，autoencoder 可以將一個 input 的樣子轉換成一個新的表示方式，我們就可以用新的視角來看待他。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>Autoencoder 算是一個非常簡單的模型，他就是將資料向量經過一個轉換過後成為新的向量，在讓新的向量經過轉換過後還原成原本的向量。</p><p>抽象上來說比較像是：</p><p>$$<br>X \rightarrow Z \rightarrow X<br>$$</p><p>Autoencoder 可以被拆解成兩個部份：一個是 encoder $X \rightarrow Z$，另一個是 decoder $Z \rightarrow X$ 的部份。</p><p>在這個架構上 encoder-decoder 是相接在一起做訓練的。</p><p>最簡單的 autoencoder 在 encoder 跟 decoder 上各自都只有一層，當然你可以不只一層，取決於你自己的狀況。</p><p>他是一個非常簡單的架構，所以有非常多的變體，像是 denoising autoencoder、sparse autoencoder、variational autoencoder…等等。</p><p>每個都有自己獨特的用法，所以這是一個非常基礎的模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;既然前一篇提到學習特徵是一件重要的事，那麼我們就來講講 autoencoder 吧！&lt;/p&gt;
&lt;p&gt;Autoencoder 就是一個 unsupervised 方法，試圖學習出可以用的特徵。雖然不少人可能會說他是一個壓縮的方法或是一個降維的方法，其實他都是。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>16 Representation learning</title>
    <link href="https://yuehhua.github.io/2018/10/16/16-representation-learning/"/>
    <id>https://yuehhua.github.io/2018/10/16/16-representation-learning/</id>
    <published>2018-10-16T15:28:04.000Z</published>
    <updated>2018-10-16T15:32:43.024Z</updated>
    
    <content type="html"><![CDATA[<p>機器學習的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。</p><p>當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個成效，像是你給的特徵不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些資料當中，那麼模型根本就預測不出來，所以玩過機器學習的人就會知道特徵工程的重要性。</p><a id="more"></a><p>以往特徵工程是需要人自己手動處理的，如今我們也希望由機器學習的模型中自動學出來。大家可以看到我們的技術進展：從以往的手寫程式進展到經典的機器學習技術，這是一個巨大的飛躍。</p><p><img src="/images/diagram-deep-learning.png" alt=""></p><blockquote><p>From <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?ie=UTF8&amp;qid=1472485235&amp;sr=8-1&amp;keywords=deep+learning+book" target="_blank" rel="noopener"><em>Deep Learning</em></a> by Ian Goodfellow and Yoshua Bengio and Aaron Courville</p></blockquote><hr><p><br><br></p><h2 id="他幫我們解決了什麼問題呢？"><a href="#他幫我們解決了什麼問題呢？" class="headerlink" title="他幫我們解決了什麼問題呢？"></a>他幫我們解決了什麼問題呢？</h2><p>以往的手寫程式需要工程師非常的聰明，他需要知道在 input 與 output 之間的所有規則，然後把這些規則化成可以執行的程式，這些實作的過程需要花非常大量的人力跟腦力。</p><p><br></p><p><img src="/images/before_ml.svg" alt=""></p><p><br></p><p>然而，我們進展到機器學習的技術，我們試圖去收集一些資料，這些資料符合我們預期的 input 與 output 之間的關係。</p><p><br></p><p><img src="/images/after_ml.svg" alt=""></p><p><br></p><p>他可以幫我們將中間的 <strong>過程</strong> 連接起來，我們不需要去 <em>手刻</em> 或是 <em>事先知道</em> 這些過程，更何況自然界很多過程都是 <strong><em>人類沒辦法理解的</em></strong> 或是 <strong><em>還不知道的</em></strong>。</p><p><br></p><p><img src="/images/mnist.svg" alt=""></p><p><br></p><p>這些過程在數學家的眼中就稱為 <strong>函數</strong>，對於機器學習專家來說，input 與 output 之間有無限多種函數的可能。哪一種可能才是最符合我們資料的長相的？我們希望挑出最有可能的那一種，就把那就把那一種當成是模型，並且輸出，這樣我們就能讓機器自動去學出 input 與 output 的對應關係，這是一個飛躍性的進展。</p><p><br></p><h2 id="特徵工程（feature-engineering）"><a href="#特徵工程（feature-engineering）" class="headerlink" title="特徵工程（feature engineering）"></a>特徵工程（feature engineering）</h2><p>接著我們意識到：我們還是需要手動去處理特徵。經典的機器學習模型只幫我們處理了 <strong>將特徵對應到輸出的關係</strong>，我們還是得藉由特徵萃取的技術來轉換，而我們很難知道什麼樣的特徵萃取才真正能夠把資料中我們想要的資訊萃取出來，這部分就進到 representation Learning 的範疇。</p><p><br></p><h2 id="自動化特徵萃取（Automatic-feature-extraction）"><a href="#自動化特徵萃取（Automatic-feature-extraction）" class="headerlink" title="自動化特徵萃取（Automatic feature extraction）"></a>自動化特徵萃取（Automatic feature extraction）</h2><p>在特徵萃取的過程中，常常我們面對的是高維度的向量，由於我們很難去理解高維度的向量之間的轉換，導致我們在轉換的時候會遇上困難，我們根本不知道需要轉換成什麼樣維度的向量，我們也不知道中間需要什麼樣的轉換函數。在數學領域當中，有相關的領域稱為微分幾何，所以常常我們會討論在數學上的流形（manifold），representation learning 就是希望連同特徵萃取以及模型可以一併處理，也就是藉由模型的過程會到回饋（從 gradient descent 等等方法），去引導特徵萃取的過程，進而去學到 <strong>特徵-特徵</strong> 之間轉換的模式。</p><p><br></p><h2 id="深度學習"><a href="#深度學習" class="headerlink" title="深度學習"></a>深度學習</h2><p>深度學習就是一種 representation Learning。他希望資料在高維度的轉換當中，可以去萃取出足夠而抽象的資訊，去進行預測。而深度學習只是將特徵-特徵之間的轉換模式以 <strong>層-層</strong> 之間的轉換實現，而高維的特徵向量以 <strong>層</strong> 的形式呈現。所以越深的網路代表著經過多次的函數處理跟萃取，所萃取的資訊的抽象程度越高，抽象程度越高，就越接近人類所想像的。</p><p><br></p><h2 id="Representation-learning"><a href="#Representation-learning" class="headerlink" title="Representation learning"></a>Representation learning</h2><p>如同前面描述到的，我們需要更少對於特徵工程的依賴，增加自動化特徵萃取的使用。所以我們用”學習”的方式讓模型自動去學到他要的特徵，自動去做特徵萃取。那麼 representation learning 更深層的意思是什麼呢？</p><blockquote><p>你需要的不是一個答案，而是一個表示方式。</p></blockquote><p>以上是我在工研院的課程對學員們講過的話，一句話解釋 representation learning 大概是這個意思。</p><p>舉個例子好了，人類在照片中可以辨識出當中的狗狗，人們在交談的時候可以以語言的”狗”來描述同一個概念，基本上他們都擁有相同的資訊量。對於狗的概念來說，照片中的圖像只是這個概念的一種表示方式，語言中也有對應的表現方式。讓機器學會狗的概念，就是要讓他可以從圖像或是語言中可以萃取出有相同資訊量的東西，這樣的東西可以是以資料結構的方式表示，或是以一個向量表示，所以你需要的不是一個答案，而是一種表示方式，讓你可以看的懂的表示方式。</p><p>最終極的情況來說，在人類腦中很多既定的概念都已經存在，並沒有什麼新的概念出現了。出現的只有新的概念以不同的形式或是姿態出現，這互相之間都是可以轉換的，當然，轉換伴隨著資訊量的流失。</p><p>記得有個實驗在測試在發展成熟的社會是不是比原始社會的人們更聰明，實驗者設計了類似配對記憶遊戲，在卡牌上畫上各種現代日常生活中會看到的物件，分別測試了都市的人們以及原始部落的人們，果然在都市的人們有比較好的成績。不過這個實驗有個為人詬病的地方，都市人當然熟悉日常生活周遭的物件，原始部落的人們卻從來也沒有看過這些東西。所以又有另一組人馬將實驗換成在原始部落中常常看到的植物的葉子，對都市的人們來說，那些葉子根本無從分辨，但是原始部落的人們的測驗結果卻跟都市的人們對日常生活物件有一樣高的分數。代表人的腦袋並沒有差別，只是認得的東西不同。</p><p>或許在人們的腦袋中，有某些概念是一樣的，但是有不同的表現形式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;機器學習的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。&lt;/p&gt;
&lt;p&gt;當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個成效，像是你給的特徵不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些資料當中，那麼模型根本就預測不出來，所以玩過機器學習的人就會知道特徵工程的重要性。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>15 為什麼要深？</title>
    <link href="https://yuehhua.github.io/2018/10/15/15-why-deep/"/>
    <id>https://yuehhua.github.io/2018/10/15/15-why-deep/</id>
    <published>2018-10-15T06:56:39.000Z</published>
    <updated>2018-10-15T07:03:37.605Z</updated>
    
    <content type="html"><![CDATA[<p>接著我們就來到了蠻重要的問題，既然一個 hidden layer 的網路架構就可以逼近任何連續函數，那麼為什麼要深度學習？</p><a id="more"></a><p>對於這個問題，台大李宏毅老師有非常詳細的講解：</p><iframe width="560" height="315" src="https://www.youtube.com/embed/FN8jclCrqY0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>影片裏面實驗很多，所以我還是總結一下：</p><ol><li>用邏輯電路類比神經網路的話，你一樣可以用單層的並聯將所有邏輯閘都起來成為一個電路，一樣可以達到相同的效果，但是用多層的串聯可以將所需要的邏輯閘數目減少（類比神經網路的神經元），所以可以達到減少參數的效果。</li><li>使用 ReLU 作為 activation function 就是要用分段線性的方式來逼近一個函數，在網路參數相進的情況下，單層網路所能產生的”段”比較少，多層網路所產生的”段”比較多，產生的線段較多就可以去逼近一個更複雜的函數，所以模型就比較強大。</li><li>計算分段線性的數量，當你有 $N$ 個神經元，單層網路裡最多只能產生 $N - 1$ 個線段，多層網路，每層安排兩個神經元，可以產生 $2^{\frac{N}{2}}$ 個線段。</li></ol><p>更有文獻提到，計算線段的數量，如果你的網路每層有 $K$ 個神經元，而且有 $H$ 層，那麼至少會有 $K^H$ 個線段。</p><p>由於深度是放在指數上面，所以增加深度就可以簡單地提高模型的複雜度，也就可以讓模型變得比較強大。</p><h2 id="計算複雜度"><a href="#計算複雜度" class="headerlink" title="計算複雜度"></a>計算複雜度</h2><p>在一些電腦的問題上，我們常常形容問題是 NP 或者不是 NP，來描述一個問題的複雜度有多高。</p><p>一個問題的時間複雜度可以在多項式時間內的，我們稱為 P，如果不是，那我們稱為 NP。</p><p>NP-complete 問題是 NP 問題當中最難的了，計算複雜度大概會是指數級成長，這種成長速度應該使用神經網路有辦法克服。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接著我們就來到了蠻重要的問題，既然一個 hidden layer 的網路架構就可以逼近任何連續函數，那麼為什麼要深度學習？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>14 淺層神經網路</title>
    <link href="https://yuehhua.github.io/2018/10/14/14-shallow-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/14/14-shallow-neural-network/</id>
    <published>2018-10-14T15:16:02.000Z</published>
    <updated>2018-10-15T07:03:50.798Z</updated>
    
    <content type="html"><![CDATA[<p>為什麼大家到現在都這麼迷神經網路模型？</p><p>我想主因不是因為他是模擬生物而來，他有一些更扎實的數學特性。</p><p>我們前面講過各種線性模型，然後將他過渡到神經網路。</p><p>今天要告訴大家，即便是淺層的神經網路也是很厲害的。</p><a id="more"></a><h2 id="Universal-approximation-theorem"><a href="#Universal-approximation-theorem" class="headerlink" title="Universal approximation theorem"></a>Universal approximation theorem</h2><p>Universal approximation theorem 是個淺層的神經網路的數學定理。</p><p>他說：一個簡單的 feedforward network，只包含了一個 hidden layer，並且有適切的 activation function，包含有限個神經元的情況下，可以去逼近任何連續函數。</p><p>在 1989 就以經由 George Cybenko 先生第一次證實了這個定理，他使用了 sigmoid activation function。</p><p>如果大家對細節有興趣，請參閱 <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" target="_blank" rel="noopener">維基百科的條目</a>。</p><p>這代表什麼呢？</p><p>到目前為止，我們都會將一個現實中的問題化成一個數學問題，一個數學問題基本上都是包含函數的。</p><p>像是影像辨識，我們就可以看成一個可以輸入影像的函數，這個函數會輸出辨識結果，也就是分類。</p><p>我們可以這樣寫 $f: \text{images} \rightarrow \text{classes}$。</p><p>所以語音辨識就是 $f: \text{speech} \rightarrow \text{text}$。</p><p>聊天機器人就是 $f: \text{text} \rightarrow \text{text}$。</p><p>…</p><p>到這邊你可以想想，幾乎人類的問題都可以化成一個函數來解答。</p><p>所以可以逼近任何連續函數的模型根本就可以解答任何問題的意思阿！</p><p>所以大家才拼了命的用這個模型去解決很多問題。</p><h2 id="待解問題"><a href="#待解問題" class="headerlink" title="待解問題"></a>待解問題</h2><p>如果這個模型這麼萬能，那麼他就真的沒有缺點嗎？</p><p>我們需要從這個定理切入，定理中描述的有限個神經元，但至少不是無限，他並沒有說需要幾個。</p><p>這理所當然，因為沒有人知道你的問題有多複雜，需要用多難的方法解嘛！</p><p>然後 activation function 也是沒有提的。</p><p>所以這就是留給現代的大家去決定的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;為什麼大家到現在都這麼迷神經網路模型？&lt;/p&gt;
&lt;p&gt;我想主因不是因為他是模擬生物而來，他有一些更扎實的數學特性。&lt;/p&gt;
&lt;p&gt;我們前面講過各種線性模型，然後將他過渡到神經網路。&lt;/p&gt;
&lt;p&gt;今天要告訴大家，即便是淺層的神經網路也是很厲害的。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>13 Kernel SVM 與 RBF network</title>
    <link href="https://yuehhua.github.io/2018/10/14/13-kernel-svm-and-rbf-network/"/>
    <id>https://yuehhua.github.io/2018/10/14/13-kernel-svm-and-rbf-network/</id>
    <published>2018-10-13T16:18:55.000Z</published>
    <updated>2018-10-15T07:08:15.501Z</updated>
    
    <content type="html"><![CDATA[<p>我們前面介紹了線性模型跟基本的神經網路模型。</p><p>可能有的人會覺得我怎麼不放神經網路的圖，看數學式子看的很痛苦。</p><p>是的，我的確沒打算放圖。一來神經網路的圖在各大網站或是 google 上遍地都是我實在沒有必要再放一張，二來因為這個模型的核心根本不是哪些圖，那些圖只是幫助理解，理解之後就都是看數學式了，再回去看圖就太小兒科了。</p><p>神經網路的概念在於將多個模型串接起來，也就是前面提到的堆疊的概念。</p><a id="more"></a><h2 id="處理流程"><a href="#處理流程" class="headerlink" title="處理流程"></a>處理流程</h2><p>堆疊的概念其實跟傳統的機器學習處理流程有點像。</p><p>機器學習的處理流程大概就跟做料理很像。</p><p>首先，你需要先買菜（蒐集資料），然後是備料（資料前處理）。備料的動作其實很不一樣，依據你要煮的料理（機器學習模型）是什麼而有所區別，該是切塊、切條、切絲，還是切丁（特徵離散化）？肉該不該先醃過（特徵轉換）？</p><p>如果你想呈現的是一道味道一體呈現的料理，而不是肉是肉、菜是菜，味道都各自獨立，你是不是該在一些烹調方式或是前處理上讓味道融為一體？（特徵正規化）（謎：又不是在吃沙拉，還味道各自分離。）</p><p>等料都備好了之後，就是重點的料理部份。料理方式要用煎煮炒炸哪一種（定義監督式、非監督式學習），然後整體的食譜（機器學習模型）是什麼？像是典型的紅酒燉牛肉就是經典食譜（很多人用的模型），當然你可以根據自己的喜好修改成自己的版本（改模型架構），不過大多數人怕失敗，所以都去找了電視上的或是名主廚的食譜（市面上常看到的套件，像 scikit-learn）。</p><p>料理好了之後就是要排盤（成果展現）啦！你總不可能紅酒燉牛肉做好之後整鍋端到餐桌上去，一定是要做些裝飾跟點綴（資料視覺化）。那些就是主廚（資料科學家或 AI 工程師）想要呈現給你的客人（通常是老闆）的東西，那除了嗅覺跟味覺，還有視覺上的效果。整體說來，需要營造的是一個氛圍或是體驗（老闆的感覺）。</p><p>身為一個主廚必須在各個小地方或是細節用心，才能拿到米其林指南推荐的殊榮（KDD 或 kaggle 競賽冠軍）。</p><p>好像有點離題了……</p><p>總之，這些步驟都是環環相扣的，而且需要從最前端串接到最後的。模型的堆疊也是做類似的事情，希望可以把前處理、料理等等步驟都串接起來成為一個模型，所以以往的機器學習 pipeline 就演化成神經網路模型了。</p><h2 id="RBF-network"><a href="#RBF-network" class="headerlink" title="RBF network"></a>RBF network</h2><p>我們回到今天的主題來，像前面我們談過 SVM 是個很厲害的分類器，主要是引進了 kernel 讓這個模型可以做非線性的處理。</p><p>那麼 kernel 能不能被放到神經網路裡呢？</p><p>其實是可以的，應該說，有一種網路模型稱為 Radial basis function network（RBF network），他其實就很像是 Gaussian kernel SVM。</p><p>我們來看看前面的 kernel SVM 模型：</p><p>$$<br>SVM(\mathbf{x}) = sign(\sum_{SV} \alpha_n y_n K(\mathbf{x}, \mathbf{x_n}) + b)<br>$$</p><p>Gaussian kernel，或是稱 RBF function：</p><p>$$<br>K(\mathbf{x}, \mathbf{x_n}) = exp(\gamma ||\mathbf{x} - \mathbf{x_n}||^2) = RBF(\mathbf{x}, \mathbf{x_n})<br>$$</p><p>那麼 RBF network 是長什麼樣子呢？</p><p>$$<br>y = \sum_{n} w_n RBF(\mathbf{x}, \boldsymbol{\mu_n})<br>$$</p><p>示意圖的話是這樣：</p><p><img src="/images/rbf_network.svg" alt=""></p><p>你可以將 $RBF(\mathbf{x}, \boldsymbol{\mu_n})$ 看成第一層，就是在計算兩個向量之間的距離，或是反過來說叫作相似度。</p><p>第二層則是做一個線性組合，這邊就很像神經網路的一層。</p><p>我們其實可以將上面的式子擴展成：</p><p>$$<br>y = \sigma(\sum_{n} w_n RBF(\mathbf{x}, \boldsymbol{\mu_n}) + b)<br>$$</p><p>或是寫成：</p><p>$$<br>y = \sigma(\mathbf{w}^T RBF(\mathbf{x}, \boldsymbol{\mu}) + b)<br>$$</p><p>我們可以將第二層變成一個神經網路的層，還包含有 activation function，所以這樣 kernel 也變成一層了。</p><p>RBF network 一開始是設計用來做函數內插的，就是有一些資料點 $\mathbf{x}$，我希望可以由這些資料來幫我們找到一條平滑的函數，愈密集的地方是愈有可能是線經過的。</p><p>今天的部份就到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們前面介紹了線性模型跟基本的神經網路模型。&lt;/p&gt;
&lt;p&gt;可能有的人會覺得我怎麼不放神經網路的圖，看數學式子看的很痛苦。&lt;/p&gt;
&lt;p&gt;是的，我的確沒打算放圖。一來神經網路的圖在各大網站或是 google 上遍地都是我實在沒有必要再放一張，二來因為這個模型的核心根本不是哪些圖，那些圖只是幫助理解，理解之後就都是看數學式了，再回去看圖就太小兒科了。&lt;/p&gt;
&lt;p&gt;神經網路的概念在於將多個模型串接起來，也就是前面提到的堆疊的概念。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>12 從線性模型到神經網路</title>
    <link href="https://yuehhua.github.io/2018/10/12/12-from-linear-model-to-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/12/12-from-linear-model-to-neural-network/</id>
    <published>2018-10-12T15:46:06.000Z</published>
    <updated>2018-10-15T07:04:53.018Z</updated>
    
    <content type="html"><![CDATA[<p>我們把線性模型們都大統一了。</p><p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y] \leftrightarrow \eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>接下來就要進入到令人興奮的神經網路模型了！</p><a id="more"></a><p>首先，我們先來介紹著名的感知器…嗯…前面不是介紹過了？</p><p>喔喔！對喔！他長這個樣子：</p><p>$$<br>y = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p><p>其中 $\mathbf{w}^T\mathbf{x} + b$ 是我們熟悉的線性模型，然後 $\sigma$ 就是所謂的 activation function。</p><p>不覺得這看起來跟上面的很相似嗎？</p><p>我們動點手腳：</p><p>$$<br>\sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>是的！發現了嗎？其實 $\sigma^{-1}$ 就是在廣義線性模型裡的鏈結函數阿！他會是 activation function 的反函數！</p><p>這樣是不是又了結了一樁心事了呢？</p><h2 id="堆疊"><a href="#堆疊" class="headerlink" title="堆疊"></a>堆疊</h2><p>在神經網路當中，我們會把一個一個神經元並排起來，數學上看起來就是把預測單一個 y 擴張成多個維度：</p><p>$$<br>\mathbf{y} = \sigma(W^T\mathbf{x} + \mathbf{b})<br>$$</p><p>所以在權重 W 的部份也隨之從一個向量擴張成一個矩陣，b 的部份也是，可以自己驗算看看。</p><p>但是預測多維向量並不是讓模型強大的地方，讓模型強大是因為把很多個這樣的模型頭尾接起來。</p><p>$$<br>\mathbf{x} \overset{f^{(0)}}{\longrightarrow} \mathbf{h}^{(1)} \overset{f^{(1)}}{\longrightarrow} \dots \overset{f^{(k-1)}}{\longrightarrow} \mathbf{h}^{(k)} \overset{f^{(k)}}{\longrightarrow} \mathbf{y}<br>$$</p><p>當中的這些函數們 $f^{(k))}$ 就是我們說的層。</p><p>$$<br>\mathbf{h}^{(k)} = f^{(k))}(\mathbf{h}^{(k-1)}) = \sigma(W^T\mathbf{h}^{(k-1)} + \mathbf{b})<br>$$</p><p>神經網路模型之所以強大的原因是因為將模型頭尾相接，並不是因為他是模擬生物系統，只是靈感是從生物系統上得來的而已。</p><p>搭配上 activation function 的非線性轉換，就可以模擬很多非線性的現象。</p><table><thead><tr><th style="text-align:center">model</th><th style="text-align:center">link function</th><th style="text-align:center">activation function</th></tr></thead><tbody><tr><td style="text-align:center">linear regression</td><td style="text-align:center">identity function: $y = x$</td><td style="text-align:center">identity function: $y = x$</td></tr><tr><td style="text-align:center">logistic regression</td><td style="text-align:center">logit function: $y = \frac{x}{1-x}$</td><td style="text-align:center">sigmoid function: $y = \frac{1}{1 + e^{-x}}$</td></tr><tr><td style="text-align:center">Poisson regression</td><td style="text-align:center">log function: $y = ln(x)$</td><td style="text-align:center">exponential function: $y = exp(x)$</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們把線性模型們都大統一了。&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;y \overset{f}{\longleftrightarrow} \mathbb{E}[y] \leftrightarrow \eta = \mathbf{w}^T\mathbf{x} + b&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;接下來就要進入到令人興奮的神經網路模型了！&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>11 廣義線性模型</title>
    <link href="https://yuehhua.github.io/2018/10/11/11-generalized-linear-model/"/>
    <id>https://yuehhua.github.io/2018/10/11/11-generalized-linear-model/</id>
    <published>2018-10-11T05:16:28.000Z</published>
    <updated>2018-10-15T07:04:40.296Z</updated>
    
    <content type="html"><![CDATA[<p>我們前面探討了不同的資料型態可以對應不同的迴歸模型。</p><p>不覺得每個迴歸模型都有那麼點相似的地方嗎？</p><a id="more"></a><p>線性迴歸：</p><p>$$<br>\mathbb{E}[y] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>羅吉斯迴歸：</p><p>$$<br>ln(\frac{\mathbb{E}[y]}{1 - \mathbb{E}[y]}) = ln(\frac{p}{1 - p}) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>Poisson 迴歸：</p><p>$$<br>ln(\mathbb{E}[y]) = ln(\lambda) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>在右手邊的部份都是一樣的，是一樣的線性組合加上一個常數。</p><p>差別在於預測出來的數值是怎麼連結到目標變量的平均值上 $\mathbb{E}[y]$。</p><p>是的，我們在預測的都是目標變量的平均值。</p><h2 id="鏈結函數（link-function）"><a href="#鏈結函數（link-function）" class="headerlink" title="鏈結函數（link function）"></a>鏈結函數（link function）</h2><p>要連結目標變量的平均值 $\mathbb{E}[y]$ 跟線性組合加上一個常數…..，姑且叫他 $\eta$ 好了。</p><p>$$<br>\mathbb{E}[y] \leftrightarrow \eta<br>$$</p><p>統計學家發展出使用鏈結函數來連結這兩者，所以不同的資料型態會對應不同的鏈結函數。</p><p>線性迴歸使用 identity function $y = x$：</p><p>$$<br>\mathbb{E}[y] = \eta<br>$$</p><p>羅吉斯迴歸使用 logit function $y = ln(\frac{x}{1 - x})$：</p><p>$$<br>ln(\frac{\mathbb{E}[y]}{1 - \mathbb{E}[y]}) = \eta<br>$$</p><p>Poisson 迴歸使用 log function $y = ln(x)$：</p><p>$$<br>ln(\mathbb{E}[y]) = \eta<br>$$</p><h2 id="廣義線性模型（generalized-linear-model）"><a href="#廣義線性模型（generalized-linear-model）" class="headerlink" title="廣義線性模型（generalized linear model）"></a>廣義線性模型（generalized linear model）</h2><p>這麼一來我們就可以把三個模型搓一搓做成 <del>撒尿牛丸</del> 廣義線性模型啦！</p><p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y] \leftrightarrow \eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>對應不同的目標變量，我們就有了萬用的模型，就像物理的大一統理論一樣。</p><p>廣義線性模型其實包含了三個部份：</p><ol><li>鏈結函數</li><li>線性預測子</li><li>指數族</li></ol><h2 id="線性預測子（linear-predictor）"><a href="#線性預測子（linear-predictor）" class="headerlink" title="線性預測子（linear predictor）"></a>線性預測子（linear predictor）</h2><p>統計學家特別給了一個線性預測子這樣的名字。</p><p>$$<br>\eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>這代表要從預測變量 $\mathbf{x}$ 去預測我們的目標變量，其中 $\mathbf{x}$ 的變數之間都是 <strong>互相獨立</strong> 的。</p><p>互相獨立的變數之間，要以 <strong>線性組合</strong> 來預測我們的目標變量。</p><h2 id="指數族（exponential-family）"><a href="#指數族（exponential-family）" class="headerlink" title="指數族（exponential family）"></a>指數族（exponential family）</h2><p>可是每一種資料的機率分佈都可以接上廣義線性模型嗎？答案是否定的。</p><p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y]<br>$$</p><p>統計學家研究了一下這個模型，發現只有符合指數族的條件才能夠用。</p><p>指數族長成這樣：</p><p>$$<br>f(\mathbf{y} \mid \boldsymbol{\theta}) = \frac{1}{Z(\boldsymbol{\theta})} h(\mathbf{y}) exp(\boldsymbol{\theta}^T \phi(\mathbf{y}))<br>$$</p><p>$\boldsymbol{\theta}$ 是機率分佈的期望值，或是稱為 natural parameter。</p><p>$\phi(\mathbf{y})$ 是 sufficient statisitcs，這邊有非常多有趣的東西，不過也有點理論。</p><p>$Z(\boldsymbol{\theta})$ 稱為 partition function，是機率分佈的分母，常常會在不同的領域見到他，像是物理。</p><p>$h(\mathbf{y})$ 就是個縮放因子，沒什麼重要性，常常是 1。</p><p>我知道大家可能會有很多疑問，但是礙於篇幅，我就不再繼續介紹下去了，這邊下去又是統計所一門課了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們前面探討了不同的資料型態可以對應不同的迴歸模型。&lt;/p&gt;
&lt;p&gt;不覺得每個迴歸模型都有那麼點相似的地方嗎？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>10 從線性迴歸到 Poisson 迴歸</title>
    <link href="https://yuehhua.github.io/2018/10/10/10-from-linear-regression-to-poisson-regression/"/>
    <id>https://yuehhua.github.io/2018/10/10/10-from-linear-regression-to-poisson-regression/</id>
    <published>2018-10-10T14:56:34.000Z</published>
    <updated>2018-10-15T07:04:28.010Z</updated>
    
    <content type="html"><![CDATA[<p>上次我們講完了線性迴歸跟羅吉斯迴歸的差異。</p><p>可是並不是每一種資料都是連續型的或是類別型的。</p><p>這次要來介紹 Poisson 迴歸，當你要預測的是計數型資料（count data）就可以用他。</p><a id="more"></a><h2 id="Poisson-distribution"><a href="#Poisson-distribution" class="headerlink" title="Poisson distribution"></a>Poisson distribution</h2><p>在講 Poisson 迴歸之前要先來講講 Poisson 分佈，他的公式大概是長的像這樣：</p><p>$$<br>P(Y=y; \lambda) = \frac{e^{- \lambda} \lambda^y}{y!}<br>$$</p><p>圖形的話看起來是這樣。</p><p><img src="/images/Poisson_distribution.svg" alt=""></p><blockquote><p>圖片來自維基百科</p></blockquote><p>要怎麼看懂這個分佈呢？</p><p>我們先想像一個情境好了，假設我們經營一家便利商店，在一天之中來光臨這家商店的人數不同時段不一樣。即使是同一個時段，你也很難準確預測會有多少人進到店裡來。這時候我們就會用機率的描述方式，在這邊 k 指的是當我們觀察每段時間區間內進入店裡的客人數量，那麼 $\lambda$ 就是平均來說，每個時間區間的來客人數。你可以看到在 $\lambda = 1$ 的分佈上，來客人數是 0 或是 1 的機率其實很高，但是大於 1 的情形並不是沒有，只是機率比較低罷了。</p><p>因此，我們可以用這樣的分佈來估算計數型的資料</p><h2 id="Poisson-迴歸"><a href="#Poisson-迴歸" class="headerlink" title="Poisson 迴歸"></a>Poisson 迴歸</h2><p>計數型的資料難道不能用一般的線性迴歸嗎？</p><p>其實這兩者有非常大的差別：</p><ol><li>計數型的資料不會有負值</li><li>計數型的資料不會有小數點</li></ol><p>基於以上兩點資料性質上的差異，我們必須把不同資料分別看待。</p><p>但是也不是完全不能用線性迴歸，只是需要動點手腳，就是對資料取 log。</p><p>如果你對上面的 Poisson 分佈取 log 的話會發生什麼事呢？</p><p>$$<br>ln(P(Y=y; \lambda)) = - \lambda + y ln(\lambda) - \sum_{y}^{j=1} j<br>$$</p><p>看到了吧！$\lambda$ 跳出來了！而平均數 $\lambda$ 是連續型的數值，可以作為線性迴歸要預測的對象的。</p><blockquote><p>注意：以上並非正式的證明，請勿用於正式推導</p></blockquote><p>其實我們的 Poisson 迴歸是長成這樣的：</p><p>$$<br>ln(\mathbb{E}[y]) = ln(\mathbb{E}[P(Y=y; \lambda)]) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>是不是跟我們前面提到的羅吉斯迴歸有 87% 像呢？</p><p>而且我們在上面有提到 $\lambda$ 是平均數，所以呢…</p><p>$$<br>ln(\mathbb{E}[y]) = ln(\lambda) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>Poisson 迴歸在預測的根本是 $ln(\lambda)$ 嘛！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上次我們講完了線性迴歸跟羅吉斯迴歸的差異。&lt;/p&gt;
&lt;p&gt;可是並不是每一種資料都是連續型的或是類別型的。&lt;/p&gt;
&lt;p&gt;這次要來介紹 Poisson 迴歸，當你要預測的是計數型資料（count data）就可以用他。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>09 從線性迴歸到羅吉斯迴歸</title>
    <link href="https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/"/>
    <id>https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/</id>
    <published>2018-10-09T14:59:32.000Z</published>
    <updated>2018-10-15T07:04:15.957Z</updated>
    
    <content type="html"><![CDATA[<p>我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。</p><p>現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。</p><a id="more"></a><h2 id="常態分佈"><a href="#常態分佈" class="headerlink" title="常態分佈"></a>常態分佈</h2><p>其實我們在前面的文章中有提到我們的線性迴歸模型假設了誤差是會呈現一個常態分佈。</p><p>對於誤差的假設這件事情其實很大程度影響了我們模型的長相，也會影響這個模型所適用的資料。</p><p>像是線性迴歸模型適用的是連續型變數的資料，更進一步來說，他是要求他的應變量（y）要是連續型的，看起來是這樣的。</p><p>$$<br>\mathbb{E}[y] = \mathbb{E}[Normal(Y=y; \mu, \sigma)] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>$Normal(Y=y; \mu, \sigma)$：代表常態分佈，以 $\mu$ 為平均數，$\sigma$ 為標準差，輸入是應變量（y），輸出是機率。</p><p>事實上，當你把你的一些 feature 輸入這個模型後，預測出來的是應變量的期望值或是平均數（$\mathbb{E}[y]$）。</p><p>如果我們要預測的應變量是二元的類別資料，那也就是把問題轉換成分類問題，我們就需要用不同的分佈。</p><h2 id="白努力分佈與二項式分佈"><a href="#白努力分佈與二項式分佈" class="headerlink" title="白努力分佈與二項式分佈"></a>白努力分佈與二項式分佈</h2><p>當我們的資料是分成兩類：0, 1，那麼他會對應到白努力分佈（Bernoulli distribution）。</p><p>當你蒐集了一群二元分類的資料，那麼就會對應到二項式分佈（Binomial distribution）。</p><p>如果你要預測的是一筆二元分類的資料，你應該要假設你的應變量（y）是白努力分佈。</p><p>$$<br>\mathbb{E}[y] = \mathbb{E}[Ber(Y=y; p)] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>$Ber(Y=y; p)$：代表白努力分佈，以 p 為成功機率，輸入是應變量（y），輸出是機率。</p><h2 id="羅吉斯迴歸"><a href="#羅吉斯迴歸" class="headerlink" title="羅吉斯迴歸"></a>羅吉斯迴歸</h2><p>羅吉斯迴歸的模型是以下這個樣子：</p><p>$$<br>y = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p><p>其中的 $\sigma$ 是 sigmoid function，由這個函數輸出的就是成功機率了：</p><p>$$<br>p = \sigma(s) = \frac{1}{1 + e^{-s}}<br>$$</p><p>那這跟我們前面提的分佈看起來都不太一樣阿！我們來反推一下，其實我們前面提到的分佈應該是跟 sigmoid function 的反函數有關係：</p><p>$$<br>\sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>我們從白努力分佈開始，當 $y \in {0, 1}$ 時：</p><p>$$<br>Ber(Y=y; p) = p^y (1-p)^{(1-y)}<br>$$</p><p>也就是，當 y 是 0 的時候，只會剩下後面那一項，當 y 是 1 的時候，只會剩下前面那一項。</p><p>那們我們要預測的其實是一個事件發生的機率 p，可是我們由 $\mathbf{w}^T\mathbf{x} + b$ 計算出來的是連續的數值。那要怎麼將這兩者接起來呢？</p><p>在統計學裡有一個叫作 logit function 的東西，中文翻譯叫作邏輯函數，但是跟邏輯沒什麼關係就是了。</p><p>$$<br>y = log \frac{x}{1 - x}<br>$$</p><p>如果將裡頭的 x 想成成功機率的話，分子就是成功的機率，分母就是失敗的機率，就非常像一個東西叫作勝算（odds），所以他又叫作 log odds。</p><p>用勝算的話就是一個連續的數值，同時也可以表達成功機率的含意，我們就從這邊開始吧！一路朝著 sigmoid function 邁進。</p><p>$$<br>s = ln \frac{p}{1 - p}<br>$$</p><p>$$<br>-s = ln \frac{1 - p}{p}<br>$$</p><p>$$<br>e^{-s} = \frac{1 - p}{p}<br>$$</p><p>$$<br>p + e^{-s}p = 1<br>$$</p><p>$$<br>(1 + e^{-s})p = 1<br>$$</p><p>$$<br>p = \frac{1}{1 + e^{-s}}<br>$$</p><p>看到了嗎？他是 sigmoid function！</p><p>也就是說，如果用線性迴歸預測的是 log odds，那就會跟羅吉斯迴歸一樣！</p><p>$$<br>ln \frac{p}{1 - p} = \sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p><p>你如果把 sigmoid function 還原回來的話：</p><p>$$<br>y = \frac{1}{1 + e^{- \mathbf{w}^T\mathbf{x} - b}} = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p><p>就是羅吉斯迴歸的樣子了！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。&lt;/p&gt;
&lt;p&gt;現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>08 l2-regularized 線性模型</title>
    <link href="https://yuehhua.github.io/2018/10/08/08-l2-regularized-linear-model/"/>
    <id>https://yuehhua.github.io/2018/10/08/08-l2-regularized-linear-model/</id>
    <published>2018-10-08T15:06:37.000Z</published>
    <updated>2018-10-16T17:13:42.913Z</updated>
    
    <content type="html"><![CDATA[<p>我們來回顧一下 SVM 模型。</p><a id="more"></a><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b, \boldsymbol{\xi}} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N \xi_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1 - \xi_n \\<br>\text{ } &amp;\ \ \ \<br>    \forall n, \xi_n \ge 0<br>\end{align}<br>$$</p><p>他可以被進一步轉成</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N max(1 - y_i (\mathbf{w}^T\mathbf{x_i} + b), 0) \\<br>\end{align}<br>$$</p><p>在 SVM 的陳述當中，有沒有發現 $\frac{1}{2} \mathbf{w}^T\mathbf{w}$ 這部份看起來跟 $l_2$ regularization 一樣。後半部份是跟誤差有關。</p><p>如果分類分對的話，$y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 就會是大於等於 1，這樣的話後面整項計算起來就會是 0。</p><p>如果分類分對但是離線太近的話，$y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 就會是小於 1，這樣後面整項會是一個小於 1 的值，代表離線太近了，算是些微的誤差。</p><p>如果分類都分錯，$y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 根本就是負值，後面整項會是一個大於 1 的值，代表離線太遠了，算是很大的誤差。</p><h2 id="Hinge-error"><a href="#Hinge-error" class="headerlink" title="Hinge error"></a>Hinge error</h2><p>這樣的誤差計算方式稱為 hinge error，也就是：</p><p>$$<br>E(y, \hat{y}) = max(1 - y\hat{y}, 0)<br>$$</p><h2 id="l-2-regularized-linear-model"><a href="#l-2-regularized-linear-model" class="headerlink" title="$l_2$ -regularized linear model"></a>$l_2$ -regularized linear model</h2><p>所以我們根本可以把模型看成是一個 $l_2$ -regularized 的線性模型。</p><p>$$<br>\mathop{\arg\min}_{\mathbf{w}, b} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N E(y, \hat{y})<br>$$</p><p>不懂 regulariztion 的朋友請左轉 <a href="https://ithelp.ithome.com.tw/articles/10186405" target="_blank" rel="noopener">我之前寫過的鐵人文章</a>。</p><p>基本上，你想把誤差函數換成其他的東西都可以，像是變成 kernel $l_2$ -regularized linear regression (kernel ridge regression)。</p><p>$$<br>y = \mathbf{w}^T\phi(\mathbf{x}) + b \\<br>E(y, \hat{y}) = \frac{1}{N} (y - \hat{y})^2 \\<br>\mathop{\arg\min}_{\mathbf{w}, b} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N E(y, \hat{y})<br>$$</p><p>或是你想要 kernel $l_2$ -regularized logistic regression。</p><p>$$<br>y = \mathbf{w}^T\phi(\mathbf{x}) + b \\<br>E(y, \hat{y}) = \frac{1}{N} log(1 + exp(-y\hat{y})) \\<br>\mathop{\arg\min}_{\mathbf{w}, b} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N E(y, \hat{y})<br>$$</p><h2 id="整理"><a href="#整理" class="headerlink" title="整理"></a>整理</h2><table><thead><tr><th></th><th>hinge error</th><th>least square error</th></tr></thead><tbody><tr><td>regression</td><td>SVR ($\epsilon$-insensitive error)</td><td>kernel ridge regression</td></tr><tr><td></td><td>$$ \mathop{\arg\min} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C\sum _{i=1}^N (\zeta_i + \zeta_i’) $$</td><td>$$ \mathop{\arg\min} \frac{1}{N} \sum _{i=1}^N (\mathbf{w}^T\phi(\mathbf{x}_i) + b - y_i)^2 + \frac{\lambda}{2} \mathbf{w}^T\mathbf{w} $$</td></tr><tr><td>classification</td><td>soft-margin SVM</td><td>LSSVM</td></tr><tr><td></td><td>$$ \mathop{\arg\min} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C\sum _{i=1}^N \zeta_i $$</td><td>$$ \mathop{\arg\min} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C\sum _{i=1}^N (1 - y_i(\mathbf{w}^T\phi(\mathbf{x}_i) + b))^2 $$</td></tr></tbody></table><p>SVR:</p><p>$$<br>\zeta_i = max(y_i - (\mathbf{w}^T\phi(\mathbf{x}_i) + b) - \epsilon, 0) \\<br>\zeta_i’ = max((\mathbf{w}^T\phi(\mathbf{x}_i) + b) - y_i - \epsilon, 0)<br>$$</p><p>soft-margin SVM：</p><p>$$<br>\zeta_i = max(1 - y_i(\mathbf{w}^T\phi(\mathbf{x}_i) + b), 0)<br>$$</p><h2 id="Representer-theorem"><a href="#Representer-theorem" class="headerlink" title="Representer theorem"></a>Representer theorem</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們來回顧一下 SVM 模型。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>07 標準 SVM</title>
    <link href="https://yuehhua.github.io/2018/10/07/07-standard-svm/"/>
    <id>https://yuehhua.github.io/2018/10/07/07-standard-svm/</id>
    <published>2018-10-07T09:20:54.000Z</published>
    <updated>2018-10-15T07:05:16.837Z</updated>
    
    <content type="html"><![CDATA[<p>雖然標題是說”標準” SVM，不過模型這種東西從來就沒有什麼標準，有的不過是變體。</p><p>所以這篇是要跟大家總結一下我們一般在用的 SVM 模型的預設值是長什麼樣子。</p><a id="more"></a><p>我們前面從 maximum-margin classifier 出發，尋找最大 margin 的分類器。</p><p>接著，為了解決非線性跟計算效能問題，引進了 kernel trick。</p><p>最後，為了避免 overfitting，我們放寬了 margin 成為 soft-margin。</p><p>這些都是標準 SVM 的預設配備。</p><p>當然還有為了解決計算效能問題，使用了 Lagrange multiplier，由於背後的數學太過煩雜就不介紹了。</p><p>簡單來說，Lagrange multiplier 的使用，讓原本的問題（primal problem）可以有另外的對偶問題（dual problem）。</p><p>我們只要解了對偶問題，我們就可以解掉原本的問題，雖然有時候會碰到一些限制。</p><p>原本問題的陳述是這樣的：</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b, \boldsymbol{\xi}} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N \xi_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1 - \xi_n \\<br>\text{ } &amp;\ \ \ \<br>    \forall n, \xi_n \ge 0<br>\end{align}<br>$$</p><p>轉變成對偶問題然後簡化後的陳述是這樣的：</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\boldsymbol{\alpha}} &amp;\ \ \ \<br>    \frac{1}{2} \sum _{n=1}^{N} \sum _{m=1}^{N} \alpha_n \alpha_m y_n y_m \mathbf{z_n}^T\mathbf{z_m} - \sum _{n=1}^N \alpha_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \sum_{n=1}^{N} \alpha_n y_n = 0, 0 \le \alpha_n \le C \\<br>\text{ } &amp;\ \ \ \<br>    \mathbf{w} = \sum_{n=1}^{N} \alpha_n y_n \mathbf{z_n}<br>\end{align}<br>$$</p><p>不要問我怎麼來的，你一問我就要開始另一篇數學了，你會怕。</p><p>解完上面的問題，我們可以得到一些資訊，我們可以從 $\alpha_n$ 的數值範圍來得知一個資料點他是不是 support vector。</p><p>只要他的值在 $0 &lt; \alpha_n &lt; C$ 這個範圍，他就是 support vector。</p><p>我們最後在形成整個 SVM 模型的時候其實只需要這些 support vector 就可以了。</p><p>$$<br>SVM(\mathbf{x}) = sign(\sum_{SV} \alpha_n y_n K(\mathbf{x_n}, \mathbf{x}) + b)<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;雖然標題是說”標準” SVM，不過模型這種東西從來就沒有什麼標準，有的不過是變體。&lt;/p&gt;
&lt;p&gt;所以這篇是要跟大家總結一下我們一般在用的 SVM 模型的預設值是長什麼樣子。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>06 從 hard-margin SVM 到 soft-margin SVM</title>
    <link href="https://yuehhua.github.io/2018/10/06/06-from-hard-margin-svm-to-soft-margin-svm/"/>
    <id>https://yuehhua.github.io/2018/10/06/06-from-hard-margin-svm-to-soft-margin-svm/</id>
    <published>2018-10-06T13:10:06.000Z</published>
    <updated>2018-10-15T07:05:25.883Z</updated>
    
    <content type="html"><![CDATA[<p>從前面的 kernel SVM 當中我們已經獲得了很強大的模型，可是他還是會有不足之處，像是當資料有雜訊的時候就容易將每個資料點都個別分開。</p><p>有時候我們反而希望模型在面對雜訊上不要那麼敏感，或是不要把每個資料點都分對，這時候怎麼辦呢？</p><a id="more"></a><h2 id="Hard-margin-SVM"><a href="#Hard-margin-SVM" class="headerlink" title="Hard-margin SVM"></a>Hard-margin SVM</h2><p><img src="/images/linear-separable2.svg" alt=""></p><p>從這張圖來看，我們或許可以接受這樣的線其實還不錯，只是資料多了一點雜訊。如果讓模型硬要把所有資料點都分對的話，邊界就會非常複雜，就會變成 overfitting。</p><p>我們是不是有什麼辦法去修正這個模型呢？</p><p>我們原本的模型是：</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>\end{align}<br>$$</p><p>我們可以藉由在最佳化目標上加上一些 regularization $\sum _{n=1}^N [y_n \ne sign(\mathbf{w}^T\mathbf{z_n} + b) ]$。</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N [y_n \ne sign(\mathbf{w}^T\mathbf{z_n} + b) ] \\<br>\text{subject to} &amp;\ \ \ \<br>    …<br>\end{align}<br>$$</p><p>這樣的 regularization 項我們可以仿照之前的方法，把他改成 $\sum _{n=1}^N y_n (\mathbf{w}^T\mathbf{z_n} + b)$。</p><p>如果考慮 $y_n (\mathbf{w}^T\mathbf{z_n} + b)$，如果答對的話，他會是大於等於 1 的值，相反，如果答錯的話………..，我們只能確定他是負值，沒辦法確定他的範圍。</p><p><img src="/images/soft-margin_SVM.svg" alt=""></p><p>這樣的話，我們 直接引入一個值 $\xi$ 來代表 $y_n (\mathbf{w}^T\mathbf{z_n} + b)$ 到底有多大程度答錯了。</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b, \mathbf{\xi}} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N \xi_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1 - \xi_n \\<br>\text{ } &amp;\ \ \ \<br>    \forall n, \xi_n \ge 0<br>\end{align}<br>$$</p><p>所以在最佳化目標上有 regularization，然後放寬一下限制，讓 $y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 的值可以有個容忍度 $1 - \xi_n$。</p><p>也就是，我們可以在 margin 儘量大的情形下，去限制讓模型不能做錯太多，這個程度可以藉由 C 做調整。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;從前面的 kernel SVM 當中我們已經獲得了很強大的模型，可是他還是會有不足之處，像是當資料有雜訊的時候就容易將每個資料點都個別分開。&lt;/p&gt;
&lt;p&gt;有時候我們反而希望模型在面對雜訊上不要那麼敏感，或是不要把每個資料點都分對，這時候怎麼辦呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>05 從 maximum-margin classifier 到 kernel SVM</title>
    <link href="https://yuehhua.github.io/2018/10/05/05-from-max-margin-classifier-to-kernel-svm/"/>
    <id>https://yuehhua.github.io/2018/10/05/05-from-max-margin-classifier-to-kernel-svm/</id>
    <published>2018-10-05T15:07:15.000Z</published>
    <updated>2018-10-15T07:05:37.356Z</updated>
    
    <content type="html"><![CDATA[<p>注意：整篇文章極度數學高能！！</p><p>沒有把前一篇文章看完的朋友別擔心，我們會在開頭先回顧一下。在一番數學技巧的替換過後，我們的 maximum-margin classifier 會被化成一個最佳化問題。這個最佳化問題可以用二次規劃（quadratic programming, QP）來解。</p><a id="more"></a><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>\end{align}<br>$$</p><p>當模型被轉化成一個最佳化問題之後，<del>你還有什麼不滿的？</del> 問題都解決了嗎？</p><p>當然還沒有阿！雖然說我們解決了前面的第二個缺點，但是第一個缺點還是在啊！</p><p>那如果資料不是線性可分的話，是不是支援非線性的分類問題就可以了？</p><h2 id="非線性轉換"><a href="#非線性轉換" class="headerlink" title="非線性轉換"></a>非線性轉換</h2><p>那問題簡單！既然這個分類器只能處理線性問題，那就把所有非線性問題透過一個轉換變成線性問題不就可以了？</p><p>當然用講的比較簡單…</p><p>我們就先假設有個非線性轉換，可以把原本的非線性資料 $\mathbf{x_i}$ 轉成線性資料 $\mathbf{z_i}$：</p><p>$$<br>\mathbf{z_i} = \phi(\mathbf{x_i})<br>$$</p><p>然後再把線性資料塞進模型裡不就得了？</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{z_i} + b) \ge 1<br>\end{align}<br>$$</p><p>嗯……可是瑞凡，你知道要放到 QP 裏面算之前，要先計算出 $[\mathbf{z_i}^T\mathbf{z_j}]$，也就是把資料經過一個非線性轉換過後，還需要將每個資料向量兩兩之間內積，得到一整個矩陣才行。</p><p>計算這所有的組合可是非常花時間（$O(N^2)$）的好嗎！！尤其你的資料點很多的時候，大數據都算不下去了。</p><h2 id="Kernel-method"><a href="#Kernel-method" class="headerlink" title="Kernel method"></a>Kernel method</h2><p>我們先來看一下比較簡單的非線性轉換好了，假設是二次多項式的轉換。</p><p>我們想像中的二次多項式應該是 $\phi_2(x) = (1, x, x^2)$，這邊有三項，可是廣義的二次多項式會有：</p><p>$$<br>\mathbf{z} = \phi_2(\mathbf{x}) = (1, x_1, x_2, \dots, x_d, \<br>    x_1^2, x_1x_2, \dots, x_1x_d, \<br>    x_2x_1, x_2^2, \dots, x_2x_d, \<br>    \dots, x_d^2)<br>$$</p><p>有一個常數項、d 個一次項跟 $d + \frac{1}{2} d(d+1)$ 個二次項，如果這個向量內積的話，乘法的時間複雜度就有 $O(d^2)$ 了。</p><p>然後再讓 $\mathbf{z_i}$ 跟 $\mathbf{z_j}$ 兩兩內積，利用內積的交換性扣掉一些不用算的，好歹也需要算 $N + \frac{1}{2} N(N+1)$ 次內積，是 $O(N^2)$。</p><p>總共應該會有 $O(d^2N^2)$。</p><blockquote><p>註：d 為資料維度，N 為資料筆數</p></blockquote><p>有沒有什麼方法可以降低這個時間呢？</p><p>我們把內積的部份拿出來看看。</p><p>$$<br>\phi_2(\mathbf{x})^T\phi_2(\mathbf{x’}) = 1 + \sum_{i=1}^{d} x_ix_i’ + \sum_{i=1}^{d} \sum_{j=1}^{d} x_ix_jx_i’x_j’<br>$$</p><p>咦！是不是可以進一步整理成這樣。</p><p>$$<br>\phi_2(\mathbf{x})^T\phi_2(\mathbf{x’}) = 1 + \sum_{i=1}^{d} x_ix_i’ + \sum_{i=1}^{d} x_ix_i’ \sum_{j=1}^{d} x_jx_j’ \\<br>= 1 + \mathbf{x}^T\mathbf{x’} + (\mathbf{x}^T\mathbf{x’})(\mathbf{x}^T\mathbf{x’}) \\<br>= 1 + \mathbf{x}^T\mathbf{x’} + (\mathbf{x}^T\mathbf{x’})^2<br>$$</p><p>這樣的話需要多少時間複雜度呢？掐指一算，$\mathbf{x}^T\mathbf{x’}$ 需要 $O(d)$，$\phi_2(\mathbf{x})^T\phi_2(\mathbf{x’})$ 總共需要 $O(d) + 1$！</p><p>大大降低了！！</p><p>所以我們就把這樣的方法稱為 kernel method，並且定義 $K_\phi(\mathbf{x}, \mathbf{x’}) = \phi(\mathbf{x})^T\phi(\mathbf{x’})$。</p><p>不過大家常用的是比較廣義的版本：</p><p>$$<br>K_2(\mathbf{x}, \mathbf{x’}) = 1 + 2 \mathbf{x}^T\mathbf{x’} + (\mathbf{x}^T\mathbf{x’})^2 = (1 + \mathbf{x}^T\mathbf{x’})^2<br>$$</p><p>然後使用者通常會想要細緻控制 kernel 的強度，所以會引入一個參數：</p><p>$$<br>K_2(\mathbf{x}, \mathbf{x’}) = 1 + 2 \gamma \mathbf{x}^T\mathbf{x’} + \gamma^2 (\mathbf{x}^T\mathbf{x’})^2 = (1 + \gamma \mathbf{x}^T\mathbf{x’})^2<br>$$</p><p>我們來看一下用起來的效果如何？</p><p><img src="/images/poly2_kernel.svg" alt=""></p><blockquote><p>picture from coursera, 《機器學習技法》 - 林軒田</p></blockquote><h2 id="無限維度-kernel"><a href="#無限維度-kernel" class="headerlink" title="無限維度 kernel"></a>無限維度 kernel</h2><p>既然可以有二次的那是不是到 M 次都可以？那有沒有無限次的呢？</p><p>有！</p><p>他長成這樣：</p><p>$$<br>K(x, x’) = exp(-(x - x’)^2)<br>$$</p><p>嗯？你不要騙我！你以為擺到次方項就可以變成無限維度？</p><p>那就來證明一下啦-~~</p><p>$$<br>= exp(-x^2 + 2xx’ - x’^2) \\<br>= exp(-x^2) exp(-x’^2) exp(2xx’) \\<br>= exp(-x^2) exp(-x’^2) (\sum_{i=0}^{\infty} \frac{(2xx’)^i}{i})\text{ (Taylor expansion)}<br>$$</p><p>透過泰勒展開式展開之後我們就看到無限維度的影子了，再進一步簡化。</p><p>$$<br>= \sum_{i=0}^{\infty} \big ( exp(-x^2) exp(-x’^2) \frac{2^i}{i} x^i x’^i) \big ) \\<br>= \sum_{i=0}^{\infty} \big ( exp(-x^2) exp(-x’^2) \frac{\sqrt{2^i}}{i} \frac{\sqrt{2^i}}{i} x^i x’^i) \big ) \\<br>= \sum_{i=0}^{\infty} \big ( \frac{\sqrt{2^i}}{i} x^i exp(-x^2) \big ) \sum_{i=0}^{\infty} \big ( \frac{\sqrt{2^i}}{i} x’^i exp(-x’^2)) \big ) \\<br>= \phi(x)^T\phi(x’)<br>$$</p><p>我們的無限維度非線性轉換就完成啦！</p><p>$$<br>\phi(x) = exp(-x^2) \cdot \big ( 1, \frac{\sqrt{2}}{1} x, \frac{\sqrt{2^2}}{2} x^2, \dots, \big )<br>$$</p><p>所以這種 kernel 叫作 Gaussian kernel，使用上也是有個參數可以調整強度。</p><p>$$<br>K(\mathbf{x}, \mathbf{x’}) = exp(- \gamma \left\lVert \mathbf{x} - \mathbf{x’} \right\rVert ^2), \gamma \gt 0<br>$$</p><p>在現在的支持向量機（Support vector machine, SVM）預設是會使用 Gaussian kernel 的，這就是這個模型強大的祕密！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;注意：整篇文章極度數學高能！！&lt;/p&gt;
&lt;p&gt;沒有把前一篇文章看完的朋友別擔心，我們會在開頭先回顧一下。在一番數學技巧的替換過後，我們的 maximum-margin classifier 會被化成一個最佳化問題。這個最佳化問題可以用二次規劃（quadratic programming, QP）來解。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>04 從感知器到 maximum-margin classifier</title>
    <link href="https://yuehhua.github.io/2018/10/04/04-from-perceptron-to-max-margin-classifier/"/>
    <id>https://yuehhua.github.io/2018/10/04/04-from-perceptron-to-max-margin-classifier/</id>
    <published>2018-10-04T15:43:46.000Z</published>
    <updated>2018-10-15T07:05:54.545Z</updated>
    
    <content type="html"><![CDATA[<p>上次我們完成了感知器的介紹，感知器也有他相對應的學習演算法：perceptron learning algorithm (PLA)。</p><p>不過我們今天沒有要講 PLA，我們來講講感知器的缺點。</p><p>感知器這個模型有很多的缺點，但是因為他是最簡單的模型，我們就放他一馬（？）。</p><a id="more"></a><p>他的缺點有：</p><ol><li>只能處理線性可分（linear-separable）的資料</li><li>只要能分，大家都是好的分類器（黑貓白貓，能抓老鼠的就是好貓？）</li></ol><h2 id="線性可分"><a href="#線性可分" class="headerlink" title="線性可分"></a>線性可分</h2><p>我們一項一項來講，什麼是線性可分（linear-separable）？</p><p><img src="/images/perceptron4.svg" alt=""></p><p>像這樣是線性可分的，可以用一條直線把不同的點分開。</p><p><img src="/images/linear-separable1.svg" alt=""></p><p>像這樣根本找不到一條線可以將點分開的，不是線性可分。</p><p><img src="/images/linear-separable2.svg" alt=""></p><p>差一點點也不行！很尷尬，差一點就可以變成線性可分的了！但是他就不是！</p><p>所以整體說起來，感知器在使用上限制頗大。</p><h2 id="好的分類器？"><a href="#好的分類器？" class="headerlink" title="好的分類器？"></a>好的分類器？</h2><p>接下來，什麼叫作一個好的分類器？</p><p><img src="/images/perceptron4.svg" alt=""></p><p><img src="/images/perceptron5.svg" alt=""></p><p><img src="/images/perceptron6.svg" alt=""></p><p>以上三張圖，大家有沒有認為哪一張分的最好？</p><p>每個人可能有不同的答案，但是有沒有覺得第一張是比較好的分類器？</p><p>為什麼？</p><p>資料點都多多少少會有一些誤差，而資料我們都假設他比較相似的會在一起，所以當新的資料點出現的時候，我們通常預期他會在舊的資料點附近。所以說，如果線可以離資料點遠一點比較好，這樣的話就不會不小心分錯。</p><p>其實你心裡想的是這個樣子的：</p><p><img src="/images/max-margin_classifier.svg" alt=""></p><p>我們可以定一個東西叫作 margin，他就是線到最近的資料點的距離，然後希望 margin 可以越大越好。</p><h2 id="Maximum-margin-classifier"><a href="#Maximum-margin-classifier" class="headerlink" title="Maximum-margin classifier"></a>Maximum-margin classifier</h2><p>所以我們希望的是一個讓 margin 最大化的分類器，也就是 Maximum-margin classifier。</p><p>那麼他要怎麼以數學表達呢？</p><p>注意：以下數學高能！！</p><p>$$<br>\begin{align}<br>\mathop{\arg\max}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) \\<br>\text{subject to} &amp;\ \ \ \<br>    \text{classify } (\mathbf{x_n}, y_n) \text{ correctly} \\<br>\text{ } &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \text{distance}(\mathbf{x_n}, \mathbf{w}, b)<br>\end{align}<br>$$</p><p>總的來說，你希望 margin 越大越好，那就是我們的最佳化目標，但是會帶有一些條件，像是我們需要將點都分對，以及 margin 的定義是線到各個點的距離最短的那個。</p><p>上方式子中的 argmax 指的是改變某些變數（$\mathbf{w}, b$），讓後方的式子的值最大。subject to 則是限制式，也就是必須要滿足的條件。</p><p>但是這樣的式子無法直接套用數學方法計算，所以我們要進一步將式子公式化。</p><h2 id="進一步公式化"><a href="#進一步公式化" class="headerlink" title="進一步公式化"></a>進一步公式化</h2><p>注意：以下極度數學高能！！</p><p>談到距離的話，我們要先講講高中數學裡的點到線距離公式：</p><p>$$<br>\text{distance}(\mathbf{x_i}, \mathbf{w}, b) = \frac{|ax_i + by_i + c|}{\sqrt{a^2 + b^2}}<br>$$</p><p>不記得的話，回去翻翻課本或是 google 一下。可以觀察一下，會發現分子是線的權重向量跟資料點向量的相乘再相加，再加上一個常數項。相乘再相加的動作其實就是兩個向量的 <strong>內積運算</strong>。分母部份是線的權重向量的 <strong>長度</strong>。我們可以把這個二維的公式擴充到 n 維，就變成了：</p><p>$$<br>\text{distance}(\mathbf{x_i}, \mathbf{w}, b) = \frac{|\mathbf{w}^T\mathbf{x_i} + b|}{\left\lVert \mathbf{w} \right\rVert}<br>$$</p><p>是不是看起來漂亮許多？</p><p>接著，如果要把資料點分對，那就表示說，將資料點 $\mathbf{x_i}$ 代入模型中算出來的值會跟真實答案 $y_i$ 在線的同側，也就是正負號是相同的。利用這點，我們發現如果將資料點代入模型中算出來的值 $(\mathbf{w}^T\mathbf{x_i} + b)$ 與真實答案 $y_i$ 相乘的話，必定為正，所以 $y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0$。</p><p>整個問題會被進一步變成這樣：</p><p>$$<br>\begin{align}<br>\mathop{\arg\max}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0 \\<br>\text{ } &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \frac{|\mathbf{w}^T\mathbf{x_i} + b|}{\left\lVert \mathbf{w} \right\rVert}<br>\end{align}<br>$$</p><p>$|\mathbf{w}^T\mathbf{x_i} + b|$ 的絕對值部份不是那麼好處理，我們可以把他替換成 $y_i (\mathbf{w}^T\mathbf{x_i} + b)$，反正他一定恆正。</p><p>$$<br>\begin{align}<br>\mathop{\arg\max}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0 \\<br>\text{ } &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \frac{y_i (\mathbf{w}^T\mathbf{x_i} + b)}{\left\lVert \mathbf{w} \right\rVert}<br>\end{align}<br>$$</p><h2 id="Scalling-trick"><a href="#Scalling-trick" class="headerlink" title="Scalling trick"></a>Scalling trick</h2><p>對於以下的部份，我們可以進一步簡化。</p><p>$$<br>\text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \frac{y_i (\mathbf{w}^T\mathbf{x_i} + b)}{\left\lVert \mathbf{w} \right\rVert}<br>$$</p><p>對於一條直線 $\mathbf{w}^T\mathbf{x_i} + b = 0$ 來說，縮放 c 倍基本上是沒有影響的 $c\mathbf{w}^T\mathbf{x_i} + cb = 0$。所以我們就乾脆將這個值縮放成剛好是 1，像下面這樣：</p><p>$$<br>\mathop{\min}_{i = 1 \dots n} y_i (\mathbf{w}^T\mathbf{x_i} + b) = 1<br>$$</p><p>這樣有什麼好處呢？如果你把他代回去我們的最佳化目標的話，margin 的大小是不是變成這樣了呢？</p><p>$$<br>\text{margin}(\mathbf{w}, b) = \frac{1}{\left\lVert \mathbf{w} \right\rVert}<br>$$</p><p>而且我們也省掉了 $y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0$ 這個限制，畢竟最小的資料點計算出來至少有 1。整個問題就變成了這樣：</p><p>$$<br>\begin{align}<br>\mathop{\arg\max} _{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{\left\lVert \mathbf{w} \right\rVert} \\<br>\text{subject to} &amp;\ \ \ \<br>    \mathop{\min} _{i = 1 \dots n} y_i (\mathbf{w}^T \mathbf{x_i} + b) = 1<br>\end{align}<br>$$</p><p>min 還是個不好處理的運算，我們與其求某個資料點代入最小值會是 1，我們不如放寬限制，變成所有資料點帶入公式都會 $\ge 1$。</p><p>$$<br>\mathop{\min}_{i = 1 \dots n} y_i (\mathbf{w}^T\mathbf{x_i} + b) = 1<br>\Rightarrow<br>y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>$$</p><p>最後，數學家都會有點潔癖，將問題變成他們喜歡的形式。像是求倒數的最大值，不如我們求最小值。要求 $\mathbf{w}$ 的長度太麻煩了，我們把長度中的根號拿掉。再加個二分之一上去。</p><p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>\end{align}<br>$$</p><p>maximum-margin classifier 的最終形式就完成了！這也是支持向量機（support vector machine）的雛型。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上次我們完成了感知器的介紹，感知器也有他相對應的學習演算法：perceptron learning algorithm (PLA)。&lt;/p&gt;
&lt;p&gt;不過我們今天沒有要講 PLA，我們來講講感知器的缺點。&lt;/p&gt;
&lt;p&gt;感知器這個模型有很多的缺點，但是因為他是最簡單的模型，我們就放他一馬（？）。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>03 從線性迴歸到感知器</title>
    <link href="https://yuehhua.github.io/2018/10/03/03-from-linear-regression-to-perceptron/"/>
    <id>https://yuehhua.github.io/2018/10/03/03-from-linear-regression-to-perceptron/</id>
    <published>2018-10-03T15:24:32.123Z</published>
    <updated>2018-10-03T15:24:32.123Z</updated>
    
    <content type="html"><![CDATA[<p>感知器（perceptron）是在 1957 年就被發明出來的的模型，對電腦的發展或是人工智慧來說都是非常早期的。</p><p>感知器模型他是一個二元分類的分類器，他解的是分類問題。相對我們前面的線性迴歸解的是迴歸問題，兩者在問題的定義上有根本性的不一樣，那他們兩個有什麼關聯性呢？</p><a id="more"></a><h2 id="分類"><a href="#分類" class="headerlink" title="分類"></a>分類</h2><p>現在想像一下，如果你手上有一些資料，這些資料都有兩個維度 $(x_1,x_2)$，可以他們畫在二維的平面上，這些資料有的被標記成方塊，有的被標記成圓圈。我現在希望有一種方法可以將不同標記的資料點分開，我們可以怎麼做？</p><p><img src="/images/perceptron1.svg" alt=""></p><p>在資料點上，大多相似的資料點會有接近的座標，所以資料點本身就會因為相似性而聚在一起。最直覺的方法就是畫一條直線將他們分開。</p><p><img src="/images/perceptron2.svg" alt=""></p><p>我們可以在二維平面上畫一條線來把這些點分開的話，那麼要怎麼以數學的方式呈現呢？</p><p>$$<br>x_2 = w x_1 + b<br>$$</p><p>這是我們的二維平面上的線，我們把他移項一下。</p><p>$$<br>x_2 - w x_1 - b = 0<br>$$</p><p>當你試著把資料點代入方程式的時候，你會發現不同的點算出來結果會分成兩種：</p><ol><li>$x_2 - w x_1 - b &gt; 0$</li><li>$x_2 - w x_1 - b &lt; 0$</li></ol><p>如果你的線畫的夠好的話，應該是不會有點剛好位於線上而讓 $x_2 - w x_1 - b = 0$ 發生的。所以我們是不是能夠透過將點代入公式中，計算出他的結果為正或是為負來決定他應該是方塊或是圓圈。新的資料點就可以透過這樣的運算來做預測。</p><p>如果要模型給出一個比較容易理解的預測值的話，我們可以在模型之後再加上一個 $sign$ 來取數值的正負號。</p><p>$$<br>sign(x_2 - w x_1 - b) = 0<br>$$</p><p>$$<br>sign(x) = \begin{cases}<br> -1, &amp;\text{if } x \lt 0\<br> 0, &amp;\text{if } x = 0\<br> 1, &amp;\text{if } x \gt 0<br>\end{cases}<br>$$</p><p>當整件事情拓展到了高維度空間的時候就變成這樣了：</p><p>$$<br>sign(\mathbf{w}^T\mathbf{x} + b)<br>$$</p><p>看到了嗎？中間是不是我們的線性迴歸裡的線性模型了呢？</p><p>準確來說，是跟線性迴歸沒什麼關係啦！但是同為線性模型可以應用在迴歸問題或是分類問題，有兩種些許不同的形式，是不是很妙呢？</p><h2 id="常數項"><a href="#常數項" class="headerlink" title="常數項"></a>常數項</h2><p>前面說過，常數項在分類器上有些許不一樣的的解釋。</p><p><img src="/images/perceptron3.svg" alt=""></p><p>基本上，對一條線來說，常數項仍舊是讓線可以有位移的機會，不過反過來想，如果沒有常數項的話，即便有 $\mathbf{w}^T\mathbf{x}$ 可以計算出結果，但是可能仍不足以分隔資料，所以需要加上（或是減掉）一個閾值或是臨界值。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;感知器（perceptron）是在 1957 年就被發明出來的的模型，對電腦的發展或是人工智慧來說都是非常早期的。&lt;/p&gt;
&lt;p&gt;感知器模型他是一個二元分類的分類器，他解的是分類問題。相對我們前面的線性迴歸解的是迴歸問題，兩者在問題的定義上有根本性的不一樣，那他們兩個有什麼關聯性呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
</feed>
