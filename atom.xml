<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Dream Maker</title>
  
  <subtitle>Love Math, Science, Biology, Computer science</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yuehhua.github.io/"/>
  <updated>2019-01-01T13:24:51.817Z</updated>
  <id>https://yuehhua.github.io/</id>
  
  <author>
    <name>Yueh-Hua Tu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>方均根、標準差、馬克士威-波茲曼分佈</title>
    <link href="https://yuehhua.github.io/2018/12/20/rms-and-variance/"/>
    <id>https://yuehhua.github.io/2018/12/20/rms-and-variance/</id>
    <published>2018-12-19T16:19:39.000Z</published>
    <updated>2019-01-01T13:24:51.817Z</updated>
    
    <content type="html"><![CDATA[<p>不久前跟好朋友聊天聊到 random walk 的問題，一個醉漢會在一維的空間上隨機往前或往後走一步。</p><p>這樣的話，他最後會走到哪裡去呢？</p><a id="more"></a><p>基本上如果有看過隨機過程的話，會知道當 $n \rightarrow \infty$ 的時候會收斂到原點。</p><h2 id="平均位移"><a href="#平均位移" class="headerlink" title="平均位移"></a>平均位移</h2><p>如果我們考慮走了 $N$ 步之後的位移 $S$</p><p>$$<br>S = \sum_{i=1}^{N} x_i<br>$$</p><p>那麼平均位移說起來就是</p><p>$$<br>\mathbb{E}[S] = \mathbb{E}[\sum_{i=1}^{N} x_i] = \sum_{i=1}^{N} \mathbb{E}[x_i]<br>$$</p><p>當中的 $\mathbb{E}[x_i]$，由於每次要不是往前走一步或是往後退一步，而且兩者發生的機率一樣，所以每步的位移平均是 0，所以整體平均位移也是 0。</p><p>$$<br>\mathbb{E}[S] = 0<br>$$</p><p>如果你真的用電腦跑模擬，去紀錄多次走 5000 步（或是更多）最終的位移會是多少。</p><p>如此一來，你會得到一個以 0 為平均的常態分佈曲線。</p><h2 id="有沒有更有意義的資訊？"><a href="#有沒有更有意義的資訊？" class="headerlink" title="有沒有更有意義的資訊？"></a>有沒有更有意義的資訊？</h2><p>我們除了可以看平均以外還可以看什麼？</p><p>我們或許可以看位移平方後的平均</p><p>$$<br>\mathbb{E}[S^2] = \mathbb{E}[(\sum_{i=1}^{N} x_i)^2] \\<br>= \mathbb{E}[(x_1 + x_2 + \dots + x_N)^2] \\<br>= \mathbb{E}[(x_1^2 + x_2^2 + \dots + x_N^2) + 2(x_1x_2 + x_1x_3 + \dots + x_{N-1}x_N)] \\<br>= \mathbb{E}[x_1^2] + \mathbb{E}[x_2^2] + \dots + \mathbb{E}[x_N^2] + 2(\mathbb{E}[x_1x_2] + \mathbb{E}[x_1x_3] + \dots + \mathbb{E}[x_{N-1}x_N)]) \\<br>$$</p><p>如同前面的假設，如果我們將位移給平方了，那我們會得到每一項都是 1。至於相乘項的部份，可以自己動手試試看計算比較小的組合，不過理論上會是 0。</p><p>$$<br>= 1 + 1 + \dots + 1 + 2(0 + 0 + \dots + 0) \\<br>= N<br>$$</p><p>我們得到了位移平方後的平均是 $N$！</p><h2 id="方均根"><a href="#方均根" class="headerlink" title="方均根"></a>方均根</h2><p>大家可能在高中物理中聽到方均根（root-mean-square）這個計算方式，我們也可以求得方均根位移。</p><p>只要再開個根號就可以了，$\sqrt{\mathbb{E}[S^2]} = \sqrt{N}$。</p><p>這東西是不是看起來跟統計上的標準差很像呢？</p><p>$$<br>\sigma = \sqrt{Var[X]} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x - \mu)^2}<br>$$</p><p>大概差別只會在於裏面有沒有把 $\mu$ 給減掉而已，不過也可以藉由將 $\mu$ 設定成 0 來達到同樣的效果。</p><p>意思也就是說，當醉漢走了 $N$ 步之後，會呈現一個常態分佈，而平均值是 0，標準差則是 $\sqrt{N}$。</p><p>也就是當醉漢走愈多步，終究會回歸原點，但是也會有機率距離原點一段距離，而這段距離會隨著步數的增加而變長。</p><p>在隨機過程中，這是個很經典的問題。</p><p>既然談到了方均根，就不難聯想到高中物理中講到的方均根速度。</p><h2 id="氣體動力論"><a href="#氣體動力論" class="headerlink" title="氣體動力論"></a>氣體動力論</h2><p>在氣體動力論當中，我們可以去計算一個空間中的氣體分子運動速度，以方均根的形式表示</p><p>$$<br>v_{rms} = \sqrt{\frac{3kT}{m}}<br>$$</p><p>而這個氣體速度會呈現一個分佈情形，稱為馬克士威-波茲曼速率分佈。</p><h2 id="馬克士威-波茲曼速率分佈"><a href="#馬克士威-波茲曼速率分佈" class="headerlink" title="馬克士威-波茲曼速率分佈"></a>馬克士威-波茲曼速率分佈</h2><p>我們可以從維基百科上找到以下的速度分佈。</p><blockquote><p>Maxwell–Boltzmann velocity distribution</p></blockquote><p>$$<br>f(\nu_x, \nu_y, \nu_z) = (\frac{m}{2 \pi kT})^{3/2} exp \big[ - \frac{m(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2kT} \big]<br>$$</p><p>他描述了一個氣體分子在三維空間上有 $\nu_x, \nu_y, \nu_z$ 三種不同的速度分量，可以利用這些分量來計算出整體的速度分佈情形。</p><p>不覺得上式跟常態分佈有點相似嗎？</p><p>來呼叫一下常態分佈。</p><p>$$<br>f(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} exp \big[ - \frac{(x - \mu)^2}{2 \sigma^2} \big]<br>$$</p><p>那我們來動動手，做點簡單的驗證吧！</p><p>$$<br>f(\nu_x, \nu_y, \nu_z) = (\frac{m}{2 \pi kT})^{3/2} exp \big[ - \frac{m(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2kT} \big] \\<br>= (\frac{1}{\sqrt{2 \pi} \sqrt{\frac{kT}{m}} })^3 exp \big[ - \frac{(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2 (\sqrt{\frac{kT}{m}})^2 } \big]<br>$$</p><p>我們做點簡單的整理，然後將標準差抓出來。</p><p>let $\sigma = \sqrt{\frac{kT}{m}}$</p><p>代入之後就會成為</p><p>$$<br>= (\frac{1}{\sqrt{2 \pi} \sigma })^3 exp \big[ - \frac{(\nu_x^2 + \nu_y^2 + \nu_z^2)}{2 \sigma^2 } \big]<br>$$</p><p>是不是變得更像了呢？那麼指數項中的速度平方和怎麼處理？</p><p>當然是把指數拆開囉！</p><p>$$<br>= \big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_x^2}{2 \sigma^2 } ] \big)<br>\big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_y^2}{2 \sigma^2 } ] \big)<br>\big ( \frac{1}{\sqrt{2 \pi} \sigma } exp [ - \frac{\nu_z^2}{2 \sigma^2 } ] \big)<br>$$</p><p>我們會發現馬克士威-波茲曼速率分佈其實是三個常態分佈的乘積，或是多元常態分佈（Multivariate normal distribution）！</p><p>$$<br>= f_x \cdot f_y \cdot f_z<br>$$</p><p>三個常態分佈各是對映三維空間中的三個速度分量，也就是不同速度分量之間是各自獨立，不互相影響的。</p><p>跟真正的常態分佈的差異仍舊是有沒有將平均值減掉。</p><p>$$<br>\nu_x = v_x - \mu_x<br>$$</p><p>$\nu_x$：相對速度</p><p>$v_x$：絕對速度</p><p>或許我們可以這樣解釋，在這整個空間中，整個氣體是靜止不動的，所以他的整體平均速度是 0，而氣體的微觀速度 $\nu_x$ 就是他真正的速度。如果整體氣體是有一個速度在移動的，那麼你可以透過相對速度及平均速度來推得氣體的絕對速度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不久前跟好朋友聊天聊到 random walk 的問題，一個醉漢會在一維的空間上隨機往前或往後走一步。&lt;/p&gt;
&lt;p&gt;這樣的話，他最後會走到哪裡去呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Physics" scheme="https://yuehhua.github.io/categories/Physics/"/>
    
    
  </entry>
  
  <entry>
    <title>紀念 - 風超大的高美濕地</title>
    <link href="https://yuehhua.github.io/2018/12/15/ning/"/>
    <id>https://yuehhua.github.io/2018/12/15/ning/</id>
    <published>2018-12-15T06:26:27.000Z</published>
    <updated>2018-12-15T06:29:57.428Z</updated>
    
    <content type="html"><![CDATA[<p>很久沒有寫文章了。</p><p>一個陽光耀眼愜意的下午，舒適的溫度。</p><p>下午約了好朋友一起去逛逛，從綠園道一路逛到科博館。</p><a id="more"></a><p>一個可以談天說地的朋友，常常可以聊整個晚上到半夜的朋友。</p><p>沿途談論科學的精確性，測量的誤差永遠是有的，但取決於想要看到多麼精細的結果。</p><p>雖然不同的學科背景，數學是很有感觸跟交集的領域。</p><p>果不其然的走入了科博館的數學館，在第一個展區把玩裝置，期待肥皂泡可以形成四維的超正方體結構。</p><p>陸續看了不少展區，可以從解析幾何通往丘成桐先生的研究。談談物理，也聊聊數學。</p><p>數學是發現吧？朋友一臉驚訝的地看著我。我輕輕的搖頭，表示自然數也可以被定義，而邏輯與語言正是用來定義自然數的工具。</p><p>這些工具本身卻是人們「定義」出來的。那是發明或是發現呢？</p><p>時間差不多就前往高美濕地，愈往濱海就感受到風力的強勁，方向盤緊握著。</p><p>誤以為這幾天天氣晴朗，不過到了之後發現雲很厚，沒辦法看到夕陽。</p><p>兩個人坐在棧道上吃著零食，隨意的聊著，儘管冷風颼颼，有好朋友一起比較不顯得冷，不孤獨。</p><p>對於孤獨，朋友似乎習以為常，畢竟能一起走的實在是太少了。</p><p>陪伴，讓集結的力量可以更強大，也可以走得比較長遠。</p><p>我也在尋找可以一起往前的伙伴，共享知識，一起成長，共享喜悅。</p><p>覺得很開心，也很幸運能遇到這樣的好朋友。</p><p>之後的路還很長，可以的話就一起冒險吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很久沒有寫文章了。&lt;/p&gt;
&lt;p&gt;一個陽光耀眼愜意的下午，舒適的溫度。&lt;/p&gt;
&lt;p&gt;下午約了好朋友一起去逛逛，從綠園道一路逛到科博館。&lt;/p&gt;
    
    </summary>
    
      <category term="My Style" scheme="https://yuehhua.github.io/categories/My-Style/"/>
    
      <category term="Friends" scheme="https://yuehhua.github.io/categories/My-Style/Friends/"/>
    
    
  </entry>
  
  <entry>
    <title>31 Variational autoencoder</title>
    <link href="https://yuehhua.github.io/2018/11/14/31-variational-autoencoder/"/>
    <id>https://yuehhua.github.io/2018/11/14/31-variational-autoencoder/</id>
    <published>2018-11-14T08:34:28.000Z</published>
    <updated>2018-12-15T06:28:08.137Z</updated>
    
    <content type="html"><![CDATA[<p>在 autoencoder 的模型裏面，會希望以一個 unsupervised 方法來做到特徵萃取的目的。</p><p>你也可以說他是一種降維的方法或是有損壓縮的方法。</p><p>基本上就是透過一個線性轉換將原來的特徵，映射到比較低維度的特徵空間上。</p><p><img src="/images/autoencoder1.svg" alt=""></p><a id="more"></a><p>圖中就是一個基本的 autoencoder 的樣子，而中間的 hidden layer 就是我們希望的特徵萃取結果。</p><p>我們希望所萃取到的特徵，可以被 <strong>還原</strong> 成原本圖形的樣子。他就會是類似壓縮跟解壓縮的概念。</p><p><img src="/images/autoencoder2.svg" alt=""></p><p>如果我們想看看他會壓縮成什麼樣子，我們可以將中間的 hidden layer 換成兩個 node 就好，如此一來，我們就可以將他視覺化。</p><p><img src="/images/autoencoder3.svg" alt=""></p><h2 id="問題"><a href="#問題" class="headerlink" title="問題"></a>問題</h2><p>常常在訓練 autoencoder 之後，我們希望他的 decoder，也就是後半的部份，可以被拿出來作為一個獨立的 generative model 使用。例如，我可以把 MNIST 資料集當中的數字 1 的圖片輸入 encoder 會得到一個特徵向量 $\mathbf{z}$，相對，這個特徵向量 $\mathbf{z}$ 可以被放到 decoder 中還原回原本的數字 1 圖片，我們希望如果日後知道某個特徵向量 $\mathbf{z}$ 也可以用同樣的作法還原出他原本的樣子。</p><p><img src="/images/autoencoder3.svg" alt=""></p><p>但往往行不通，在 $\mathbf{z}$ 上有些許的差異，就有可能產生很奇怪的結果。原因在於訓練資料通過 encoder 之後所產生的特徵向量的（流形）空間分佈，只有在訓練資料相對應的特徵向量分佈的附近才能產生出好的結果，離這些特徵向量太遠的是沒辦法產生好的結果，所以模型沒有看到過的資料就無法產生好的結果。</p><p>Variational autoencoder（VAE） 就是為了解決這樣的問題引進了 evidence lower bound（ELOB）的方法，並且將他改進成可用在 gradient-based method 上的模型。接下來會以兩種觀點切入講解，先講比較直觀的觀點。</p><h2 id="直觀觀點"><a href="#直觀觀點" class="headerlink" title="直觀觀點"></a>直觀觀點</h2><p>如果不在點附近的區域就無法產生合理的結果，我們可以去找出這些點是不是會呈現什麼樣的分佈，並且去得到這些分佈的參數。</p><p>如果是使用機率分佈的話，就可以減少空間上有 <strong>洞</strong> 的情形發生，也就是比較不會有模型沒看過的資料的地方，導致產生的圖不合理的狀況。</p><p>在 VAE 中，假設特徵向量會呈現常態分佈，所以我們會去計算這個分佈的平均值 $\mu$ 與標準差 $\sigma$，每個 hidden layer 的 node 都有一個相對應的平均值與標準差向量。</p><p><img src="/images/VAE1.svg" alt=""></p><p>相似的特徵向量會在空間分佈上在比較相近的位置，不同的特徵向量就各自形成各自的分佈情形。舉例來說，數字 1 的特徵向量會在空間分佈上比較接近，所以會形成一個分佈，跟數字 2 的分佈是不同的。如此一來，就可以用機率分佈的方式去涵蓋一些沒有被模型看過的資料。</p><p><img src="/images/VAE2.svg" alt=""></p><p>但是這要怎麼接續後面的 decoder 呢？當我們知道分佈之後，我們可以從分佈中做抽樣阿！</p><p>既然是數字 1 的分佈，我就可以從這個分佈當中做抽樣，抽樣出來的向量應該要可以還原回原本的樣子。</p><p>我們就可以將 encoder 跟 decoder 一起做訓練了！</p><h2 id="機率圖模型觀點"><a href="#機率圖模型觀點" class="headerlink" title="機率圖模型觀點"></a>機率圖模型觀點</h2><p>接下來會以比較抽象的觀點來說明。</p><h3 id="隱含因素"><a href="#隱含因素" class="headerlink" title="隱含因素"></a>隱含因素</h3><p>我們的目標是想造出一個 generative model，這個 generative model 需要產出不同的結果（$\mathbf{x}$），例如 MNIST 數字，而且我們會給一個 input（$\mathbf{z}$）來決定要產生出什麼數字，這個 input 就是決定要產生出什麼數字的 <strong>因素</strong>，我們希望從模型自己去學出來。整體來說，他是一個 unsupervised 問題，我們只能從結果（$\mathbf{x}$）去做回推這個 generative model（$p(\mathbf{x}, \mathbf{z})$）的長相，並且試圖猜測 input（$\mathbf{z}$）。</p><p>作者從貝氏定理出發，我們手上的資料只有 $\mathbf{x}$，想去求 $\mathbf{z}$，我們會假設資料產生的過程是個隨機過程，他牽涉到一個無法觀察的變數 $\mathbf{z}$。</p><p>$$<br>p_{\theta}(\mathbf{z} | \mathbf{x}) = \frac{p_{\theta}(\mathbf{x}, \mathbf{z})}{p_{\theta}(\mathbf{x})} = \frac{p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z})}{p_{\theta}(\mathbf{x})}<br>$$</p><p>這個過程包含了兩個部份：</p><ol><li>$p(\mathbf{z})$，$\mathbf{z}$ 是怎麼被產生的？</li><li>$p(\mathbf{x} | \mathbf{z})$，$\mathbf{x}$ 是如何從 $\mathbf{z}$ 產生的？</li></ol><h3 id="難題"><a href="#難題" class="headerlink" title="難題"></a>難題</h3><p>一般來說，使用貝氏定理會遇到一個難題，也就是需要知道分母怎麼估算，其中需要對 $\mathbf{z}$ 積分，那麼就需要知道所有的 $\mathbf{z}$ 排列組合，但是這是不可能的。</p><p>$$<br>p_{\theta}(\mathbf{x}) = \int p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z}) d\mathbf{z}<br>$$</p><p>目前貝氏的方法是用抽樣的方法（Markov chain Monte Calro）去估 $p_{\theta}(\mathbf{x})$，避開了直接去積分他。而且這樣的方法也太慢了，所以作者希望用 SGD 的方法來取代抽樣的方法。</p><h3 id="Evidence-lower-bound"><a href="#Evidence-lower-bound" class="headerlink" title="Evidence lower bound"></a>Evidence lower bound</h3><p>在這邊作者使用了 evidence lower bound（ELOB）的方法，既然要估 $p_{\theta}(\mathbf{x})$ 很困難，那麼我們直接去估 $p_{\theta}(\mathbf{z} | \mathbf{x})$ 如何？</p><p>那要怎麼估 $p_{\theta}(\mathbf{z} | \mathbf{x})$ 呢？如果直接算的話就是回到上面的方法，所以 ELOB 方法假設了另一個類似的 $q_{\phi}(\mathbf{z} | \mathbf{x})$ 來逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$。</p><p>要逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$，也就是要求兩個機率分佈的 KL divergence（$D_{KL}[q_{\phi}(\mathbf{z} | \mathbf{x}) || p_{\theta}(\mathbf{z} | \mathbf{x})]$）愈小愈好。</p><h3 id="架構"><a href="#架構" class="headerlink" title="架構"></a>架構</h3><p><img src="/images/VAE3.svg" alt=""></p><p>原本的問題就從左圖變成右圖，也就是以 $q_{\phi}(\mathbf{z} | \mathbf{x})$ 來逼近 $p_{\theta}(\mathbf{z} | \mathbf{x})$。$q_{\phi}(\mathbf{z} | \mathbf{x})$ 就是虛線箭頭的部份。</p><p><img src="/images/VAE4.svg" alt=""></p><p>我們再把右圖當中的兩個箭頭拆開，$q_{\phi}(\mathbf{z} | \mathbf{x})$ 就是 encoder，而在</p><p>$$<br>p_{\theta}(\mathbf{z} | \mathbf{x}) = \frac{p_{\theta}(\mathbf{x} | \mathbf{z}) p_{\theta}(\mathbf{z})}{p_{\theta}(\mathbf{x})}<br>$$</p><p>當中的 $p_{\theta}(\mathbf{x} | \mathbf{z})$ 就是 decoder 的部份。Encoder 對應到實際神經網路模型中，則是一個 $\mathbf{z} = f(\mathbf{x})$ 函數，decoder 對應的是另一個函數 $\mathbf{x} = g(\mathbf{z})$。</p><p><img src="/images/VAE5.svg" alt=""></p><p>我們可以把他轉換成這個樣子，這樣就形成了 autoencoder 的架構雛型了。</p><h3 id="抽樣"><a href="#抽樣" class="headerlink" title="抽樣"></a>抽樣</h3><p>回到前面的老問題，</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 autoencoder 的模型裏面，會希望以一個 unsupervised 方法來做到特徵萃取的目的。&lt;/p&gt;
&lt;p&gt;你也可以說他是一種降維的方法或是有損壓縮的方法。&lt;/p&gt;
&lt;p&gt;基本上就是透過一個線性轉換將原來的特徵，映射到比較低維度的特徵空間上。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/autoencoder1.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>30 結語</title>
    <link href="https://yuehhua.github.io/2018/10/31/30-conclusions/"/>
    <id>https://yuehhua.github.io/2018/10/31/30-conclusions/</id>
    <published>2018-10-30T16:14:33.000Z</published>
    <updated>2018-10-30T16:14:33.301Z</updated>
    
    <content type="html"><![CDATA[<p>原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。</p><p>我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。</p><p>這次的系列文章是在基礎之上，讓大家得以理解不同模型之間的來龍去脈以及變化性，這樣才有辦法更進一步進展到深度學習的領域。</p><p>完成這次鐵人賽的意義在於讓大家理解模型的來龍去脈，以及為什麼要用什麼樣的數學元件去兜一個模型。</p><p>當你遇到問題的時候，不可能會有一個 ready-to-use 的模型等在那邊給你用，對於人工智慧的技術應用，你必須要為自己所面對的問題和狀況自己去量身訂作自己的模型。</p><p>是的！你沒看錯，必須要由領域專家去理解自己要的是什麼，然後自己做出專門給這個情境的模型。</p><p>當你的情境非常特殊的時候，讓深度學習專家來深入其他知識領域是非常花時間的。如果以領域專家及深度學習專家之間以合作模式進行，那將會花費更高的成本在溝通上，因為人工智慧的技術應用需要對特定領域非常敏感。我個人認為只有領域專家繼續鑽研成為深度學習專家才有辦法徹底解決特定領域的問題。當然這條路非常的漫長，等於是需要一個特定領域的博士，以及深度學習的博士的等級。這些問題，只有當領域專家自己 <strong>理解</strong> 之後，不是只有 <strong>解決</strong> 問題，才能夠算是真正的 <strong>解決</strong> 了。</p><p>這系列文獻給擁有機器學習基礎，想繼續晉升到深度學習領域的朋友們。</p><p>感謝大家的支持！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。&lt;/p&gt;
&lt;p&gt;我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。&lt;/p&gt;
&lt;p&gt;這次的系列文章是在基礎之上，讓大家得以理解不同模型之間
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>29 Autoregressive generative model</title>
    <link href="https://yuehhua.github.io/2018/10/29/29-autoregressive-generative-model/"/>
    <id>https://yuehhua.github.io/2018/10/29/29-autoregressive-generative-model/</id>
    <published>2018-10-29T15:07:42.000Z</published>
    <updated>2018-10-31T07:44:57.360Z</updated>
    
    <content type="html"><![CDATA[<p>在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。</p><p>在 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a> 這篇文章以及他的論文當中又在重述了這件事。</p><p>他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？</p><a id="more"></a><p>答案幾乎是肯定的。為什麼說幾乎是肯定的呢？因為他需要滿足一些條件，才能達成訓練上模型的穩定性要求。</p><h2 id="Stable-recurrent-models"><a href="#Stable-recurrent-models" class="headerlink" title="Stable recurrent models"></a>Stable recurrent models</h2><p>問題描述是這樣的：</p><blockquote><p>理論上來說，一個有 <strong>好行為（well-behaved）</strong> 的 recurrent neural network 是否總是可以被差不多大小的 feed-forward network 取代，在不損失效能的情況下？</p></blockquote><p>這時候我們就需要知道什麼樣的 RNN 是有好行為（well-behaved）的？</p><p>當然你可以設計一個非線性的 RNN 讓 feed-forward network 無法取代，只要讓他無法用 gradient descent 訓練起來就可以了。</p><p>也就是說，<strong>好行為</strong> 的 RNN 就是，有辦法用 gradient descent 訓練起來，而不會讓梯度爆炸或是消失的模型。這樣穩定（stable）的模型就有辦法用 feed-forward network 去逼近。</p><p>論文中證明了一個重要的定理（論文中有正式的版本），我先寫他的原始描述，然後解釋：</p><blockquote><p>Thm.</p></blockquote><p>Assume the system $\phi$ is <em>$\lambda$-contractive</em>. Under <em>additional smoothness</em> and <em>Lipschitz assumptions</em> on the system $\phi$, the prediction function $f$, and the loss $p$, if</p><p>$$<br>k \ge O(log(N^{1/(1-\lambda)^3} / (\epsilon (1-\lambda)^2)))<br>$$</p><p>then after N steps of projected gradient descent with decaying step size $\alpha_t = O(1/t)$, $||w_{recurr} - w_{trunc}|| \le \epsilon$, which in turn implies $||y_t(w_{recurr}) - y_t^k(w_{trunc})|| \le O(\epsilon)$.</p><p>當你把以上定理認真看完之後你就會昏了。基本上是說，一個模型本身會要滿足幾個條件：</p><ol><li>$\lambda$-contractive</li><li>additional smoothness</li><li>Lipschitz assumptions</li></ol><p>這幾個條件簡單來說，就是你的 loss function 需要是平滑的，那麼你的梯度就不會起伏太大，導致梯度爆炸或是消失的狀況。在這樣的狀況下，就可以用 feed-forward network 去逼近。</p><h2 id="Feed-forward-network-逼近"><a href="#Feed-forward-network-逼近" class="headerlink" title="Feed-forward network 逼近"></a>Feed-forward network 逼近</h2><p><img src="/images/wavenet.gif" alt=""></p><blockquote><p>動畫取自 <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">WaveNet 官網</a></p></blockquote><p>那用 feed-forward network 去逼近有什麼好處呢？</p><p>文章中提到三大好處：</p><ol><li>平行化：你可以善用 GPU 加速</li><li>可訓練：以往 RNN 都不好訓練，但是如果可以換成 feed-forward network 就會比較容易訓練起來</li><li>推論速度：在速度上更快</li></ol><p>近年來非常多的模型都採用了 auto-regressive 的架構，從前面提到的 Transformer，到新版的 Google 小姐 - WaveNet 都用了這樣的架構。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。&lt;/p&gt;
&lt;p&gt;在 &lt;a href=&quot;http://www.offconvex.org/2018/07/27/approximating-recurrent/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;When Recurrent Models Don’t Need to be Recurrent&lt;/a&gt; 這篇文章以及他的論文當中又在重述了這件事。&lt;/p&gt;
&lt;p&gt;他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>28 Transformer</title>
    <link href="https://yuehhua.github.io/2018/10/28/28-transformer/"/>
    <id>https://yuehhua.github.io/2018/10/28/28-transformer/</id>
    <published>2018-10-28T14:58:09.000Z</published>
    <updated>2018-10-31T07:44:47.883Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。</p><p>這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。</p><p>Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。</p><p>（跟變形金剛一樣的名字耶！帥吧！）</p><a id="more"></a><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>這個架構上延續了 encoder-decoder 的架構，encoder 會將輸入的序列 $(x_1, x_2, …, x_n)$ 轉換成 $\mathbf{z} = (z_1, z_2, …, z_n)$，而 decoder 會將 $\mathbf{z}$ 轉換成 $(y_1, y_2, …, y_m)$，一次轉換一個。在每一步當中，模型都是 auto-regressive 的，也就是說，前一次產生的結果會被當成下一次的輸入進行運算。</p><p>整個 Transformer 的架構就是在 encoder 及 decoder 上使用了 stacked self-attention 以及全連接的網路。我們來看圖，在圖的左半邊就是 encoder，右半邊就是 decoder：</p><p><img src="/images/transformer1.svg" alt=""></p><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder 由 N 個完全一樣的層堆疊（stack）起來（$N = 6$）。每層包含兩個子層，第一個是一個 multi-head self-attention 的機制，第二個是簡單的全連接層網路。每個子層外都包了 residual connection 以及 layer normalization，看起來就像 $LayerNorm(x + Sublayer(x))$。</p><p>Residual connection 主要可以將前層的資訊繞過一層，直接與下一層做運算。Layer normalization 有穩定學習效果的作用。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder 大致上與 encoder 相同，差別是在用了兩個 multi-head self-attention 的機制，所以總共有3個子層。</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>在 attention 的機制上，我們在上一篇講過了。這邊我們要進一步探討這個模型用到的 scaled dot product attention。在這邊就是分成 query、key 跟 value 三者，首先要先將 query 跟所有 key 做過一次的內積，並且除以 $\sqrt{d_k}$，然後過一次 softmax 函數。計算到這邊就是權重的部份，最後權重再跟 value 去計算結果。其中 $d_k$ 是 key 向量的維度。公式在這邊：</p><p>$$<br>Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><p><img src="/images/transformer2.svg" alt=""></p><p>圖的左邊是 scaled dot product attention。為什麼要除以 $\sqrt{d_k}$ 呢？文章中有提到，內積會讓整個結果變很大，會讓梯度變得極小，這會不利於訓練，所以需要除以 $\sqrt{d_k}$。</p><p>在圖的右邊，是 multi-head self-attention，核心就是平行計算多個 scaled dot product attention 的結果，並把這些結果都串接起來。有了這樣的機制就可以不只注意一個地方，可以有多個關注點。</p><p>在 self-attention 的機制，意味著所有的 query、key 跟 value 都來自於自己。不像之前的 attention 橫跨 encoder 跟 decoder，所以資訊會從雙方而來。</p><p>在 Transformer 模型當中，有一個是 encoder-decoder attention layer，然後 encoder 跟 decoder 各有一個 self-attention layer，就是各自的第一個子層。</p><p>如此構成了整個 Transformer 模型，如果各位想知道這個模型的應用跟效能的話，請移駕去看論文，論文寫的還蠻簡單易懂的。</p><p>當然這麼模型當中有不少巧思在裡頭，有需要說明的話就提問囉！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。&lt;/p&gt;
&lt;p&gt;這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。&lt;/p&gt;
&lt;p&gt;Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。&lt;/p&gt;
&lt;p&gt;（跟變形金剛一樣的名字耶！帥吧！）&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>27 Attention model</title>
    <link href="https://yuehhua.github.io/2018/10/27/27-attention-model/"/>
    <id>https://yuehhua.github.io/2018/10/27/27-attention-model/</id>
    <published>2018-10-27T15:10:46.000Z</published>
    <updated>2018-10-31T07:44:40.087Z</updated>
    
    <content type="html"><![CDATA[<p>繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。</p><p>Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。</p><p>Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。</p><a id="more"></a><p>有幾篇論文算是開始用這樣的機制：</p><ol><li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li><li><a href="https://arxiv.org/abs/1509.00685" target="_blank" rel="noopener">A Neural Attention Model for Abstractive Sentence Summarization</a></li><li><a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noopener">Reasoning about Entailment with Neural Attention</a></li></ol><p>Attention model 在很多模型當中都是做為 encoder-decoder 之間的橋樑，原本的 encoder 跟 decoder 之間是只有一個 vector 來傳遞所有的訊息，但是多了 attention mechanism 就不一樣了。</p><p><img src="/images/seq2seq2.svg" alt=""></p><p>Attention mechanism 主要可以動態的去抓到 encoder 中傳遞的訊息，並且將這些訊息與 decoder 輸出的前一個訊息互相比對之後，透過線性組合之後輸出。這樣的輸出有什麼效果呢？他可以動態地去找到兩邊最相符的資訊，並且將他重要的部份以權重的方式凸顯出來，所以這部份是做線性組合。</p><p><img src="/images/attention.svg" alt=""></p><p>我看到一個廣義的描述方法，他是這樣說的，我們可以把 attention model 想成是一個函數，這個函數會吃兩種東西，一種是 query，另一種是 key-value 的資料結構，讓 query 去比對所有的 key 找到吻合的，會透過一個 compatibility function 去計算吻合的程度，並且作為權重，最後將權重與相對應的 value 做內積。在這邊 query、key、value 三者都是向量。</p><p>在這邊 query 會是 decoder 的 $z_0$，而 key 就是 encoder 的 $h^1, h^2, …$。每一個 key 都會去跟 query 個別通過 compatibility function 算一次吻合程度 $\alpha_0^1, \alpha_0^2, …$。這些 $\alpha_0^1, \alpha_0^2, …$ 就是權重，會去跟 value $h^1, h^2, …$ 做線性組合。在這邊為了簡單所以讓 value 跟 key 是一樣的，其實可以是不同的東西。計算出來的結果 $c^1$ 就是 context vector，會作為 decoder 的輸入，與 $z_0$ 一起計算出 $z_1$。</p><p>這樣就算是完成一輪 attention mechanism 了。下一次再繼續用 $z_1$ 當成 query 進行比對。</p><p>如此一來，就可以以動態的方式去產生序列了。Encoder 負責的是將輸入的序列轉成固定大小的向量，decoder 將這樣的向量轉換回序列，而中間需要動態調整的部份就像人的注意力一樣，會去掃視跟比對哪個部份的翻譯是最吻合的，然後將他做一個線性組合的調整。</p><p>今天的解析就到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。&lt;/p&gt;
&lt;p&gt;Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。&lt;/p&gt;
&lt;p&gt;Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>26 seq2seq model</title>
    <link href="https://yuehhua.github.io/2018/10/26/26-sequence-to-sequence-model/"/>
    <id>https://yuehhua.github.io/2018/10/26/26-sequence-to-sequence-model/</id>
    <published>2018-10-26T15:40:23.000Z</published>
    <updated>2018-10-27T03:40:37.108Z</updated>
    
    <content type="html"><![CDATA[<p>前面有提到 seq2seq model，我們就從這邊開始。</p><p>Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！</p><p><img src="/images/seq2seq.svg" alt=""></p><a id="more"></a><p>在以往的 language model 的作法裡，會是把一個 word 塞進 RNN 裡，那麼 RNN 就會立刻吐出一個相對應的 word 出來。</p><p>像是放進一個英文字，會吐出一個相對應的法文字，然後將這一層的預測結果帶給下一層。</p><p>這麼做雖然很直覺，但是他並不能完整的翻譯一個句子。</p><p>語言的語法各不相同，所以很難將詞語一一對映做成翻譯。</p><p>這個 seq2seq model 採用了不同的作法，將一組 LSTM 作為 encoder，負責將要翻譯的句子轉換成固定長度的向量，再將這個向量交給另一個 LSTM 轉換成目標句子，後面這個 LSTM 就是 decoder 的角色。</p><p>這樣的架構之下將一個模型拆成 encoder 跟 decoder 的兩個部份，讓兩個部份都可以各自接受或是產生不同長度的句子，並且得到很好的分數。</p><p>在實作上，的確是將兩個 LSTM 接起來，所以就沒什麼細節好講的。</p><p>但是這個模型確實的解決了以不同長度的句子產生不同長度的句子的問題。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前面有提到 seq2seq model，我們就從這邊開始。&lt;/p&gt;
&lt;p&gt;Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/seq2seq.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>25 Recurrent model 之死</title>
    <link href="https://yuehhua.github.io/2018/10/25/25-death-of-recurrent-model/"/>
    <id>https://yuehhua.github.io/2018/10/25/25-death-of-recurrent-model/</id>
    <published>2018-10-25T15:21:36.000Z</published>
    <updated>2018-10-25T15:21:36.182Z</updated>
    
    <content type="html"><![CDATA[<p>當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。</p><p>不要再用 RNN 為基礎的模型了！！</p><p>就是這篇 <a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" target="_blank" rel="noopener">The fall of RNN / LSTM</a></p><p>為什麼呢？</p><p>基本上裏面提到 vanishing gradient 的問題一直沒有解決以外，還有沒有辦法善用硬體的侷限在。</p><p>像這種循序型的模型，模型天生無法平行化運算，所以 GPU 就無用武之地，只能靠 CPU 慢慢跑。</p><p>那有什麼解決辦法呢？</p><h2 id="Self-attention-model"><a href="#Self-attention-model" class="headerlink" title="Self-attention model"></a>Self-attention model</h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> 這篇文章提出了 Transformer 這個模型，基本上這個模型使用了 self-attention 的機制。</p><p>要講這個之前我們要先聊聊 attention model。在 attention model 之前，sequence-to-sequence model 做出了重大的突破。一個具有彈性，可以任意組合的模型誕生了，管你是要生成句子還是怎麼樣。原本是只有 RNN 一個單元一個單元慢慢去對映 X 到 Y，sequence-to-sequence model 將這樣的對應關係解耦，由一個 encoder 負責將 X 的資訊萃取出來，再經由 decoder 將資訊轉換成 Y 輸出。</p><p>但是 LSTM 還是沒辦法記憶夠長的，後來 attention model 就誕生了。乾脆就將 encoder 所萃取到的資訊紀錄下來，變成一個，然後再丟到 decoder 去將資訊還原成目標語言，就可以完成機器翻譯了。</p><p>但是這種方式還是不脫 recurrent model，那就乾脆做成 self-attention 的機制，也就是這邊的 Transformer，完全摒棄了 recurrent 的限制。</p><h2 id="Autoregressive-generative-model"><a href="#Autoregressive-generative-model" class="headerlink" title="Autoregressive generative model"></a>Autoregressive generative model</h2><p>接著是今年6月的文章 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a>，當你的 recurrent model 不必再 recurrent！</p><p>也就是將 RNN 的問題又重述了一遍，並且提出大家都漸漸以 autoregressive generative model 來解決這樣的問題。</p><p>這篇算這引言，我接下來會開始一一解釋模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。&lt;/p&gt;
&lt;p&gt;不要再用 RNN 為基礎的模型了！！&lt;/p&gt;
&lt;p&gt;就是這篇 &lt;a href=&quot;https://towardsdatascience.com/the-fall-of-rnn-lstm-
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>24 Recurrent neural network</title>
    <link href="https://yuehhua.github.io/2018/10/23/24-recurrent-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/23/24-recurrent-neural-network/</id>
    <published>2018-10-23T13:43:49.000Z</published>
    <updated>2018-10-23T13:43:49.912Z</updated>
    
    <content type="html"><![CDATA[<p>接續上一篇。</p><h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p><ul><li>狀態都是 <strong>連續</strong> 的。</li><li>時間是離散的。</li><li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li><li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li><li><strong>允許在每個時間點給輸入</strong></li><li><strong>引入非線性</strong></li></ul><p>首先，在這邊的狀態會以一個向量做表示，大家應該也知道 RNN 的 input 是一個向量，當中的狀態也是一個向量，最後的 output 也是一個向量。而這些向量當中的的值都是連續的 $\mathbb{R}^n$（假設向量大小為 n），不像上面的模型都是離散的 $k$（假設有 k 個狀態），所以在空間上的大小可以說是擴大非常多。</p><p>接下來我們來看看時間的狀態轉換：</p><p><br></p><p><img src="/images/rnn_time.svg" alt=""></p><p><br></p><p><img src="/images/rnn_expand_time.svg" alt=""></p><p><br></p><p>在 RNN 中一樣含有內在狀態，但不同的是 RNN 可以在每個時間點上給輸入向量（$\mathbf{x^{(t)}}$），所以可以根據前一個時間點的內在狀態（$\mathbf{h^{(t)}}$）跟輸入向量去計算輸出，或是外在狀態（$\mathbf{y^{(t)}}$）。</p><p>所以大家會在一些論文上看到模型的狀態關係式長下面這個樣子：</p><p>$$<br>\mathbf{h^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}}) = \mathbf{x^{(t)}} W_x + \mathbf{h^{(t-1)}} W_h + \mathbf{b}<br>$$</p><p>$$<br>\mathbf{y^{(t)}} = g(\mathbf{h^{(t)}}) = sigm(\mathbf{h^{(t)}} W_y)<br>$$</p><p>這邊特別引入了非線性的轉換（$sigm$）來讓模型更強大。</p><p>隨著從一開始的馬可夫模型到這邊應該對這幾個模型有點感覺，其實 RNN 可以說是很大的突破，在假設上放了很多元素讓模型變得更強大。</p><h2 id="Long-short-term-memory"><a href="#Long-short-term-memory" class="headerlink" title="Long short-term memory"></a>Long short-term memory</h2><p>人們為了改進 RNN這個模型的記憶性，希望他可以記住更遠以前的東西，所以設計了 LSTM 來替換他的 hidden layer 的運作模式，後期更有 GRU，還有人說只需要 forget gate 就有很強大的效能的 MGU。這些都是對於記憶性做的改進，個人覺得這些在工程上的貢獻比較大，真正學術上的突破其實還好。</p><p>今天的整理就先到這邊啦！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;接續上一篇。&lt;/p&gt;
&lt;h2 id=&quot;Recurrent-neural-network&quot;&gt;&lt;a href=&quot;#Recurrent-neural-network&quot; class=&quot;headerlink&quot; title=&quot;Recurrent neural network&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>23 Markov chain 及 HMM</title>
    <link href="https://yuehhua.github.io/2018/10/23/23-markov-chain-and-hmm/"/>
    <id>https://yuehhua.github.io/2018/10/23/23-markov-chain-and-hmm/</id>
    <published>2018-10-23T01:21:38.000Z</published>
    <updated>2018-10-23T01:21:38.594Z</updated>
    
    <content type="html"><![CDATA[<p>上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。</p><p>這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。</p><p>在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。</p><a id="more"></a><h2 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h2><p>馬可夫模型是這樣的，他假設一個變數有不同種的狀態，例如下圖：</p><p><br></p><p><img src="/images/markov_model.svg" alt=""></p><p><br></p><p>在這邊有4個狀態，一個圓圈代表一個狀態，狀態跟狀態之間會隨著時間改變，每個狀態會有一定機率變成其他狀態，或是維持原本的狀態不變。</p><p>我們可以把目前的狀態用一個向量來表達：</p><p>$$<br>\mathbf{y} =<br>\begin{bmatrix}<br>y_1&amp;y_2&amp;y_3&amp;y_4 \\<br>\end{bmatrix}<br>$$</p><blockquote><p>注意：我們這邊使用的向量為列向量（row vector），一般在其他數學領域使用的則是行向量（column vector），兩者是可以藉由轉置來互換的，並不影響運算結果</p></blockquote><p>我們這邊用 $\mathbf{y}$ 代表他是可以被觀察到的。狀態變化我們可以用一個矩陣來表達：</p><p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p><p>其中 $a_{ij}$ 代表的是由狀態 $i$ 變成狀態 $j$ 的機率，這邊要注意的是，每一個列（row）的機率總和要是 1。</p><blockquote><p>補充：<br>由於我們的向量都是 row vector<br>$A$ 為一 right stochastic matrix，運算形式為 $\mathbf{y} A$<br>由狀態 $i$ 變成其他狀態之機率和為 1（$A \mathbf{1} = \mathbf{1}$）<br>感謝陳杰翰先生幫忙指正</p></blockquote><p>所以不同時間點的狀態變化關係可以寫成以下式子：</p><p>$$<br>\mathbf{y^{(t)}} = \mathbf{y^{(t-1)}} A<br>$$</p><p>$\mathbf{y^{(t)}}$ 的意思是第 t 次（或是時間為 t）的狀態，$\mathbf{y^{(t-1)}}$ 狀態會經過一次轉換或是運算轉變成 $\mathbf{y^{(t)}}$。</p><p>如果你把其中的第一項的運算拆開來看就會長這樣，可以自行檢驗狀態的變化：</p><p>$$<br>y_1^{(t)} = a_{11}y_1^{(t-1)} + a_{12}y_2^{(t-1)} + a_{13}y_3^{(t-1)} + a_{14}y_4^{(t-1)}<br>$$</p><p>從時間軸上來看，我們可以把狀態的轉變畫出來像是這樣：</p><p><br></p><p><img src="/images/markov_model_expand_time.svg" alt=""></p><p><br></p><p>每次的轉變我們都可以看成一個函數 $f$，他其實等同於上面提到的矩陣：</p><p>$$<br>\mathbf{y^{(t)}} = f(\mathbf{y^{(t-1)}}) = \mathbf{y^{(t-1)}} A<br>$$</p><p>所以他的意思是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$，所以這是單純的狀態變化。</p><p>上面的矩陣當中其實內容是機率，我們也可以把他轉成機率的寫法，但是解釋會變得不太一樣：</p><p>$$<br>p = f(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p><p>這邊的解釋是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$ <strong>的機率</strong>。</p><p>下句跟上句的不同在於，上句的描述是肯定的，他只描述了狀態的改變，但是下句多加描述了 <strong>這件事會發生的機率</strong>，所以應該要把 $\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}$ 理解成 <strong>這一件事</strong>，那麼 $f$ 的輸出就是機率了。</p><p>我們可以把上圖 <em>收起來</em>，所以看起來會像這樣：</p><p><br></p><p><img src="/images/markov_model_time.svg" alt=""></p><p><br></p><p>花了點時間把一些符號跟數學概念講完了，來談談他的假設，一般來說，馬可夫模型最大的假設在於：</p><p>$$<br>P(\mathbf{y^{(t)}} \mid \mathbf{y^{(1)}}, \mathbf{y^{(2)}}, \dots, \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p><p>也就是要預測第 t 單位時間的狀態，我們經歷了第 1~(t - 1) 單位時間，但是他只需要用前一個時間單位的狀態就可以預測下一個狀態，前面很多狀態都是不必要的，這我們稱為一階馬可夫模型（first-order Markov chain）。</p><p>當然可以推廣到 m 階馬可夫模型（m th-order Markov chain），那代表需要前 m 個狀態來預測下一個狀態，順帶一提，有零階馬可夫模型，那就跟我們一般的機率分佈模型（$P(\mathbf{y^{(t)}}）$）一樣。</p><p>沒有特別提的話，通常大家談的馬可夫模型都是一階馬可夫模型。一般來說，他有個非常重要的特性，就是 <strong>無記憶性</strong>，也就是他不會去記住他所經歷的狀態，他只需要用現在的狀態就可以預測下一個狀態。</p><p>不過我要特別提一下這個模型的一些其他假設：</p><ul><li>狀態是離散的。在馬可夫模型的狀態空間中是離散的，也就是你可以用一個正整數來數出有幾種狀態存在。</li><li>時間是離散的。我們剛剛有看到他計算的是第 t 單位時間，下一次就是乘上一個矩陣之後成為第 t+1 單位時間。</li><li>狀態是可被觀察的。</li><li>以一個隨機變數作為一個狀態。</li></ul><p>接下來我們來談談另一個模型。</p><p><br></p><h2 id="Hidden-Markov-model"><a href="#Hidden-Markov-model" class="headerlink" title="Hidden Markov model"></a>Hidden Markov model</h2><p>接下來是進階版的隱馬可夫模型（hidden Markov model），他的假設是這樣的，在一個系統中存在一些我們看不到的狀態，是系統的內在狀態，隨著系統的內在狀態不同，他所表現出來的外在狀態也不同，而外在狀態是我們可以觀測到的。</p><p><br></p><p><img src="/images/hmm.svg" alt=""></p><p><br></p><p>大家可以看到這個圖跟剛剛的很相似，帶是又多了一些東西。較大的圈圈是內在狀態，小的圈圈是外在狀態。隨著時間改變，內在狀態會隨著變動，內在狀態的變動我們可以用一個矩陣來表示：</p><p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p><p>裏面裝的一樣是機率。接下來，不同的內在狀態有不同的機率會噴出（emit）外在狀態，這也會用另一個矩陣表示：</p><p>$$<br>B = [b_{ij}] =<br>\begin{bmatrix}<br>b_{11}&amp; b_{12}&amp; b_{13}&amp; b_{14} \\<br>b_{21}&amp; b_{22}&amp; b_{23}&amp; b_{24} \\<br>b_{31}&amp; b_{32}&amp; b_{33}&amp; b_{34} \\<br>\end{bmatrix}<br>$$</p><p>寫成狀態轉移的關係式的話會變成：</p><p>$$<br>\mathbf{h^{(t)}} = \mathbf{h^{(t-1)}} A<br>$$</p><p>$\mathbf{h^{(t)}}$ 代表在第 t 單位時間的內在狀態。</p><p>$$<br>\mathbf{y^{(t)}} = \mathbf{h^{(t)}} B<br>$$</p><p>$\mathbf{y^{(t)}}$ 代表在第 t 單位時間根據內在狀態噴出的外在狀態。</p><p>如果在時間軸上表達的話是這個樣子：</p><p><br></p><p><img src="/images/hmm_expand_time.svg" alt=""></p><p><br></p><p><img src="/images/hmm_time.svg" alt=""></p><p><br></p><p>由於在這邊又多了一個內在狀態，所以在模型的表達力上遠遠超越馬可夫模型。舉個例子好了，假設小明很好奇在不同天氣的時候外面的人吃冰淇淋的狀況是如何，但是小明又很懶得出門看天氣，這時候他就假設天氣（晴天、陰天、雨天）是內在狀態（看不到），然後他觀察路上的人吃冰淇淋（外在狀態，吃、不吃）的多寡，這時候這麼模型就可以派上用場，他藉由持續觀察路人有沒有吃冰淇淋，可以推論外面天氣的變化狀況。</p><p>這時候我們也來總結一下，這個模型的假設：</p><ul><li>內在狀態跟外在狀態都是離散的。</li><li>時間是離散的。</li><li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li><li>以一個隨機變數作為一個狀態。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。&lt;/p&gt;
&lt;p&gt;這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。&lt;/p&gt;
&lt;p&gt;在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>22 Convolutional encoder-decoder 架構</title>
    <link href="https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/"/>
    <id>https://yuehhua.github.io/2018/10/22/22-convolutional-encoder-decoder-architecture/</id>
    <published>2018-10-22T15:10:35.000Z</published>
    <updated>2018-10-23T00:53:57.844Z</updated>
    
    <content type="html"><![CDATA[<p>標題這不是一個專有名詞。</p><p>在電腦視覺的領域中有幾個有名的問題：</p><ol><li>影像辨識（Image recognition）</li><li>物件辨識（Object detection）</li><li>語意分割（Semantic segmentation）</li></ol><a id="more"></a><p><img src="/images/cv-tasks.jpg" alt=""></p><p>影像辨識是給一張影像，希望模型可以辨識出當中的東西是什麼。輸入模型的會是影像向量，輸出的會是類別向量。</p><p>物件辨識給的同樣是一張影像，除了需要辨識出當中的物件以外，還要給出這個物體所在的位置，輸出的除了類別向量以外，還有座標。</p><p>語意分割可能無法一眼看出當中的含意，對一個句子來說，詞本身帶有一些含意，對比到影像上，句子就是影像，而詞意就是影像中的物體。語意分割是給一張影像，需要將影像中的物件切割出來，所以必須對每個像素做分類。</p><p><img src="/images/semantic_seg.jpeg" alt=""></p><p>三者是各自不同的任務。</p><p>不同的任務有些共通性，這些共通性讓他們可能都可以適用 CNN 的架構。不過這麼說還是太過粗糙了。</p><p>對於影像辨識來說，一般性架構會是有 convolution layer 為主的 feature extractor，接著會是以 fully-connected layer 為主的 classifier。在不同階段有不同的目的，在輸入影像之後要先對影像進一步抽取特徵，有了足夠的特徵之後才進行分類。</p><p>語意分割也有類似的架構，在前面會有 convolution layer 為主的 feature extractor，但是為了將每個樣素做分類，必須對每一個像素做預測，預測像素的類別。在後半的部份，有人提出了 Fully convolution network，試圖做像素的類別預測。</p><p>像素的類別預測這件事從另一個角度切入，會很像是一種生成的過程。也就是，我們在前面要將影像的特徵萃取出來，是一種將資訊壓縮的過程，在後半我們希望將壓縮的資訊還原到某種程度，我們需要產生器（generator）。</p><p><img src="/images/segnet.svg" alt=""></p><blockquote><p><a href="http://mi.eng.cam.ac.uk/projects/segnet/" target="_blank" rel="noopener">SegNet [2015, University of Cambridge]</a></p></blockquote><p>在語意分割這個問題，後來就一路發展到了 encoder-decoder 架構，我們又回到類似 autoencoder 的樣子，讓 encoder 跟 decoder 一起訓練的模型架構。在這邊 encoder 就是 feature extractor，decoder 就是一種 generator。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;標題這不是一個專有名詞。&lt;/p&gt;
&lt;p&gt;在電腦視覺的領域中有幾個有名的問題：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;影像辨識（Image recognition）&lt;/li&gt;
&lt;li&gt;物件辨識（Object detection）&lt;/li&gt;
&lt;li&gt;語意分割（Semantic segmentation）&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>21 Activation functions and ReLU</title>
    <link href="https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/"/>
    <id>https://yuehhua.github.io/2018/10/19/21-activation-functions-and-ReLU/</id>
    <published>2018-10-19T14:34:55.000Z</published>
    <updated>2018-10-19T14:34:55.385Z</updated>
    
    <content type="html"><![CDATA[<p>今天我們來談談 activation function 吧！</p><h2 id="先談談線性轉換"><a href="#先談談線性轉換" class="headerlink" title="先談談線性轉換"></a>先談談線性轉換</h2><p>談 activation function 之前先要談談線性轉換。</p><p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。</p><p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p><p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p><a id="more"></a><p>$$<br>A = U \Sigma V^T<br>$$</p><p>這 3 個矩陣分別是有意義的，$U$、$\Sigma$、$V$ 的意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：</p><p><img src="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300" alt=""></p><blockquote><p>圖來自以上提到的文章</p></blockquote><p>所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。</p><p>在模型上，我們還會加上一個 bias 來達到平移的效果。</p><p>我們可以來造一些點：</p><p><img src="/images/activation1.png" alt=""></p><p>我們可以 random 造一個矩陣：</p><p>$$<br>A =<br>\begin{bmatrix}<br>0.35166263&amp; 0.1124966 \\<br>0.25249535&amp; 0.65481947 \\<br>\end{bmatrix}<br>$$</p><p>所以我們可以對每個點做運算 $Ax$：</p><p><img src="/images/activation2.png" alt=""></p><h2 id="去吧！Activation-function！"><a href="#去吧！Activation-function！" class="headerlink" title="去吧！Activation function！"></a>去吧！Activation function！</h2><p>接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？</p><p><img src="/images/activation3.png" alt=""></p><p>大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。</p><p>所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。</p><p>那如果是用 sigmoid 的效果呢？</p><p><img src="/images/activation4.png" alt=""></p><p>他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。</p><p>我們放大來看看他整體形狀有沒有什麼變化。</p><p><img src="/images/activation5.png" alt=""></p><p>整體形狀有些微被扭曲了，不知道大家有沒有發現呢？</p><p>所以在引進 activation function 之後，模型擁有了 <strong>扭曲</strong> 的能力！</p><p>那麼 activation function 到底實際上做了什麼事呢？</p><h2 id="雕塑的工匠"><a href="#雕塑的工匠" class="headerlink" title="雕塑的工匠"></a>雕塑的工匠</h2><p>以 ReLU 來說，他就像一個工匠正在雕塑一個作品。</p><p>ReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：</p><p><img src="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/image/a51d2-dsc_7226.jpg" alt=""></p><blockquote><p>圖來自 <a href="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/" target="_blank" rel="noopener">漫遊‧藝術網＠成大校園</a></p></blockquote><p>原諒我私心用成大的雕塑品當範例。XD</p><p>ReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。</p><h2 id="消失的梯度"><a href="#消失的梯度" class="headerlink" title="消失的梯度"></a>消失的梯度</h2><p>以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。</p><p>其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。</p><p>那如果放在 CNN 的話，就會發生梯度消失的問題。</p><p>在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。</p><p>讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。</p><p>這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。</p><h2 id="模型怎麼知道要從哪裡下刀？"><a href="#模型怎麼知道要從哪裡下刀？" class="headerlink" title="模型怎麼知道要從哪裡下刀？"></a>模型怎麼知道要從哪裡下刀？</h2><p>我想蠻多人應該會有跟我一樣的問題。</p><p>我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？</p><p>答案是 backpropagation (gradient descent method) 會告訴你！</p><p>藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。</p><h2 id="ReLU-與-softplus"><a href="#ReLU-與-softplus" class="headerlink" title="ReLU 與 softplus"></a>ReLU 與 softplus</h2><p>softplus 是一個跟 ReLU 非常像的 activation function。</p><p>$$<br>softplus(x) = log(1 + e^x)<br>$$</p><p>兩者的差別：</p><p><img src="/images/Rectifier_and_softplus_functions.svg" alt=""></p><p>你可以把 softplus 看成 ReLU 的可微分版本，或是將 ReLU 看成 softplus 的簡化版本。</p><p>他在負的區域看起來跟 sigmoid function 很像，另一邊正的區域就會非常接近 identity function。</p><p>在行為上也很符合神經的特性，就是有正向的訊號就會是正向的輸出，如果是負向的訊號就不輸出。</p><p>很有趣的特性吧！</p><h2 id="為什麼-activation-function-一定要長這樣？"><a href="#為什麼-activation-function-一定要長這樣？" class="headerlink" title="為什麼 activation function 一定要長這樣？"></a>為什麼 activation function 一定要長這樣？</h2><p>其實沒有規定 activation function 一定要長怎樣。</p><p>但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。</p><p>所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>不同的模型跟問題其實適用不同的 activation function。</p><p>像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。</p><p>所以在 feature extraction 的階段就需要找適用的 activation function。</p><p>那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。</p><p>這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD</p><p>今天就到這邊告一個段落囉。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天我們來談談 activation function 吧！&lt;/p&gt;
&lt;h2 id=&quot;先談談線性轉換&quot;&gt;&lt;a href=&quot;#先談談線性轉換&quot; class=&quot;headerlink&quot; title=&quot;先談談線性轉換&quot;&gt;&lt;/a&gt;先談談線性轉換&lt;/h2&gt;&lt;p&gt;談 activation function 之前先要談談線性轉換。&lt;/p&gt;
&lt;p&gt;有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。&lt;/p&gt;
&lt;p&gt;推薦可以看周老師的線代啟示錄 &lt;a href=&quot;https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;奇異值分解 (SVD)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>20 Convolutional neural network</title>
    <link href="https://yuehhua.github.io/2018/10/17/20-convolutional-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/17/20-convolutional-neural-network/</id>
    <published>2018-10-17T14:58:40.000Z</published>
    <updated>2018-10-17T14:58:40.101Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Convolution-layer"><a href="#Convolution-layer" class="headerlink" title="Convolution layer"></a>Convolution layer</h2><p>這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？</p><p>我們先來看二維的 cross-correlation 長什麼樣子。</p><a id="more"></a><p><img src="/images/ccor2d.svg" alt=""></p><p>然後是反序的 convolution。</p><p><img src="/images/conv2d.svg" alt=""></p><p>這樣有沒有搞懂一點 convolution 的運算了呢？</p><p>接著，想像在右手邊比較小的方框就是你的 filter （或是 kernel），然後他會沿著兩個軸去移動，去掃描看看有沒有跟 filter 的 pattern 很像的，當他偵測到很像的 pattern 的時候，輸出的 feature map 的值就會很高，所以這樣就可以做到上面講的第二點，也就是位移的不變性。不過說起來也不是真的有什麼位移不變性啦！他只是沿著軸去做掃描可以減少訓練的參數，這樣 filter 還是有在位移阿！只是對於要偵測的 pattern 看起來好像有”位移不變性”一樣。到這邊我們第一點跟第二點都解決了，剩下第三點。</p><h2 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h2><p>跟第三點相關的就是 subsampling，也就是如果讓圖片變小，不只可以降低要辨識的區塊大小，還可以降低需要訓練的參數量。那要怎麼讓圖片變小？</p><p>Maxpooling 是目前主流的方法，也就是在一個 window 的範圍內去找最大值，只保留最大值。還有一種是 meanpooling，顧名思義，他取整個 window 的平均值作為保留值。</p><p>所以 subsampling 在一些應用場景是需要的，有些是不需要的，像是有些 pattern 的辨識是不能去除細節的，一旦去除細節就會造成辨識困難，那就代表他沒有辦法用 subsampling。有時候照片縮小到一定程度人類也會無法辨識當中的圖像是什麼，所以也不要用過頭。</p><h2 id="Feature-extractor-classifier-架構"><a href="#Feature-extractor-classifier-架構" class="headerlink" title="Feature extractor-classifier 架構"></a>Feature extractor-classifier 架構</h2><p>經過以上介紹後，我們可以把 convolution layer 跟 subsampling 結合起來，成為所謂的 feature extractor。</p><p>經由以上三點特性，這些 layer 的巧妙運用可以是非常棒的 feature extractor。不同種的資料特性需要不同設計的 feature extractor，接下來就是 classifier 上場了。</p><p>典型的 classifier 可以是 SVM，或是要用比較潮的 deep learning 也可以，最單純的就是前一篇提到的 MLP 了。</p><p>這樣前後組合好就是個 CNN 的雛型了！</p><h2 id="MLP-做不好的事情"><a href="#MLP-做不好的事情" class="headerlink" title="MLP 做不好的事情"></a>MLP 做不好的事情</h2><p>前一篇我們有提到 MLP 因為會從整體特徵去做內積，所以整體的模式會優先被考慮，如果有區域性的特徵並不一定會被凸顯出來。在 MLP 相對上會比較注重整體性而不是區域性，所以使用 MLP 在影像處理上就比 CNN 不是那麼有利。</p><h2 id="關鍵在哪裡？"><a href="#關鍵在哪裡？" class="headerlink" title="關鍵在哪裡？"></a>關鍵在哪裡？</h2><p>我個人認為關鍵在資料的 <strong>區域性</strong>，也就是你想做的事情其實是跟資料的區域性有關係，或是你的資料是週期性資料（週期性出現的模式也可以視為是一種區域性模式重複出現），這樣你就可以用 convolution layer！（注意！不是 CNN！）</p><p>像是音樂當中有週期性的模式就可以用，在生物領域，蛋白質會去辨認 DNA 序列，有被辨認到的部份就會有蛋白質黏附上去，生物學家會想知道到底哪些地方有蛋白質黏附，黏附的序列是區域性的，所以也有應用 CNN 技術在這方面上。</p><p>最重要的還是去了解你的資料的特性，然後了解模型當中各元件的性質跟數學特性，這樣才能正確地使用這些技術解決問題，走到這邊需要同時是領域知識的專家，也同時是 DL 的專家才行阿！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Convolution-layer&quot;&gt;&lt;a href=&quot;#Convolution-layer&quot; class=&quot;headerlink&quot; title=&quot;Convolution layer&quot;&gt;&lt;/a&gt;Convolution layer&lt;/h2&gt;&lt;p&gt;這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？&lt;/p&gt;
&lt;p&gt;我們先來看二維的 cross-correlation 長什麼樣子。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>19 Convolution 運算</title>
    <link href="https://yuehhua.github.io/2018/10/17/19-convolution-operation/"/>
    <id>https://yuehhua.github.io/2018/10/17/19-convolution-operation/</id>
    <published>2018-10-17T14:58:29.000Z</published>
    <updated>2018-10-18T13:45:31.881Z</updated>
    
    <content type="html"><![CDATA[<p>熱身運動都做好了，接下來我們就一路往影像處理上的重要技術 CNN 前進啦！</p><p>Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。</p><p>那為什麼 convolution 這麼重要，重要到要放在名稱上呢？</p><a id="more"></a><p>我先推荐大家去看台大李宏毅老師的介紹，看完再繼續往下看文章。</p><iframe width="560" height="315" src="https://www.youtube.com/embed/FrKWiRv254g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><hr><h2 id="影像處理中的祕密"><a href="#影像處理中的祕密" class="headerlink" title="影像處理中的祕密"></a>影像處理中的祕密</h2><p>我們希望從資料當中找到某種規律，這種規律我們叫作模式（pattern），模式是會重複出現的特徵，像是你可以辨識出蘋果，因為一顆蘋果有他特定的特徵，像是顏色、形狀、大小等等，當這些組合起來，並且他們都會一起出現而且重複出現，我們就可以稱他為模式。</p><p>在影片當中有提到在影像處理上有幾個特點：</p><ol><li>一些模式比整張圖小</li><li>同樣的模式可能出現在圖片的不同地方</li><li>縮放圖片不影響圖片中物件的辨識</li></ol><p>第 1 點講的是，要去辨識一個模式並不需要整張圖，也就是，<strong>local 的資訊比 global 的資訊重要</strong>。在影片中有舉例，一隻鳥的特徵有鳥喙、羽毛、翅膀等等特徵，你並不會去在意他背景的圖片長什麼樣子。鳥喙這樣的特徵他是 <strong>區域性的</strong>，你不需要整張圖片的資訊去判斷這張圖是不是鳥喙，所以在設計模型的原則上需要去擷取區域性的資訊。</p><p>第 2 點講的是，同樣的模式可能會出現在不同圖片的不同地方，這邊其實隱含了一個概念，就是 <strong>位移不變性（translation invariance）</strong>。由於同樣模式可以在不同地方上被找到，所以我們只需要一個 node 去偵測他就好了 ，這樣的話可以節省非常多的 node（或是 weight），這稱為 shared weight。</p><p>第 3 點，如果圖片縮放不影響圖片辨識，那麼。這時候我們可以做 subsampling，除了可以減少資料處理的量，也不會影響圖片的辨識結果。</p><hr><p>在我們講 CNN 之前先來幫大家惡補一下 convolution 這個運算。</p><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>在 CNN 中會用到的重要運算就是 convolution，而在 CNN 中有所謂的 convolution layer，也就是這一層的運算就是做 convolution 而不是直接內積。</p><p>我們說到 convolution layer，他真正的作用是用來做什麼的呢？他其實是用來 <strong>擷取 local 的資訊</strong> 的。</p><p>承接前面第一點提到的，在圖片當中，pattern 是 local 的資訊而不是 global 的，而 pattern 是我們想抓的資訊，所以我們要的資訊只有 local 的而已。</p><p>那麼 convolution 要如何擷取 local 的資訊呢？</p><h2 id="Convolution-運算"><a href="#Convolution-運算" class="headerlink" title="Convolution 運算"></a>Convolution 運算</h2><p>我們先來看 convolution 的原始定義，這邊假設兩個函數 $f$、$g$，則兩個函數的 convolution 為：</p><p>$$<br>(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d\tau<br>$$</p><p>以上我們看到他是一個積分式，當中引入了另一個變數 $\tau$，他代表的是<em>一個時間區間</em>，那接下來是兩個函數的<em>相乘</em>，然後將他們對變數 $\tau$ <em>積分</em>。我們來想想看，先不管變數 $\tau$，相乘後積分的運算跟什麼樣的運算很像？</p><p>是的！內積！我們來看看函數的內積長什麼樣子。</p><p>$$<br>\lt f, g \gt = \int_{-\infty}^{\infty} f(t) g(t) dt<br>$$</p><p>什麼？你跟我說這不是你認識的內積？不不不，你認識的內積其實是這個內積的離散版本。</p><p>$$<br>\lt f, g \gt = \sum_{i=1}^{n} f_i g_i<br>$$</p><p>$$<br>&lt;a, b&gt; = \sum_{i=1}^{n} a_i b_i = \mathbf{a}^T\mathbf{b}<br>$$</p><p>這樣是不是比較清楚一點了？我們來比較一下，因為積分是在 <strong>連續空間的加總</strong>，相對應的加總就是在 <strong>離散空間</strong> 的版本，那麼在連續空間上需要一個 $d\tau$ 來把連續空間切成一片一片的，但是在離散空間上，他很自然的就是 $1$ 了。這樣是不是又發覺他們根本是一樣的呢？</p><p>那你知道函數其實是一種向量嗎？不知道的同學表示沒有讀過或是沒有認真讀線性代數。</p><p>那這樣大家應該接受了函數的內積以及向量的內積其實是一樣的。接下來我們來討論一下那個神奇的 $\tau$。</p><p>$\tau$ 是一個時間區間，而積分其實是在對這個時間區間做切片然後加總，他其實跟我們在做訊號處理上的 window 的概念是一樣的，所以他其實是在某個 window 中做內積的意思。我們先來看看有 window 的內積長什麼樣子。</p><p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n + m]<br>$$</p><p>在下圖我們可以假想左邊的向量是 $b$，右邊的是 $a$，而向量 $a$ 是有被 window 給限定範圍的（m = 1…k），所以在下面這張圖就是當 n = 1、m = 1…4 的時候的情境。箭頭則是向量元素相乘的對象，每次內積完，n 就會往下移動一個元素。</p><p><img src="/images/ccor1d.svg" alt=""></p><p>計算完之後就變成一個新的向量，這就是 window 版本的內積的運作原理了！他其實有一個正式的名字，稱為 cross-correlation。</p><p>我們來看看把 convolution 離散化來看看是長什麼樣子。剛剛我們看到的 convolution 是連續的版本，是函數的版本，那我們實際上的運算是以向量去操作的，那麼離散版本的 convolution 是：</p><p>$$<br>(a * b)[n] = \sum_{m=-\infty}^{\infty} a[m] b[n - m]<br>$$</p><p>這邊的 window 就是 $m$ 這個參數，其實我們可以給他一個區間，不要是負無限大到正無限大。</p><p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n - m]<br>$$</p><p>所以這邊的 window 大小調成是 $k$ 了！</p><p><img src="/images/conv1d.svg" alt=""></p><p>你會發現，convolution 會跟 cross-correlation 很像，差別在於順序，也就是 convolution 內積的順序是相反的，所以他在數學式上的表達是用相減的，這邊的情境是 n = 6、m = 1…4。</p><p>我們來總結一下 convolution 這個運算，他其實是 local 版本的內積運算，而且他的內積方向是反序的。</p><p>補充一點，convolution 其實也是一種線性的運算喔！是不是跟前面談到的線性模型 $\sigma(W^T\mathbf{x} + \mathbf{b})$ 有點異曲同工的感覺呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;熱身運動都做好了，接下來我們就一路往影像處理上的重要技術 CNN 前進啦！&lt;/p&gt;
&lt;p&gt;Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。&lt;/p&gt;
&lt;p&gt;那為什麼 convolution 這麼重要，重要到要放在名稱上呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>18 Multi-layer preceptron</title>
    <link href="https://yuehhua.github.io/2018/10/17/18-multi-layer-preceptron/"/>
    <id>https://yuehhua.github.io/2018/10/17/18-multi-layer-preceptron/</id>
    <published>2018-10-17T14:58:18.000Z</published>
    <updated>2018-10-17T14:58:18.099Z</updated>
    
    <content type="html"><![CDATA[<p>我們來更具體一點講 multi-layer perceptron (MLP)。</p><p>最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。</p><a id="more"></a><hr><p>一般線性模型：$f(\mathbf{x}) = W^{\prime T}\mathbf{x} + b = W^T\mathbf{x}$</p><p>Linear MLP：</p><p>$f_1(\mathbf{x}) = W_1^T\mathbf{x}$</p><p>$f_2(\mathbf{x}) = W_2^Tf_1(\mathbf{x})$</p><p>$f_3(\mathbf{x}) = W_3^Tf_2(\mathbf{x})$</p><p>…</p><p>$f_n(\mathbf{x}) = W_n^Tf_{n-1}(\mathbf{x})$</p><hr><p>那這樣這個有什麼好講的呢？</p><p>大家應該有看到在這邊唯一的運算：內積（inner product）</p><h2 id="內積的意義"><a href="#內積的意義" class="headerlink" title="內積的意義"></a>內積的意義</h2><p>有念過線性代數的人應該對內積這個運算還算熟悉（在這邊都假設大家有一定線性代數基礎）。</p><p>$$<br>&lt;\mathbf{x}, \mathbf{y}&gt; = \mathbf{x}^T \mathbf{y}<br>$$</p><p>$$<br>= \begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}^T</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}</p><p>=<br>\begin{bmatrix}<br>x_1, x_2, \cdots, x_n<br>\end{bmatrix}</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p><p>內積，要先定義矩陣相乘的運算，而矩陣的相乘其實是一種線性轉換。</p><p>$$<br>f(\mathbf{x}) = A\mathbf{x}<br>$$</p><p>我們來觀察一下內積這個運算，這兩個向量會先把相對應的分量相乘。</p><p>$$<br>\begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}</p><p>\leftrightarrow</p><p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p><p>接著，再相加。</p><p>$$<br>x_1y_1 + x_2y_2 + \cdots + x_ny_n<br>$$</p><p>這時候我們可以想想看，如果當一邊是權重另一邊是資料的時候所代表的意義是什麼？</p><p>當兩個分量的大小都很大的時候，相乘會讓整個值變很大，相對，如果兩個都很接近零的話，結果值就不大。如果很多分量乘積結果都很大，相加會讓整體結果變得很大。</p><p>內積，其實隱含了 <strong>相似性</strong> 的概念在裡面，也就是說，如果你的權重跟資料很匹配的話，計算出來的值會很大。大家有沒有從裏面看出些端倪呢？</p><p>我們再由另一個角度切入看內積，內積我們可以把他寫成另一種形式，這個應該在大家的高中數學課本當中都有：</p><p>$$<br>\mathbf{x}^T \mathbf{y} = ||\mathbf{x}|| ||\mathbf{y}|| cos \theta<br>$$</p><p>這時候我們就可以看到內積可以被拆成3個部份：分別是兩個向量的大小跟向量夾角的 $cos \theta$ 值。</p><p>而當中 $cos \theta$ 就隱含著相似性在裡頭，也就是說，當兩個向量的夾角愈小，$cos \theta$ 會愈接近 1。相反，如果兩個向量夾角愈接近 180 度，那 $cos \theta$ 會愈接近 -1。剛好呈現 90 度就代表這兩個向量是 <strong>沒有關係的</strong>。</p><p>這時候可能有人會說內積又不是完全反應相似性而已，沒錯！因為他也考慮了兩個向量的長度，當一組向量夾角與另一組向量夾角相同，但是第1組的向量長度都比較長，那內積的結果第1組向量就會比較大。</p><p>所以內積是沒有去除掉向量長度因素的運算，如果單純想要用向量夾角來當成相似性的度量的話可以考慮用 cos similarity。</p><p>$$<br>cos \theta = \frac{\mathbf{x}^T \mathbf{y}}{||\mathbf{x}|| ||\mathbf{y}||}<br>$$</p><h2 id="內積與-MLP"><a href="#內積與-MLP" class="headerlink" title="內積與 MLP"></a>內積與 MLP</h2><p>那 MLP 當中內積扮演了什麼樣的角色呢？</p><p>在純粹線性的 MLP 當中，多層的 $f(\mathbf{x})$ 疊起來，我們可以把他看做是做非常多次的線性轉換或是座標轉換（change of basis），但是這是在 inference 階段的解釋。</p><p>那在 training 階段內積扮演了什麼樣的角色呢？</p><p>這邊提供一個新的想法：在 training 的過程中，我們的 dataset 是不變的，會變動的是 weight ，而內積則是在衡量這兩者之間的 feature norm 及向量夾角，所以 weight 會調整成匹配這樣特性的樣子。換句話說，內積考慮了 data 與 weight 之間的相似性與大小，並且藉由 training 去調整 weight 讓他與資料匹配。</p><p>在 inference 階段，你就可以把他看成是，weight 正在幫你做出某種程度的篩選，跟 weight 匹配的資料，內積值就會比較大，相對的是，weight 不匹配的資料，內積值就會比較小，藉由這樣將內積結果遞進到下一層的運算。</p><h2 id="機率與內積"><a href="#機率與內積" class="headerlink" title="機率與內積"></a>機率與內積</h2><p>其實還有一個觀點，就是機率觀點，機率要求一個 distribution 的長度為 1，$\int_{-\infty}^{\infty} P(X) = 1$。在這邊我們的 distribution 常常以一個 vector（或是 random variable）的形式呈現。事實上就是把一個計算好的向量去除以他的長度。如此一來，我們就去除了長度影響的因素，以符合機率的要求。</p><p>那機率當中的內積指的是什麼呢？</p><p>你如果動動手 google 一下就會發現在機率當中的內積就是這個運算</p><p>$$<br>\mathbb{E}[XY] = \int XY dP<br>$$</p><p>如果有念過統計的人，是不是覺得這東西很眼熟呢？</p><p>$$<br>cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]<br>$$</p><p>是的！他跟共變異數是有相關的，共變異數還是跟我們要去度量兩個隨機變數之間的 <strong>相似性</strong> 有關係。</p><p>$$<br>\rho = \frac{cov(X, Y)}{\sigma_X \sigma_Y}<br>$$</p><p>只要把他除以隨機變數的標準差就可以得到相關係數了呢！</p><h2 id="加入非線性"><a href="#加入非線性" class="headerlink" title="加入非線性"></a>加入非線性</h2><p>事實上，在我們生活中遇到的事物都是非線性的居多，線性模型可以施展手腳的範疇就不大了。</p><p>這時我們就希望在 MLP 中加入非線性的元素以增加模型的表達力。這時候模型的每一層就變成了：</p><p>$$<br>f(\mathbf{x}) = \sigma (W^T \mathbf{x})<br>$$</p><p>而當中的 $\sigma$ 就成了我們的 activation function 了，也就是非線性的來源！</p><h2 id="Fully-connected-layer"><a href="#Fully-connected-layer" class="headerlink" title="Fully connected layer"></a>Fully connected layer</h2><p>當這些層的 node 都互相連接，就代表了所有 node 都參與了計算，這個計算所考慮的資料是 <strong>global</strong> 的。</p><p>這些層所做的運算是相對 <strong>簡單</strong> 的（相對 convolution 來說）。</p><p>每個 node 對每一層運算所做的貢獻是 <strong>弱</strong> 的。當一層的 node 數很多，e.g. 上千個 node，每個 node 的運算結果就會被稀釋掉了。即便內積運算有包含個別值的大小的成份在裡頭，當 node 數一多，這樣的影響也會被減弱，剩下的是整體向量與向量之間的相似性。但有一個情況例外，當有 node 的值極大，e.g. $x_i / x_j = 1000$，當有人是別人的千倍以上的話就要注意一下了，這也是很常在機器學習當中會遇到的問題，這時候就會需要做 normalization 來處理。</p><p>最後提醒，內積的運算中雖然有隱含相似性在其中，但是他 <em>不等同</em> 於 <strong>去計算相似性</strong>。</p><p>今天的討論就到這邊告一個段落，希望在大家思考 deep learning 模型的時候，這些東西有幫上一些忙。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我們來更具體一點講 multi-layer perceptron (MLP)。&lt;/p&gt;
&lt;p&gt;最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>17 Autoencoder</title>
    <link href="https://yuehhua.github.io/2018/10/17/17-autoencoder/"/>
    <id>https://yuehhua.github.io/2018/10/17/17-autoencoder/</id>
    <published>2018-10-17T14:36:06.000Z</published>
    <updated>2018-10-19T15:26:36.709Z</updated>
    
    <content type="html"><![CDATA[<p>既然前一篇提到學習特徵是一件重要的事，那麼我們就來講講 autoencoder 吧！</p><p>Autoencoder 就是一個 unsupervised 方法，試圖學習出可以用的特徵。雖然不少人可能會說他是一個壓縮的方法或是一個降維的方法，其實他都是。</p><a id="more"></a><p>Autoencoder 就是希望給一個 input ，經過轉換之後會成為一個新的、維度比較低的向量，並且可以再由這個向量透過轉換還原成原本的 input。既然低維的向量可以還原成 input 的樣子，那代表他含有足夠的資訊可以還原，所以你可以把他看成是一種壓縮或是降維。我想強調的是，autoencoder 可以將一個 input 的樣子轉換成一個新的表示方式，我們就可以用新的視角來看待他。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>Autoencoder 算是一個非常簡單的模型，他就是將資料向量經過一個轉換過後成為新的向量，在讓新的向量經過轉換過後還原成原本的向量。</p><p>抽象上來說比較像是：</p><p>$$<br>X \rightarrow Z \rightarrow X<br>$$</p><p>Autoencoder 可以被拆解成兩個部份：一個是 encoder $X \rightarrow Z$，另一個是 decoder $Z \rightarrow X$ 的部份。</p><p>在這個架構上 encoder-decoder 是相接在一起做訓練的。</p><p>最簡單的 autoencoder 在 encoder 跟 decoder 上各自都只有一層，當然你可以不只一層，取決於你自己的狀況。</p><p>他是一個非常簡單的架構，所以有非常多的變體，像是 denoising autoencoder、sparse autoencoder、variational autoencoder…等等。</p><p>每個都有自己獨特的用法，所以這是一個非常基礎的模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;既然前一篇提到學習特徵是一件重要的事，那麼我們就來講講 autoencoder 吧！&lt;/p&gt;
&lt;p&gt;Autoencoder 就是一個 unsupervised 方法，試圖學習出可以用的特徵。雖然不少人可能會說他是一個壓縮的方法或是一個降維的方法，其實他都是。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>16 Representation learning</title>
    <link href="https://yuehhua.github.io/2018/10/16/16-representation-learning/"/>
    <id>https://yuehhua.github.io/2018/10/16/16-representation-learning/</id>
    <published>2018-10-16T15:28:04.000Z</published>
    <updated>2018-10-16T15:32:43.024Z</updated>
    
    <content type="html"><![CDATA[<p>機器學習的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。</p><p>當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個成效，像是你給的特徵不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些資料當中，那麼模型根本就預測不出來，所以玩過機器學習的人就會知道特徵工程的重要性。</p><a id="more"></a><p>以往特徵工程是需要人自己手動處理的，如今我們也希望由機器學習的模型中自動學出來。大家可以看到我們的技術進展：從以往的手寫程式進展到經典的機器學習技術，這是一個巨大的飛躍。</p><p><img src="/images/diagram-deep-learning.png" alt=""></p><blockquote><p>From <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?ie=UTF8&amp;qid=1472485235&amp;sr=8-1&amp;keywords=deep+learning+book" target="_blank" rel="noopener"><em>Deep Learning</em></a> by Ian Goodfellow and Yoshua Bengio and Aaron Courville</p></blockquote><hr><p><br><br></p><h2 id="他幫我們解決了什麼問題呢？"><a href="#他幫我們解決了什麼問題呢？" class="headerlink" title="他幫我們解決了什麼問題呢？"></a>他幫我們解決了什麼問題呢？</h2><p>以往的手寫程式需要工程師非常的聰明，他需要知道在 input 與 output 之間的所有規則，然後把這些規則化成可以執行的程式，這些實作的過程需要花非常大量的人力跟腦力。</p><p><br></p><p><img src="/images/before_ml.svg" alt=""></p><p><br></p><p>然而，我們進展到機器學習的技術，我們試圖去收集一些資料，這些資料符合我們預期的 input 與 output 之間的關係。</p><p><br></p><p><img src="/images/after_ml.svg" alt=""></p><p><br></p><p>他可以幫我們將中間的 <strong>過程</strong> 連接起來，我們不需要去 <em>手刻</em> 或是 <em>事先知道</em> 這些過程，更何況自然界很多過程都是 <strong><em>人類沒辦法理解的</em></strong> 或是 <strong><em>還不知道的</em></strong>。</p><p><br></p><p><img src="/images/mnist.svg" alt=""></p><p><br></p><p>這些過程在數學家的眼中就稱為 <strong>函數</strong>，對於機器學習專家來說，input 與 output 之間有無限多種函數的可能。哪一種可能才是最符合我們資料的長相的？我們希望挑出最有可能的那一種，就把那就把那一種當成是模型，並且輸出，這樣我們就能讓機器自動去學出 input 與 output 的對應關係，這是一個飛躍性的進展。</p><p><br></p><h2 id="特徵工程（feature-engineering）"><a href="#特徵工程（feature-engineering）" class="headerlink" title="特徵工程（feature engineering）"></a>特徵工程（feature engineering）</h2><p>接著我們意識到：我們還是需要手動去處理特徵。經典的機器學習模型只幫我們處理了 <strong>將特徵對應到輸出的關係</strong>，我們還是得藉由特徵萃取的技術來轉換，而我們很難知道什麼樣的特徵萃取才真正能夠把資料中我們想要的資訊萃取出來，這部分就進到 representation Learning 的範疇。</p><p><br></p><h2 id="自動化特徵萃取（Automatic-feature-extraction）"><a href="#自動化特徵萃取（Automatic-feature-extraction）" class="headerlink" title="自動化特徵萃取（Automatic feature extraction）"></a>自動化特徵萃取（Automatic feature extraction）</h2><p>在特徵萃取的過程中，常常我們面對的是高維度的向量，由於我們很難去理解高維度的向量之間的轉換，導致我們在轉換的時候會遇上困難，我們根本不知道需要轉換成什麼樣維度的向量，我們也不知道中間需要什麼樣的轉換函數。在數學領域當中，有相關的領域稱為微分幾何，所以常常我們會討論在數學上的流形（manifold），representation learning 就是希望連同特徵萃取以及模型可以一併處理，也就是藉由模型的過程會到回饋（從 gradient descent 等等方法），去引導特徵萃取的過程，進而去學到 <strong>特徵-特徵</strong> 之間轉換的模式。</p><p><br></p><h2 id="深度學習"><a href="#深度學習" class="headerlink" title="深度學習"></a>深度學習</h2><p>深度學習就是一種 representation Learning。他希望資料在高維度的轉換當中，可以去萃取出足夠而抽象的資訊，去進行預測。而深度學習只是將特徵-特徵之間的轉換模式以 <strong>層-層</strong> 之間的轉換實現，而高維的特徵向量以 <strong>層</strong> 的形式呈現。所以越深的網路代表著經過多次的函數處理跟萃取，所萃取的資訊的抽象程度越高，抽象程度越高，就越接近人類所想像的。</p><p><br></p><h2 id="Representation-learning"><a href="#Representation-learning" class="headerlink" title="Representation learning"></a>Representation learning</h2><p>如同前面描述到的，我們需要更少對於特徵工程的依賴，增加自動化特徵萃取的使用。所以我們用”學習”的方式讓模型自動去學到他要的特徵，自動去做特徵萃取。那麼 representation learning 更深層的意思是什麼呢？</p><blockquote><p>你需要的不是一個答案，而是一個表示方式。</p></blockquote><p>以上是我在工研院的課程對學員們講過的話，一句話解釋 representation learning 大概是這個意思。</p><p>舉個例子好了，人類在照片中可以辨識出當中的狗狗，人們在交談的時候可以以語言的”狗”來描述同一個概念，基本上他們都擁有相同的資訊量。對於狗的概念來說，照片中的圖像只是這個概念的一種表示方式，語言中也有對應的表現方式。讓機器學會狗的概念，就是要讓他可以從圖像或是語言中可以萃取出有相同資訊量的東西，這樣的東西可以是以資料結構的方式表示，或是以一個向量表示，所以你需要的不是一個答案，而是一種表示方式，讓你可以看的懂的表示方式。</p><p>最終極的情況來說，在人類腦中很多既定的概念都已經存在，並沒有什麼新的概念出現了。出現的只有新的概念以不同的形式或是姿態出現，這互相之間都是可以轉換的，當然，轉換伴隨著資訊量的流失。</p><p>記得有個實驗在測試在發展成熟的社會是不是比原始社會的人們更聰明，實驗者設計了類似配對記憶遊戲，在卡牌上畫上各種現代日常生活中會看到的物件，分別測試了都市的人們以及原始部落的人們，果然在都市的人們有比較好的成績。不過這個實驗有個為人詬病的地方，都市人當然熟悉日常生活周遭的物件，原始部落的人們卻從來也沒有看過這些東西。所以又有另一組人馬將實驗換成在原始部落中常常看到的植物的葉子，對都市的人們來說，那些葉子根本無從分辨，但是原始部落的人們的測驗結果卻跟都市的人們對日常生活物件有一樣高的分數。代表人的腦袋並沒有差別，只是認得的東西不同。</p><p>或許在人們的腦袋中，有某些概念是一樣的，但是有不同的表現形式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;機器學習的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。&lt;/p&gt;
&lt;p&gt;當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個成效，像是你給的特徵不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些資料當中，那麼模型根本就預測不出來，所以玩過機器學習的人就會知道特徵工程的重要性。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>15 為什麼要深？</title>
    <link href="https://yuehhua.github.io/2018/10/15/15-why-deep/"/>
    <id>https://yuehhua.github.io/2018/10/15/15-why-deep/</id>
    <published>2018-10-15T06:56:39.000Z</published>
    <updated>2018-12-12T14:52:34.204Z</updated>
    
    <content type="html"><![CDATA[<p>接著我們就來到了蠻重要的問題，既然一個 hidden layer 的網路架構就可以逼近任何連續函數，那麼為什麼要深度學習？</p><a id="more"></a><p>對於這個問題，台大李宏毅老師有非常詳細的講解：</p><iframe width="560" height="315" src="https://www.youtube.com/embed/FN8jclCrqY0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>影片裏面實驗很多，所以我還是總結一下：</p><ol><li>用邏輯電路類比神經網路的話，你一樣可以用單層的並聯將所有邏輯閘都起來成為一個電路，一樣可以達到相同的效果，但是用多層的串聯可以將所需要的邏輯閘數目減少（類比神經網路的神經元），所以可以達到減少參數的效果。</li><li>使用 ReLU 作為 activation function 就是要用分段線性的方式來逼近一個函數，在網路參數相進的情況下，單層網路所能產生的”段”比較少，多層網路所產生的”段”比較多，產生的線段較多就可以去逼近一個更複雜的函數，所以模型就比較強大。</li><li>計算分段線性的數量，當你有 $N$ 個神經元，單層網路裡最多只能產生 $N - 1$ 個線段，多層網路，每層安排兩個神經元，可以產生 $2^{\frac{N}{2}}$ 個線段。</li></ol><p>更有文獻提到，計算線段的數量，如果你的網路每層有 $K$ 個神經元，而且有 $H$ 層，那麼至少會有 $K^H$ 個線段。</p><p>由於深度是放在指數上面，所以增加深度就可以簡單地提高模型的複雜度，也就可以讓模型變得比較強大。</p><h2 id="計算複雜度"><a href="#計算複雜度" class="headerlink" title="計算複雜度"></a>計算複雜度</h2><p>在一些電腦的問題上，我們常常形容問題是 NP 或者不是 NP，來描述一個問題的複雜度有多高。</p><p>一個問題的時間複雜度可以由現代電腦在多項式時間內解決，我們稱為 P（polynomial time）。如果一個問題至少跟 P 一樣或是更難，那我們稱為 NP（nondeterministic polynomial time）。</p><blockquote><p>NP 的形式定義為：一個問題可以由 non-deterministic Turing machine 在多項式時間內解決的問題。</p></blockquote><p>NP-complete 問題是 NP 問題當中最難的了，計算複雜度大概會是指數級成長，這種成長速度應該使用神經網路有辦法克服。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接著我們就來到了蠻重要的問題，既然一個 hidden layer 的網路架構就可以逼近任何連續函數，那麼為什麼要深度學習？&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
  <entry>
    <title>14 淺層神經網路</title>
    <link href="https://yuehhua.github.io/2018/10/14/14-shallow-neural-network/"/>
    <id>https://yuehhua.github.io/2018/10/14/14-shallow-neural-network/</id>
    <published>2018-10-14T15:16:02.000Z</published>
    <updated>2018-10-15T07:03:50.798Z</updated>
    
    <content type="html"><![CDATA[<p>為什麼大家到現在都這麼迷神經網路模型？</p><p>我想主因不是因為他是模擬生物而來，他有一些更扎實的數學特性。</p><p>我們前面講過各種線性模型，然後將他過渡到神經網路。</p><p>今天要告訴大家，即便是淺層的神經網路也是很厲害的。</p><a id="more"></a><h2 id="Universal-approximation-theorem"><a href="#Universal-approximation-theorem" class="headerlink" title="Universal approximation theorem"></a>Universal approximation theorem</h2><p>Universal approximation theorem 是個淺層的神經網路的數學定理。</p><p>他說：一個簡單的 feedforward network，只包含了一個 hidden layer，並且有適切的 activation function，包含有限個神經元的情況下，可以去逼近任何連續函數。</p><p>在 1989 就以經由 George Cybenko 先生第一次證實了這個定理，他使用了 sigmoid activation function。</p><p>如果大家對細節有興趣，請參閱 <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" target="_blank" rel="noopener">維基百科的條目</a>。</p><p>這代表什麼呢？</p><p>到目前為止，我們都會將一個現實中的問題化成一個數學問題，一個數學問題基本上都是包含函數的。</p><p>像是影像辨識，我們就可以看成一個可以輸入影像的函數，這個函數會輸出辨識結果，也就是分類。</p><p>我們可以這樣寫 $f: \text{images} \rightarrow \text{classes}$。</p><p>所以語音辨識就是 $f: \text{speech} \rightarrow \text{text}$。</p><p>聊天機器人就是 $f: \text{text} \rightarrow \text{text}$。</p><p>…</p><p>到這邊你可以想想，幾乎人類的問題都可以化成一個函數來解答。</p><p>所以可以逼近任何連續函數的模型根本就可以解答任何問題的意思阿！</p><p>所以大家才拼了命的用這個模型去解決很多問題。</p><h2 id="待解問題"><a href="#待解問題" class="headerlink" title="待解問題"></a>待解問題</h2><p>如果這個模型這麼萬能，那麼他就真的沒有缺點嗎？</p><p>我們需要從這個定理切入，定理中描述的有限個神經元，但至少不是無限，他並沒有說需要幾個。</p><p>這理所當然，因為沒有人知道你的問題有多複雜，需要用多難的方法解嘛！</p><p>然後 activation function 也是沒有提的。</p><p>所以這就是留給現代的大家去決定的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;為什麼大家到現在都這麼迷神經網路模型？&lt;/p&gt;
&lt;p&gt;我想主因不是因為他是模擬生物而來，他有一些更扎實的數學特性。&lt;/p&gt;
&lt;p&gt;我們前面講過各種線性模型，然後將他過渡到神經網路。&lt;/p&gt;
&lt;p&gt;今天要告訴大家，即便是淺層的神經網路也是很厲害的。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://yuehhua.github.io/categories/Machine-Learning/"/>
    
      <category term="機器學習模型圖書館：從傳統模型到深度學習" scheme="https://yuehhua.github.io/categories/Machine-Learning/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B%E5%9C%96%E6%9B%B8%E9%A4%A8%EF%BC%9A%E5%BE%9E%E5%82%B3%E7%B5%B1%E6%A8%A1%E5%9E%8B%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92/"/>
    
    
  </entry>
  
</feed>
