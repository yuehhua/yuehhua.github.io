
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"Website","@id":"https://yuehhua.github.io","author":{"@type":"Person","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"},"name":"Dream Maker","description":null,"url":"https://yuehhua.github.io"}</script>
    <meta property="og:type" content="blog">
<meta property="og:title" content="Dream Maker">
<meta property="og:url" content="https://yuehhua.github.io/page/2/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dream Maker">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/03-recommend/">
                            Recommended docker apps
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:58:45+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Docker/">Docker</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <pre><code>docker pull sequenceiq/hadoop-ubuntu
docker pull nginx  # official repos
docker pull php  # official repos
docker pull dockerfile/java
docker pull dockerfile/python
docker pull pypy  # official repos
docker pull rocker/rstudio:latest
</code></pre>
                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/03-recommend/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/02-dockerfiles/">
                            Writing a Dockerfile
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:49:48+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Docker/">Docker</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>We already know how to get a image and modify it, then share it through Docker Hub. It can be more portable and easy to customize. Just write a script in Dockerfile, and then use <code>docker build</code> to build an image following the script in Dockerfile!</p>
<p>A demo of a Dockerfile:</p>
<pre><code>FROM tutum/apache-php:latest
MAINTAINER Huang AMing 
RUN sed -i &apos;s/archive.ubuntu.com/free.nchc.org.tw/g&apos; \
  /etc/apt/sources.list
RUN apt-get update \
  &amp;&amp; apt-get install -y php5-xdebug
ADD https://phar.phpunit.de/phpunit.phar /usr/local/bin/
RUN cd /usr/local/bin \ 
  &amp;&amp; chmod +x phpunit.phar \ 
  &amp;&amp; mv phpunit.phar phpunit
</code></pre><blockquote>
<p><strong>FROM</strong>: what image would be the base to build<br><strong>MAINTAINER</strong>: declare the maintainer<br><strong>RUN</strong>: to run command in the container in order to build image<br><strong>ADD</strong>: to add file from <em>host OS</em> or <em>remote directory</em> to the specified <em>directory of  image</em>, the file would be unziped automatically if it is compressed</p>
</blockquote>
<p>Build the image:</p>
<pre><code>docker build &lt;the-Dockerfile-placed&gt;
example&gt; # build image following the Dockerfile in the current directory and tag as ymhuang0808/phpunit-testing:php5.5.9
example&gt; docker build -t=&quot;ymhuang0808/phpunit-testing:php5.5.9&quot; .
</code></pre><blockquote>
<p><strong>Flags:</strong><br><code>-t</code> flag assign the new image a tag</p>
</blockquote>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/02-dockerfiles/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/01-basic-commands/">
                            Basic commands
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:48:11+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Docker/">Docker</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>Usage:  <code>[sudo] docker [command] [flags] [arguments] ..</code></p>
<h2 id="Get-information"><a href="#Get-information" class="headerlink" title="Get information"></a>Get information</h2><p>The version of docker:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">example&gt; docker version</span><br><span class="line">Client version: 1.4.1</span><br><span class="line">Client API version: 1.16</span><br><span class="line">Go version (client): go1.3.3</span><br><span class="line">Git commit (client): 5bc2ff8</span><br><span class="line">OS/Arch (client): linux/amd64</span><br><span class="line">Server version: 1.4.1</span><br><span class="line">Server API version: 1.16</span><br><span class="line">Go version (server): go1.3.3</span><br><span class="line">Git commit (server): 5bc2ff8</span><br></pre></td></tr></table></figure>
<p>Show the information about docker status:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">example&gt; docker info</span><br><span class="line">Containers: 5</span><br><span class="line">Images: 5</span><br><span class="line">Storage Driver: aufs</span><br><span class="line">	Root Dir: /var/lib/docker/aufs</span><br><span class="line">	Dirs: 15</span><br><span class="line">Execution Driver: native-0.2</span><br><span class="line">Kernel Version: 3.13.0-32-generic</span><br><span class="line">WARNING: No swap <span class="built_in">limit</span> support</span><br></pre></td></tr></table></figure>
<h2 id="Basic-operations"><a href="#Basic-operations" class="headerlink" title="Basic operations"></a>Basic operations</h2><p>Download image file provided from Docker Hub:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull &lt;image-name&gt;</span><br><span class="line">example&gt; docker pull ubuntu:14.04</span><br></pre></td></tr></table></figure>
<p>Run specified cmd on this image:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run &lt;image-name&gt; &lt;cmd&gt;</span><br><span class="line">example&gt; <span class="comment"># Run interactively in the foreground</span></span><br><span class="line">example&gt; docker run -i -t ubuntu /bin/bash</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>Flags:</strong></p>
</blockquote>
<blockquote>
<p><code>-t</code> flag assigns a terminal inside our new container.</p>
</blockquote>
<blockquote>
<p><code>-i</code> flag allows us to make an interactive connection by grabbing the standard in (<code>STDIN</code>) of the container.</p>
</blockquote>
<blockquote>
<p><code>-d</code> flag tells Docker to run the container and put it in the background, to daemonize it.</p>
</blockquote>
<blockquote>
<p><code>-p</code> flag is new and tells Docker to map any required network ports inside our container to our host.</p>
</blockquote>
<blockquote>
<p><code>-P</code> flag, a shortcut for <code>-p</code>,  maps any required network ports exposed in our image to a high port (from the range 49153 to 65535) on the local Docker host.</p>
</blockquote>
<blockquote>
<p><code>-v</code> flag mount the host directory into container directory, ex <code>-v &lt;host-dir&gt;:&lt;container-dir&gt;</code> or <code>-v &lt;container-dir&gt;</code></p>
</blockquote>
<blockquote>
<p><code>--name</code> flag assigns a name to this container</p>
</blockquote>
<blockquote>
<p><code>&lt;image-name&gt;</code>: ubuntu:14.04</p>
</blockquote>
<blockquote>
<p><code>&lt;cmd&gt;</code>: /bin/echo “hello world”</p>
</blockquote>
<p>View info of container:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker inspect &lt;container-id-or-name&gt;</span><br><span class="line">example&gt;</span><br></pre></td></tr></table></figure>
<p>Lists containers:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">example&gt; <span class="comment"># To check which docker is running</span></span><br><span class="line">example&gt; docker ps</span><br><span class="line">example&gt; <span class="comment"># </span></span><br><span class="line">example&gt; docker ps -l</span><br><span class="line">example&gt; <span class="comment"># Show all docker in execution</span></span><br><span class="line">example&gt; docker ps -a</span><br></pre></td></tr></table></figure>
<p>Stops running containers:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker stop &lt;container-id-or-name&gt;</span><br><span class="line">example&gt; <span class="comment"># This will stop the container id of cfc68</span></span><br><span class="line">example&gt; docker stop cfc68</span><br></pre></td></tr></table></figure>
<p>Commit a modified image:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker commit &lt;container-id-or-name&gt; &lt;image-name&gt;</span><br><span class="line">example&gt; docker commit -m=<span class="string">"Add PHPUnit and xdebug"</span> \</span><br><span class="line">example&gt; -a=<span class="string">"Huang Yi-Ming"</span> aeeb1980c96e \</span><br><span class="line">example&gt; ymhuang0808/phpunit-testing:php5.5.9</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>Flags:</strong></p>
</blockquote>
<blockquote>
<p><code>-m</code> flag is used to give a comment to this commit.</p>
</blockquote>
<blockquote>
<p><code>-a</code> flag declare the maintainer’s name.</p>
</blockquote>
<p>Remove the image:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi &lt;image-id&gt;</span><br></pre></td></tr></table></figure>
<p>Login Docker Hub (before you push a image to repository):</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">example&gt; docker login</span><br><span class="line">Username: [ENTER YOUR USERNAME] Password:</span><br><span class="line">Email: [ENTER YOUR EMAIL] Login Succeeded</span><br></pre></td></tr></table></figure>
<p>Push image to the repository (usually on Docker Hub):</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker push &lt;image-name&gt;</span><br><span class="line">example&gt; docker push a504082002/ubuntu:14.04</span><br></pre></td></tr></table></figure>
<p>Shows us the standard output of a container:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs</span><br></pre></td></tr></table></figure>
<h2 id="Some-advanced-applications"><a href="#Some-advanced-applications" class="headerlink" title="Some advanced applications"></a>Some advanced applications</h2><p>Query the port mapping of a container:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker port &lt;container-id-or-name&gt; &lt;port-exposed&gt;</span><br><span class="line">example&gt; <span class="comment"># which port was mapped to the port 5000 on container named of nostalgic_morse</span></span><br><span class="line">example&gt; docker port nostalgic_morse 5000</span><br><span class="line">0.0.0.0:49155</span><br></pre></td></tr></table></figure>
<p>Running a web application in docker:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example&gt; <span class="comment"># we're going to run a Python Flask application with image: training/webapp</span></span><br><span class="line">example&gt; docker run -d -P training/webapp python app.py</span><br></pre></td></tr></table></figure>
<p>Assign command to specific docker:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example&gt; <span class="comment"># Return to docker again after exit</span></span><br><span class="line">example&gt; docker <span class="built_in">exec</span> -t -i 0f83f1728262 bash</span><br></pre></td></tr></table></figure>
<p>To remove docker that you don’t need anymore:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example&gt; docker rm 0f83f1728262</span><br></pre></td></tr></table></figure>
<p>To mount a host directory as a disk in docker:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">example&gt; <span class="comment"># Mount /myData into docker</span></span><br><span class="line">example&gt; docker run -t -i -v /myData centos:centos6 bash</span><br></pre></td></tr></table></figure>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/01-basic-commands/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/mindset/">
                            Mindset（中譯：心態致勝）
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:46:01+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Psychology/">Psychology</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p><img src="https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/001/074/84/0010748470.jpg&amp;v=58d4f5ad&amp;w=348&amp;h=348" alt="心態致勝：全新成功心理學"></p>
<blockquote>
<p>圖片取自博客來</p>
</blockquote>
<p>今天看到書的前三分之一，但是忍不住要來跟大家分享書中的內容。這本看名字跟外表很容易被埋沒那些講成功的商業書籍中，但是這本可是史丹佛大學的心理學權威 Dr. Carol Dweck 的研究成果阿！他也有在 TED 發表過演講，是很值得看的一本書！</p>
<iframe width="560" height="315" class="center" src="https://www.youtube.com/embed/PfX1YpHzr64" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<p>他把人的心態分成兩種：成長心態與定型心態。教授的觀察來自一群面對難題的十歲男孩</p>
<blockquote>
<p>面對費解謎題時，一位十歲男孩把椅子往前拉，搓揉雙手，咂嘴喊道：「我愛挑戰！」另一個孩子在為難題傷腦筋時，抬起頭，露出愉悅表情，自信地說：「你知道嗎？我原本就期待這會很有教育性！」</p>
</blockquote>
<p>教授把人分為這樣的兩種心態。</p>
<p>對定型心態的人來說，他相信他自身的素質是天生、無法改變的，那人要不就是聰明要不就是不聰明，失敗意味著不聰明。這種心態使得自己需要一再的證明自己，證明自己是聰明的。若是你有一定程度的智力、一種特定性格、一種特定品性，你最好證明你自己是聰明的，不然就大事不妙。</p>
<p>成長心態剛好相反，這些人認為可以透過努力、策略與他人的幫助，培養或是加強個人的素質。每個人的生長環境、資質、興趣跟性格可能各自不同，但是大家都可以透過努力進而改變成長，他們樂於接受挑戰並拓展自己的領域。</p>
<h2 id="反應"><a href="#反應" class="headerlink" title="反應"></a>反應</h2><p>兩種心態的人面對同一件糟糕的事情反應有所不同，定型心態的人會認為自己被否定、是個失敗者，但他們在面對失敗之前都跟成長心態者一樣的樂觀。成長心態面對同樣的事情，同樣會有低落的情緒反應，但是除了情緒反應會開始思考要如何突破這樣的困境，他們知道可以藉由努力的方式幫助自己，他們面對挫折跟失敗的態度相當坦然。</p>
<p>定型心態者他們不相信一些對於冒險及毅力的格言，例如，失敗為成功之母。對他們來說，失敗只不是一再的證明了他們的資質差與不聰慧。人們對於風險與努力的看法源自於他們的基本心態，成長心態的人重視自我挑戰的價值與努力的重要性，然而定型心態的人不喜歡挑戰與努力，因為他們認為人的特質是不變的，努力沒辦法改變什麼，他們很快就會畏懼挑戰，並認為努力的人都像是笨蛋。</p>
<h2 id="低努力"><a href="#低努力" class="headerlink" title="低努力"></a>低努力</h2><p>研究發現，有定型心態的青少年對於資源的運用，不是用於學習，而是用來保護自尊。他們在學校中的主要目標，除了讓自己看起來比較聰明，另外一個就是儘可能的不費力。這些人會試圖降低自己努力的程度，好讓考試無法真正測驗出他們的實力，進而讓自己有理由看起來不愚蠢。但在成長心態的學生看來，停止嘗試努力是無法理解的，這絕對是一段探索自己喜歡什麼、學習新科目，並思考將來想做什麼的好時機。</p>
<h2 id="天份"><a href="#天份" class="headerlink" title="天份"></a>天份</h2><p>書中也談到關於藝術天份，有些人可能不需要怎麼努力就可以畫的很好，但有些人總是不論怎麼嘗試努力就是無法開竅。書中談到一個例子，是一些人在去上某個課程前都拙於繪畫，但是在掌握方法之後，他們都可以畫出一定水準的畫作來。因為大多數人不了解繪畫的構成要素，繪畫的重點不在於繪畫技巧，而是在於如何『看』，在於感知光影的變化、邊緣輪廓與空間關係。有些人能夠在生活中掌握這些方法，其他人則是需要下功夫去學習他，但只要掌握方法，人人都可以做的到。</p>
<h2 id="讚美"><a href="#讚美" class="headerlink" title="讚美"></a>讚美</h2><p>書中提到，讚美一個人的「能力」，與讚美一個人的「努力」會得到不一樣的效果。實驗結果顯示，讚美一個人的能力，會讓他認知到自己的成功來自於自身的能力，所以需要繼續保有這樣的能力，但是這樣使學生進入定型心態，後續即便測試同樣難度的題目，表現就不如前次好。如果讚美一個人的努力，會讓他認知到這次的成功是來自自身的努力，所以他們也更願意努力更富挑戰性的任務。</p>
<p>讚美一個人的能力，使他們拒絕了富挑戰性的任務，他們儘可能的不想暴露缺點、使才能遭受懷疑。這使得他們更願意嘗試作弊、撒謊來確保他們的成功。對那些被讚美很努力的學生來說，困難意味著需要更多的努力，但是失敗並不意味著他們的不聰慧，他們更能享受挑戰所帶來的樂趣。</p>
<h2 id="負標籤"><a href="#負標籤" class="headerlink" title="負標籤"></a>負標籤</h2><p>負標籤也會在這個地方發揮作用。作者陳述到自身高中的數學成績非常好，但後來遇上了一個認為女性就是不擅長數學的老師，接著，作者的數學成績就下滑了。在刻版印象中認為女性是不擅長數學與科學的，這樣的負標籤會讓人們產生種種分心的想法，即便是一名女性與多名男性在同一間教室，都會讓女性的數學成績下滑。但這並不是對所有人都這樣，他只對定型心態者有影響，當人們認知到素質是不變的，那麼刻版印象就會發生作用。在定型心態下，好評跟負標籤都會發生作用，但是在成長心態下，卻可以不受影響，他們不相信永遠的低劣，他們相信只要經過努力便可以迎頭趕上。</p>
<p>這邊大致摘要了書中前三分之一的內容，如果大家有興趣，請取找書來看吧！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/mindset/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/about/">
                            About me
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:38:00+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
</div>

            </div>
            
                <div class="postShorten-content">
                    <h2 id="杜岳華"><a href="#杜岳華" class="headerlink" title="杜岳華"></a>杜岳華</h2><p>Yueh-Hua Tu</p>
<p>兼俱科學家與工程師思維</p>
<p>熱愛數學、科學、生物學、電腦科學</p>
<p>專業是Computational Biology, Systems Biology, Network Biology</p>
<p>我要成為生醫資料科學家！</p>
<h3 id="學歷："><a href="#學歷：" class="headerlink" title="學歷："></a>學歷：</h3><ul>
<li>國立陽明大學 生物醫學資訊所 碩士 第1名畢業</li>
<li>國立成功大學 醫學檢驗生物技術學系 學士</li>
<li>國立成功大學 資訊工程學系 學士</li>
</ul>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/about/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/why-rnn/">
                            RNN 的意義
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:21:30+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>今天又是個長長的故事。</p>
<p>上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。</p>
<p>這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。</p>
<p>在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。</p>
<h2 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h2><p>馬可夫模型是這樣的，他假設一個變數有不同種的狀態，例如下圖：</p>
<p><br></p>
<p><img src="/images/markov_model.svg" alt=""></p>
<p><br></p>
<p>在這邊有4個狀態，一個圓圈代表一個狀態，狀態跟狀態之間會隨著時間改變，每個狀態會有一定機率變成其他狀態，或是維持原本的狀態不變。</p>
<p>我們可以把目前的狀態用一個向量來表達：</p>
<p>$$<br>\mathbf{y} =<br>\begin{bmatrix}<br>y_1 \\<br>y_2 \\<br>y_3 \\<br>y_4 \\<br>\end{bmatrix}<br>$$</p>
<p>我們這邊用 $\mathbf{y}$ 代表他是可以被觀察到的。狀態變化我們可以用一個矩陣來表達：</p>
<p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p>
<p>其中 $a_{ij}$ 代表的是由狀態 $j$ 變成狀態 $i$ 的機率，這邊要注意的是，每一個列（row）的機率總和要是 1。</p>
<p>所以不同時間點的狀態變化關係可以寫成以下式子：</p>
<p>$$<br>\mathbf{y^{(t)}} = A \mathbf{y^{(t-1)}}<br>$$</p>
<p>$\mathbf{y^{(t)}}$ 的意思是第 t 次（或是時間為 t）的狀態，$\mathbf{y^{(t-1)}}$ 狀態會經過一次轉換或是運算轉變成 $\mathbf{y^{(t)}}$。</p>
<p>如果你把其中的第一項的運算拆開來看就會長這樣，可以自行檢驗狀態的變化：</p>
<p>$$<br>y_1^{(t)} = a_{11}y_1^{(t-1)} + a_{12}y_2^{(t-1)} + a_{13}y_3^{(t-1)} + a_{14}y_4^{(t-1)}<br>$$</p>
<p>從時間軸上來看，我們可以把狀態的轉變畫出來像是這樣：</p>
<p><br></p>
<p><img src="/images/markov_model_expand_time.svg" alt=""></p>
<p><br></p>
<p>每次的轉變我們都可以看成一個函數 $f$，他其實等同於上面提到的矩陣：</p>
<p>$$<br>\mathbf{y^{(t)}} = f(\mathbf{y^{(t-1)}}) = A \mathbf{y^{(t-1)}}<br>$$</p>
<p>所以他的意思是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$，所以這是單純的狀態變化。</p>
<p>上面的矩陣當中其實內容是機率，我們也可以把他轉成機率的寫法，但是解釋會變得不太一樣：</p>
<p>$$<br>p = f(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p>
<p>這邊的解釋是，$\mathbf{y^{(t-1)}}$ 會經由 $f$ 變成 $\mathbf{y^{(t)}}$ <strong>的機率</strong>。</p>
<p>下句跟上句的不同在於，上句的描述是肯定的，他只描述了狀態的改變，但是下句多加描述了 <strong>這件事會發生的機率</strong>，所以應該要把 $\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}}$ 理解成 <strong>這一件事</strong>，那麼 $f$ 的輸出就是機率了。</p>
<p>我們可以把上圖 <em>收起來</em>，所以看起來會像這樣：</p>
<p><br></p>
<p><img src="/images/markov_model_time.svg" alt=""></p>
<p><br></p>
<p>花了點時間把一些符號跟數學概念講完了，來談談他的假設，一般來說，馬可夫模型最大的假設在於：</p>
<p>$$<br>P(\mathbf{y^{(t)}} \mid \mathbf{y^{(1)}}, \mathbf{y^{(2)}}, \dots, \mathbf{y^{(t-1)}}) = P(\mathbf{y^{(t)}} \mid \mathbf{y^{(t-1)}})<br>$$</p>
<p>也就是要預測第 t 單位時間的狀態，我們經歷了第 1~(t - 1) 單位時間，但是他只需要用前一個時間單位的狀態就可以預測下一個狀態，前面很多狀態都是不必要的，這我們稱為一階馬可夫模型（first-order Markov chain）。</p>
<p>當然可以推廣到 m 階馬可夫模型（m th-order Markov chain），那代表需要前 m 個狀態來預測下一個狀態，順帶一提，有零階馬可夫模型，那就跟我們一般的機率分佈模型（$P(\mathbf{y^{(t)}}）$）一樣。</p>
<p>沒有特別提的話，通常大家談的馬可夫模型都是一階馬可夫模型。一般來說，他有個非常重要的特性，就是 <strong>無記憶性</strong>，也就是他不會去記住他所經歷的狀態，他只需要用現在的狀態就可以預測下一個狀態。</p>
<p>不過我要特別提一下這個模型的一些其他假設：</p>
<ul>
<li>狀態是離散的。在馬可夫模型的狀態空間中是離散的，也就是你可以用一個正整數來數出有幾種狀態存在。</li>
<li>時間是離散的。我們剛剛有看到他計算的是第 t 單位時間，下一次就是乘上一個矩陣之後成為第 t+1 單位時間。</li>
<li>狀態是可被觀察的。</li>
<li>以一個隨機變數作為一個狀態。</li>
</ul>
<p>接下來我們來談談另一個模型。</p>
<p><br></p>
<h2 id="Hidden-Markov-model"><a href="#Hidden-Markov-model" class="headerlink" title="Hidden Markov model"></a>Hidden Markov model</h2><p>接下來是進階版的隱馬可夫模型（hidden Markov model），他的假設是這樣的，在一個系統中存在一些我們看不到的狀態，是系統的內在狀態，隨著系統的內在狀態不同，他所表現出來的外在狀態也不同，而外在狀態是我們可以觀測到的。</p>
<p><br></p>
<p><img src="/images/hmm.svg" alt=""></p>
<p><br></p>
<p>大家可以看到這個圖跟剛剛的很相似，帶是又多了一些東西。較大的圈圈是內在狀態，小的圈圈是外在狀態。隨著時間改變，內在狀態會隨著變動，內在狀態的變動我們可以用一個矩陣來表示：</p>
<p>$$<br>A = [a_{ij}] =<br>\begin{bmatrix}<br>a_{11}&amp; a_{12}&amp; a_{13}&amp; a_{14} \\<br>a_{21}&amp; a_{22}&amp; a_{23}&amp; a_{24} \\<br>a_{31}&amp; a_{32}&amp; a_{33}&amp; a_{34} \\<br>a_{41}&amp; a_{42}&amp; a_{43}&amp; a_{44} \\<br>\end{bmatrix}<br>$$</p>
<p>裏面裝的一樣是機率。接下來，不同的內在狀態有不同的機率會噴出（emit）外在狀態，這也會用另一個矩陣表示：</p>
<p>$$<br>B = [b_{ij}] =<br>\begin{bmatrix}<br>b_{11}&amp; b_{12}&amp; b_{13}&amp; b_{14} \\<br>b_{21}&amp; b_{22}&amp; b_{23}&amp; b_{24} \\<br>b_{31}&amp; b_{32}&amp; b_{33}&amp; b_{34} \\<br>\end{bmatrix}<br>$$</p>
<p>寫成狀態轉移的關係式的話會變成：</p>
<p>$$<br>\mathbf{h^{(t)}} = A \mathbf{h^{(t-1)}}<br>$$</p>
<p>$\mathbf{h^{(t)}}$ 代表在第 t 單位時間的內在狀態。</p>
<p>$$<br>\mathbf{y^{(t)}} = B \mathbf{h^{(t)}}<br>$$</p>
<p>$\mathbf{y^{(t)}}$ 代表在第 t 單位時間根據內在狀態噴出的外在狀態。</p>
<p>如果在時間軸上表達的話是這個樣子：</p>
<p><br></p>
<p><img src="/images/hmm_expand_time.svg" alt=""></p>
<p><br></p>
<p><img src="/images/hmm_time.svg" alt=""></p>
<p><br></p>
<p>由於在這邊又多了一個內在狀態，所以在模型的表達力上遠遠超越馬可夫模型。舉個例子好了，假設小明很好奇在不同天氣的時候外面的人吃冰淇淋的狀況是如何，但是小明又很懶得出門看天氣，這時候他就假設天氣（晴天、陰天、雨天）是內在狀態（看不到），然後他觀察路上的人吃冰淇淋（外在狀態，吃、不吃）的多寡，這時候這麼模型就可以派上用場，他藉由持續觀察路人有沒有吃冰淇淋，可以推論外面天氣的變化狀況。</p>
<p>這時候我們也來總結一下，這個模型的假設：</p>
<ul>
<li>內在狀態跟外在狀態都是離散的。</li>
<li>時間是離散的。</li>
<li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li>
<li>以一個隨機變數作為一個狀態。</li>
</ul>
<p><br></p>
<h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p>
<ul>
<li>狀態都是 <strong>連續</strong> 的。</li>
<li>時間是離散的。</li>
<li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li>
<li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li>
<li><strong>允許在每個時間點給輸入</strong></li>
<li><strong>引入非線性</strong></li>
</ul>
<p>首先，在這邊的狀態會以一個向量做表示，大家應該也知道 RNN 的 input 是一個向量，當中的狀態也是一個向量，最後的 output 也是一個向量。而這些向量當中的的值都是連續的 $\mathbb{R}^n$（假設向量大小為 n），不像上面的模型都是離散的 $k$（假設有 k 個狀態），所以在空間上的大小可以說是擴大非常多。</p>
<p>接下來我們來看看時間的狀態轉換：</p>
<p><br></p>
<p><img src="/images/rnn_time.svg" alt=""></p>
<p><br></p>
<p><img src="/images/rnn_expand_time.svg" alt=""></p>
<p><br></p>
<p>在 RNN 中一樣含有內在狀態，但不同的是 RNN 可以在每個時間點上給輸入向量（$\mathbf{x^{(t)}}$），所以可以根據前一個時間點的內在狀態（$\mathbf{h^{(t)}}$）跟輸入向量去計算輸出，或是外在狀態（$\mathbf{y^{(t)}}$）。</p>
<p>所以大家會在一些論文上看到模型的狀態關係式長下面這個樣子：</p>
<p>$$<br>\mathbf{y^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}}) = sigm(W_x\mathbf{x^{(t)}} + W_h\mathbf{h^{(t-1)}} + \mathbf{b})<br>$$</p>
<p>這邊特別引入了非線性的轉換（$sigm$）來讓模型更強大。</p>
<p>隨著從一開始的馬可夫模型到這邊應該對這幾個模型有點感覺，其實 RNN 可以說是很大的突破，在假設上放了很多元素讓模型變得更強大。</p>
<h2 id="Long-short-term-memory"><a href="#Long-short-term-memory" class="headerlink" title="Long short-term memory"></a>Long short-term memory</h2><p>人們為了改進 RNN這個模型的記憶性，希望他可以記住更遠以前的東西，所以設計了 LSTM 來替換他的 hidden layer 的運作模式，後期更有 GRU，還有人說只需要 forget gate 就有很強大的效能的 MGU。這些都是對於記憶性做的改進，個人覺得這些在工程上的貢獻比較大，真正學術上的突破其實還好。</p>
<p>今天的整理就先到這邊啦！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/why-rnn/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/why-cnn/">
                            CNN 的意義
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:19:44+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>前面大致介紹完了 Deep learning 跟 MLP 的設計，我們接下來介紹在影像處理上的重要技術。</p>
<p>Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。</p>
<p>那為什麼 convolution 這麼重要，重要到要放在名稱上呢？</p>
<p>我先推荐大家去看台大李宏毅老師的介紹，看完再繼續往下看文章。</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FrKWiRv254g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<hr>
<h2 id="影像處理中的祕密"><a href="#影像處理中的祕密" class="headerlink" title="影像處理中的祕密"></a>影像處理中的祕密</h2><p>我們希望從資料當中找到某種規律，這種規律我們叫作模式（pattern），模式是會重複出現的特徵，像是你可以辨識出蘋果，因為一顆蘋果有他特定的特徵，像是顏色、形狀、大小等等，當這些組合起來，並且他們都會一起出現而且重複出現，我們就可以稱他為模式。</p>
<p>在影片當中有提到在影像處理上有幾個特點：</p>
<ol>
<li>一些模式比整張圖小</li>
<li>同樣的模式可能出現在圖片的不同地方</li>
<li>縮放圖片不影響圖片中物件的辨識</li>
</ol>
<p>第 1 點講的是，要去辨識一個模式並不需要整張圖，也就是，<strong>local 的資訊比 global 的資訊重要</strong>。在影片中有舉例，一隻鳥的特徵有鳥喙、羽毛、翅膀等等特徵，你並不會去在意他背景的圖片長什麼樣子。鳥喙這樣的特徵他是 <strong>區域性的</strong>，你不需要整張圖片的資訊去判斷這張圖是不是鳥喙，所以在設計模型的原則上需要去擷取區域性的資訊。</p>
<p>第 2 點講的是，同樣的模式可能會出現在不同圖片的不同地方，這邊其實隱含了一個概念，就是 <strong>位移不變性（translation invariance）</strong>。由於同樣模式可以在不同地方上被找到，所以我們只需要一個 node 去偵測他就好了 ，這樣的話可以節省非常多的 node（或是 weight），這稱為 shared weight。</p>
<p>第 3 點，如果圖片縮放不影響圖片辨識，那麼。這時候我們可以做 subsampling，除了可以減少資料處理的量，也不會影響圖片的辨識結果。</p>
<h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>相信很多人並沒有真正理解這個運算到底在做什麼就拿來用了……</p>
<p>我們說到 convolution layer，他真正的作用是用來做什麼的呢？他其實是用來 <strong>擷取 local 的資訊</strong> 的。</p>
<p>承接前面第一點提到的，在圖片當中，pattern 是 local 的資訊而不是 global 的，而 pattern 是我們想抓的資訊，所以我們要的資訊只有 local 的而已。</p>
<p>那麼 convolution 要如何擷取 local 的資訊呢？</p>
<h2 id="Convolution-運算"><a href="#Convolution-運算" class="headerlink" title="Convolution 運算"></a>Convolution 運算</h2><p>我們先來看 convolution 的原始定義，這邊假設兩個函數 $f$、$g$，則兩個函數的 convolution 為：</p>
<p>$$<br>(f * g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d\tau<br>$$</p>
<p>以上我們看到他是一個積分式，當中引入了另一個變數 $\tau$，他代表的是<em>一個時間區間</em>，那接下來是兩個函數的<em>相乘</em>，然後將他們對變數 $\tau$ <em>積分</em>。我們來想想看，先不管變數 $\tau$，相乘後積分的運算跟什麼樣的運算很像？</p>
<p>是的！內積！我們來看看函數的內積長什麼樣子。</p>
<p>$$<br>\lt f, g \gt = \int_{-\infty}^{\infty} f(t) g(t) dt<br>$$</p>
<p>什麼？你跟我說這不是你認識的內積？不不不，你認識的內積其實是這個內積的離散版本。</p>
<p>$$<br>\lt f, g \gt = \sum_{i=1}^{n} f_i g_i<br>$$</p>
<p>$$<br>&lt;a, b&gt; = \sum_{i=1}^{n} a_i b_i = \mathbf{a}^T\mathbf{b}<br>$$</p>
<p>這樣是不是比較清楚一點了？我們來比較一下，因為積分是在 <strong>連續空間的加總</strong>，相對應的加總就是在 <strong>離散空間</strong> 的版本，那麼在連續空間上需要一個 $d\tau$ 來把連續空間切成一片一片的，但是在離散空間上，他很自然的就是 $1$ 了。這樣是不是又發覺他們根本是一樣的呢？</p>
<p>那你知道函數其實是一種向量嗎？不知道的同學表示沒有讀過或是沒有認真讀線性代數。</p>
<p>那這樣大家應該接受了函數的內積以及向量的內積其實是一樣的。接下來我們來討論一下那個神奇的 $\tau$。</p>
<p>$\tau$ 是一個時間區間，而積分其實是在對這個時間區間做切片然後加總，他其實跟我們在做訊號處理上的 window 的概念是一樣的，所以他其實是在某個 window 中做內積的意思。我們先來看看有 window 的內積長什麼樣子。</p>
<p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n + m]<br>$$</p>
<p>在下圖我們可以假想左邊的向量是 $b$，右邊的是 $a$，而向量 $a$ 是有被 window 給限定範圍的（m = 1…k），所以在下面這張圖就是當 n = 1、m = 1…4 的時候的情境。箭頭則是向量元素相乘的對象，每次內積完，n 就會往下移動一個元素。</p>
<p><img src="/images/ccor1d.svg" alt=""></p>
<p>計算完之後就變成一個新的向量，這就是 window 版本的內積的運作原理了！他其實有一個正式的名字，稱為 cross-correlation。</p>
<p>我們來看看把 convolution 離散化來看看是長什麼樣子。剛剛我們看到的 convolution 是連續的版本，是函數的版本，那我們實際上的運算是以向量去操作的，那麼離散版本的 convolution 是：</p>
<p>$$<br>(a * b)[n] = \sum_{m=-\infty}^{\infty} a[m] b[n - m]<br>$$</p>
<p>這邊的 window 就是 $m$ 這個參數，其實我們可以給他一個區間，不要是負無限大到正無限大。</p>
<p>$$<br>(a * b)[n] = \sum_{m=1}^{k} a[m] b[n - m]<br>$$</p>
<p>所以這邊的 window 大小調成是 $k$ 了！</p>
<p><img src="/images/conv1d.svg" alt=""></p>
<p>你會發現，convolution 會跟 cross-correlation 很像，差別在於順序，也就是 convolution 內積的順序是相反的，所以他在數學式上的表達是用相減的，這邊的情境是 n = 6、m = 1…4。</p>
<p>我們來總結一下 convolution 這個運算，他其實是 local 版本的內積運算，而且他的內積方向是反序的。</p>
<h2 id="Convolution-layer"><a href="#Convolution-layer" class="headerlink" title="Convolution layer"></a>Convolution layer</h2><p>這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？</p>
<p>我們先來看二維的 cross-correlation 長什麼樣子。</p>
<p><img src="/images/ccor2d.svg" alt=""></p>
<p>然後是反序的 convolution。</p>
<p><img src="/images/conv2d.svg" alt=""></p>
<p>這樣有沒有搞懂一點 convolution 的運算了呢？</p>
<p>接著，想像在右手邊比較小的方框就是你的 filter （或是 kernel），然後他會沿著兩個軸去移動，去掃描看看有沒有跟 filter 的 pattern 很像的，當他偵測到很像的 pattern 的時候，輸出的 feature map 的值就會很高，所以這樣就可以做到上面講的第二點，也就是位移的不變性。不過說起來也不是真的有什麼位移不變性啦！他只是沿著軸去做掃描可以減少訓練的參數，這樣 filter 還是有在位移阿！只是對於要偵測的 pattern 看起來好像有”位移不變性”一樣。到這邊我們第一點跟第二點都解決了，剩下第三點。</p>
<h2 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h2><p>跟第三點相關的就是 subsampling，也就是如果讓圖片變小，不只可以降低要辨識的區塊大小，還可以降低需要訓練的參數量。那要怎麼讓圖片變小？</p>
<p>Maxpooling 是目前主流的方法，也就是在一個 window 的範圍內去找最大值，只保留最大值。還有一種是 meanpooling，顧名思義，他取整個 window 的平均值作為保留值。</p>
<p>所以 subsampling 在一些應用場景是需要的，有些是不需要的，像是有些 pattern 的辨識是不能去除細節的，一旦去除細節就會造成辨識困難，那就代表他沒有辦法用 subsampling。有時候照片縮小到一定程度人類也會無法辨識當中的圖像是什麼，所以也不要用過頭。</p>
<h2 id="Feature-extractor-classifier-架構"><a href="#Feature-extractor-classifier-架構" class="headerlink" title="Feature extractor-classifier 架構"></a>Feature extractor-classifier 架構</h2><p>經過以上介紹後，我們可以把 convolution layer 跟 subsampling 結合起來，成為所謂的 feature extractor。</p>
<p>經由以上三點特性，這些 layer 的巧妙運用可以是非常棒的 feature extractor。不同種的資料特性需要不同設計的 feature extractor，接下來就是 classifier 上場了。</p>
<p>典型的 classifier 可以是 SVM，或是要用比較潮的 deep learning 也可以，最單純的就是前一篇提到的 MLP 了。</p>
<p>這樣前後組合好就是個 CNN 的雛型了！</p>
<h2 id="MLP-做不好的事情"><a href="#MLP-做不好的事情" class="headerlink" title="MLP 做不好的事情"></a>MLP 做不好的事情</h2><p>前一篇我們有提到 MLP 因為會從整體特徵去做內積，所以整體的模式會優先被考慮，如果有區域性的特徵並不一定會被凸顯出來。在 MLP 相對上會比較注重整體性而不是區域性，所以使用 MLP 在影像處理上就比 CNN 不是那麼有利。</p>
<h2 id="關鍵在哪裡？"><a href="#關鍵在哪裡？" class="headerlink" title="關鍵在哪裡？"></a>關鍵在哪裡？</h2><p>我個人認為關鍵在資料的 <strong>區域性</strong>，也就是你想做的事情其實是跟資料的區域性有關係，或是你的資料是週期性資料（週期性出現的模式也可以視為是一種區域性模式重複出現），這樣你就可以用 convolution layer！（注意！不是 CNN！）</p>
<p>像是音樂當中有週期性的模式就可以用，在生物領域，蛋白質會去辨認 DNA 序列，有被辨認到的部份就會有蛋白質黏附上去，生物學家會想知道到底哪些地方有蛋白質黏附，黏附的序列是區域性的，所以也有應用 CNN 技術在這方面上。</p>
<p>最重要的還是去了解你的資料的特性，然後了解模型當中各元件的性質跟數學特性，這樣才能正確地使用這些技術解決問題，走到這邊需要同時是領域知識的專家，也同時是 DL 的專家才行阿！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/why-cnn/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/why-mlp/">
                            Multi-Layer Preceptron 的意義
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:15:58+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>在前一篇講完了 deep learning 的意義之後我們來更具體一點講 multi-layer perceptron (MLP)。</p>
<p>最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。</p>
<hr>
<p>一般線性模型：$f(\mathbf{x}) = W^{\prime T}\mathbf{x} + b = W^T\mathbf{x}$</p>
<p>Linear MLP：</p>
<p>$f_1(\mathbf{x}) = W_1^T\mathbf{x}$</p>
<p>$f_2(\mathbf{x}) = W_2^Tf_1(\mathbf{x})$</p>
<p>$f_3(\mathbf{x}) = W_3^Tf_2(\mathbf{x})$</p>
<p>…</p>
<p>$f_n(\mathbf{x}) = W_n^Tf_{n-1}(\mathbf{x})$</p>
<hr>
<p>那這樣這個有什麼好講的呢？</p>
<p>大家應該有看到在這邊唯一的運算：內積（inner product）</p>
<h2 id="內積的意義"><a href="#內積的意義" class="headerlink" title="內積的意義"></a>內積的意義</h2><p>有念過線性代數的人應該對內積這個運算還算熟悉（在這邊都假設大家有一定線性代數基礎）。</p>
<p>$$<br>&lt;\mathbf{x}, \mathbf{y}&gt; = \mathbf{x}^T \mathbf{y}<br>$$</p>
<p>$$<br>= \begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}^T</p>
<p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}</p>
<p>=<br>\begin{bmatrix}<br>x_1, x_2, \cdots, x_n<br>\end{bmatrix}</p>
<p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p>
<p>內積，要先定義矩陣相乘的運算，而矩陣的相乘其實是一種線性轉換。</p>
<p>$$<br>f(\mathbf{x}) = A\mathbf{x}<br>$$</p>
<p>我們來觀察一下內積這個運算，這兩個向量會先把相對應的分量相乘。</p>
<p>$$<br>\begin{bmatrix}<br>x_1 \<br>x_2 \<br>\vdots \<br>x_n<br>\end{bmatrix}</p>
<p>\leftrightarrow</p>
<p>\begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_n<br>\end{bmatrix}<br>$$</p>
<p>接著，再相加。</p>
<p>$$<br>x_1y_1 + x_2y_2 + \cdots + x_ny_n<br>$$</p>
<p>這時候我們可以想想看，如果當一邊是權重另一邊是資料的時候所代表的意義是什麼？</p>
<p>當兩個分量的大小都很大的時候，相乘會讓整個值變很大，相對，如果兩個都很接近零的話，結果值就不大。如果很多分量乘積結果都很大，相加會讓整體結果變得很大。</p>
<p>內積，其實隱含了 <strong>相似性</strong> 的概念在裡面，也就是說，如果你的權重跟資料很匹配的話，計算出來的值會很大。大家有沒有從裏面看出些端倪呢？</p>
<p>我們再由另一個角度切入看內積，內積我們可以把他寫成另一種形式，這個應該在大家的高中數學課本當中都有：</p>
<p>$$<br>\mathbf{x}^T \mathbf{y} = ||\mathbf{x}|| ||\mathbf{y}|| cos \theta<br>$$</p>
<p>這時候我們就可以看到內積可以被拆成3個部份：分別是兩個向量的大小跟向量夾角的 $cos \theta$ 值。</p>
<p>而當中 $cos \theta$ 就隱含著相似性在裡頭，也就是說，當兩個向量的夾角愈小，$cos \theta$ 會愈接近 1。相反，如果兩個向量夾角愈接近 180 度，那 $cos \theta$ 會愈接近 -1。剛好呈現 90 度就代表這兩個向量是 <strong>沒有關係的</strong>。</p>
<p>這時候可能有人會說內積又不是完全反應相似性而已，沒錯！因為他也考慮了兩個向量的長度，當一組向量夾角與另一組向量夾角相同，但是第1組的向量長度都比較長，那內積的結果第1組向量就會比較大。</p>
<p>所以內積是沒有去除掉向量長度因素的運算，如果單純想要用向量夾角來當成相似性的度量的話可以考慮用 cos similarity。</p>
<p>$$<br>cos \theta = \frac{\mathbf{x}^T \mathbf{y}}{||\mathbf{x}|| ||\mathbf{y}||}<br>$$</p>
<h2 id="內積與-MLP"><a href="#內積與-MLP" class="headerlink" title="內積與 MLP"></a>內積與 MLP</h2><p>那 MLP 當中內積扮演了什麼樣的角色呢？</p>
<p>在純粹線性的 MLP 當中，多層的 $f(\mathbf{x})$ 疊起來，我們可以把他看做是做非常多次的線性轉換或是座標轉換（change of basis），但是這是在 inference 階段的解釋。</p>
<p>那在 training 階段內積扮演了什麼樣的角色呢？</p>
<p>這邊提供一個新的想法：在 training 的過程中，我們的 dataset 是不變的，會變動的是 weight ，而內積則是在衡量這兩者之間的 feature norm 及向量夾角，所以 weight 會調整成匹配這樣特性的樣子。換句話說，內積考慮了 data 與 weight 之間的相似性與大小，並且藉由 training 去調整 weight 讓他與資料匹配。</p>
<p>在 inference 階段，你就可以把他看成是，weight 正在幫你做出某種程度的篩選，跟 weight 匹配的資料，內積值就會比較大，相對的是，weight 不匹配的資料，內積值就會比較小，藉由這樣將內積結果遞進到下一層的運算。</p>
<h2 id="機率與內積"><a href="#機率與內積" class="headerlink" title="機率與內積"></a>機率與內積</h2><p>其實還有一個觀點，就是機率觀點，機率要求一個 distribution 的長度為 1，$\int_{-\infty}^{\infty} P(X) = 1$。在這邊我們的 distribution 常常以一個 vector（或是 random variable）的形式呈現。事實上就是把一個計算好的向量去除以他的長度。如此一來，我們就去除了長度影響的因素，以符合機率的要求。</p>
<p>那機率當中的內積指的是什麼呢？</p>
<p>你如果動動手 google 一下就會發現在機率當中的內積就是這個運算</p>
<p>$$<br>\mathbb{E}[XY] = \int XY dP<br>$$</p>
<p>如果有念過統計的人，是不是覺得這東西很眼熟呢？</p>
<p>$$<br>cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]<br>$$</p>
<p>是的！他跟共變異數是有相關的，共變異數還是跟我們要去度量兩個隨機變數之間的 <strong>相似性</strong> 有關係。</p>
<p>$$<br>\rho = \frac{cov(X, Y)}{\sigma_X \sigma_Y}<br>$$</p>
<p>只要把他除以隨機變數的標準差就可以得到相關係數了呢！</p>
<h2 id="加入非線性"><a href="#加入非線性" class="headerlink" title="加入非線性"></a>加入非線性</h2><p>事實上，在我們生活中遇到的事物都是非線性的居多，線性模型可以施展手腳的範疇就不大了。</p>
<p>這時我們就希望在 MLP 中加入非線性的元素以增加模型的表達力。這時候模型的每一層就變成了：</p>
<p>$$<br>f(\mathbf{x}) = \sigma (W^T \mathbf{x})<br>$$</p>
<p>而當中的 $\sigma$ 就成了我們的 activation function 了，也就是非線性的來源！</p>
<h2 id="Fully-connected-layer"><a href="#Fully-connected-layer" class="headerlink" title="Fully connected layer"></a>Fully connected layer</h2><p>當這些層的 node 都互相連接，就代表了所有 node 都參與了計算，這個計算所考慮的資料是 <strong>global</strong> 的。</p>
<p>這些層所做的運算是相對 <strong>簡單</strong> 的（相對 convolution 來說）。</p>
<p>每個 node 對每一層運算所做的貢獻是 <strong>弱</strong> 的。當一層的 node 數很多，e.g. 上千個 node，每個 node 的運算結果就會被稀釋掉了。即便內積運算有包含個別值的大小的成份在裡頭，當 node 數一多，這樣的影響也會被減弱，剩下的是整體向量與向量之間的相似性。但有一個情況例外，當有 node 的值極大，e.g. $x_i / x_j = 1000$，當有人是別人的千倍以上的話就要注意一下了，這也是很常在機器學習當中會遇到的問題，這時候就會需要做 normalization 來處理。</p>
<p>最後提醒，內積的運算中雖然有隱含相似性在其中，但是他 <em>不等同</em> 於 <strong>去計算相似性</strong>。</p>
<p>今天的討論就到這邊告一個段落，希望在大家思考 deep learning 模型的時候，這些東西有幫上一些忙。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/why-mlp/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/21/why-deep-learning/">
                            我們為什麼需要 Deep Learning？
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-21T22:04:31+08:00">
	
		    Jul 21, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>目前我們 machine learning 的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。</p>
<p>當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個 performance，像是你給的 feature 不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些 data 當中，那麼模型根本就預測不出來，所以玩過 machine learning 的人就會知道 feature engineering 的重要性。</p>
<p>以往 feature engineering 是需要人自己手動處理的，如今我們也希望由 machine learning 的模型中自動學出來。大家可以看到我們的技術進展：從以往的手寫程式進展到經典的 machine learning 技術，這是一個巨大的飛躍。</p>
<p><img src="/images/diagram-deep-learning.png" alt=""></p>
<blockquote>
<p>From <a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?ie=UTF8&amp;qid=1472485235&amp;sr=8-1&amp;keywords=deep+learning+book" target="_blank" rel="noopener"><em>Deep Learning</em></a> by Ian Goodfellow and Yoshua Bengio and Aaron Courville</p>
</blockquote>
<hr>
<p><br><br></p>
<h2 id="他幫我們解決了什麼問題呢？"><a href="#他幫我們解決了什麼問題呢？" class="headerlink" title="他幫我們解決了什麼問題呢？"></a>他幫我們解決了什麼問題呢？</h2><p>以往的手寫程式需要工程師非常的聰明，他需要知道在 input 與 output 之間的所有規則，然後把這些規則化成可以執行的程式，這些實作的過程需要花非常大量的人力跟腦力。</p>
<p><br></p>
<p><img src="/images/before_ml.svg" alt=""></p>
<p><br></p>
<p>然而，我們進展到 machine learning 的技術，我們試圖去收集一些資料，這些資料符合我們預期的 input 與 output 之間的關係。</p>
<p><br></p>
<p><img src="/images/after_ml.svg" alt=""></p>
<p><br></p>
<p>他可以幫我們將中間的 <strong>過程</strong> 連接起來，我們不需要去 <em>手刻</em> 或是 <em>事先知道</em> 這些過程，更何況自然界很多過程都是 <strong><em>人類沒辦法理解的</em></strong> 或是 <strong><em>還不知道的</em></strong>。</p>
<p><br></p>
<p><img src="/images/mnist.svg" alt=""></p>
<p><br></p>
<p>這些過程在數學家的眼中就稱為 <strong>函數</strong>，對於機器學習專家來說，input 與 output 之間有無限多種函數的可能。哪一種可能才是最符合我們資料的長相的？我們希望挑出最有可能的那一種，就把那就把那一種當成是模型，並且輸出，這樣我們就能讓機器自動去學出 input 與 output 的對應關係，這是一個飛躍性的進展。</p>
<p><br></p>
<h2 id="Feature-engineering"><a href="#Feature-engineering" class="headerlink" title="Feature engineering"></a>Feature engineering</h2><p>接著我們意識到：我們還是需要手動去處理 feature。經典的 machine learning 模型只幫我們處理了 <strong>將 feature 對應到 output 的關係</strong>，我們還是得藉由 feature extraction 的技術來轉換，而我們很難知道什麼樣的feature extraction 才真正能夠把資料中我們想要的資訊萃取出來，這部分就進到 representation Learning 的範疇。</p>
<p><br></p>
<h2 id="Automatic-feature-extraction"><a href="#Automatic-feature-extraction" class="headerlink" title="Automatic feature extraction"></a>Automatic feature extraction</h2><p>在 feature extraction 的過程中，常常我們面對的是高維度的向量，由於我們很難去理解高維度的向量之間的轉換，導致我們在轉換的時候會遇上困難，我們根本不知道需要轉換成什麼樣維度的向量，我們也不知道中間需要什麼樣的轉換函數。在數學領域當中，有相關的領域稱為微分幾何，所以常常我們會討論在數學上的 manifold，representation learning 就是希望連同 feature extraction 以及 modeling 可以一併處理，也就是藉由 modeling 的過程會到回饋（從 gradient descent 等等方法），去引導 feature extraction 的過程，進而去學到 feature-feature 之間轉換的 <strong>模式</strong>。</p>
<p><br></p>
<h2 id="Deep-learning"><a href="#Deep-learning" class="headerlink" title="Deep learning"></a>Deep learning</h2><p>Deep learning 就是一種 representation Learning。他希望 data 在高維度的轉換當中，可以去萃取出足夠而抽象的資訊，去進行預測。而 deep learning 只是將 feature-feature 之間的轉換模式以 layer-layer 之間的轉換實現，而高維的 feature vector 以 layer 的形式呈現。所以越深的網路代表著經過多次的函數處理跟萃取，所萃取的資訊的抽象程度越高，抽象程度越高，就越接近人類所想像的。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/07/21/why-deep-learning/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
          <li class="pagination-prev">
            <a class="btn btn--default btn--small" href="/">
              <i class="fa fa-angle-left text-base icon-mr"></i>
              <span>NEWER POSTS</span>
            </a>
          </li>
        
        
        <li class="pagination-number">page 2 of 2</li>
    </ul>
</div>

</section>


                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>author.bio</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Research assistant, Taiwan CDC</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
