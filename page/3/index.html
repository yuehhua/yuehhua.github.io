
<!DOCTYPE html>
<html lang="zh-tw,en,default">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"Website","@id":"https://yuehhua.github.io","author":{"@type":"Person","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"},"name":"Dream Maker","description":null,"url":"https://yuehhua.github.io","keywords":"machine learning, deep learning, topology"}</script>
    <meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="Dream Maker">
<meta property="og:url" content="https://yuehhua.github.io/page/3/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:locale" content="zh-tw">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dream Maker">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="2">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="2">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="首頁"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首頁</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="分類"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分類</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="標籤"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">標籤</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="所有文章"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">所有文章</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="搜尋"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">搜尋</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="關於"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">關於</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Email">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Email</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="Atom"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Atom</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="2"
                 class="
                        hasCoverMetaIn
                        ">
                <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2019/01/15/compare-clustering-and-embedding/">
                            Compare Clustering and Embedding
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2019-01-15T09:10:33+08:00">
	
		    1月 15, 2019
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <table>
<thead>
<tr>
<th></th>
<th>Clustering</th>
<th>Embedding</th>
</tr>
</thead>
<tbody>
<tr>
<td>Target space</td>
<td>discrete</td>
<td>continuous</td>
</tr>
<tr>
<td>Target dimension</td>
<td>$d$</td>
<td>$\mathbb{R}^d$</td>
</tr>
<tr>
<td>Transformed result can be</td>
<td>composable</td>
<td>correlated</td>
</tr>
<tr>
<td>Assumption</td>
<td>globally static context (dataset)</td>
<td>globally dynamic context (dataset)</td>
</tr>
</tbody>
</table>

                    
                        

                    
                    
                        <p>
                            <a href="/2019/01/15/compare-clustering-and-embedding/#post-footer" class="postShorten-excerpt_link link">
                                留言與分享
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/12/20/rms-and-variance/">
                            方均根、標準差、馬克士威-波茲曼分佈
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-12-20T00:19:39+08:00">
	
		    12月 20, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Physics/">Physics</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>不久前跟好朋友聊天聊到 random walk 的問題，一個醉漢會在一維的空間上隨機往前或往後走一步。</p>
<p>這樣的話，他最後會走到哪裡去呢？</p>
                    
                        <a href="/2018/12/20/rms-and-variance/" class="postShorten-excerpt_link link">
                            繼續閱讀
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/12/15/ning/">
                            紀念 - 風超大的高美濕地
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-12-15T14:26:27+08:00">
	
		    12月 15, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/My-Style/">My Style</a>, <a class="category-link" href="/categories/My-Style/Friends/">Friends</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    Private post
                    
                        <a href="/2018/12/15/ning/" class="postShorten-excerpt_link link">
                            繼續閱讀
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/11/14/31-variational-autoencoder/">
                            31 Variational autoencoder
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-11-14T16:34:28+08:00">
	
		    11月 14, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>在 autoencoder 的模型裏面，會希望以一個 unsupervised 方法來做到特徵萃取的目的。</p>
<p>你也可以說他是一種降維的方法或是有損壓縮的方法。</p>
<p>基本上就是透過一個線性轉換將原來的特徵，映射到比較低維度的特徵空間上。</p>
<p><img src="/images/autoencoder1.svg" alt=""></p>
                    
                        <a href="/2018/11/14/31-variational-autoencoder/" class="postShorten-excerpt_link link">
                            繼續閱讀
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/31/30-conclusions/">
                            30 結語
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-31T00:14:33+08:00">
	
		    10月 31, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。</p>
<p>我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。</p>
<p>這次的系列文章是在基礎之上，讓大家得以理解不同模型之間的來龍去脈以及變化性，這樣才有辦法更進一步進展到深度學習的領域。</p>
<p>完成這次鐵人賽的意義在於讓大家理解模型的來龍去脈，以及為什麼要用什麼樣的數學元件去兜一個模型。</p>
<p>當你遇到問題的時候，不可能會有一個 ready-to-use 的模型等在那邊給你用，對於人工智慧的技術應用，你必須要為自己所面對的問題和狀況自己去量身訂作自己的模型。</p>
<p>是的！你沒看錯，必須要由領域專家去理解自己要的是什麼，然後自己做出專門給這個情境的模型。</p>
<p>當你的情境非常特殊的時候，讓深度學習專家來深入其他知識領域是非常花時間的。如果以領域專家及深度學習專家之間以合作模式進行，那將會花費更高的成本在溝通上，因為人工智慧的技術應用需要對特定領域非常敏感。我個人認為只有領域專家繼續鑽研成為深度學習專家才有辦法徹底解決特定領域的問題。當然這條路非常的漫長，等於是需要一個特定領域的博士，以及深度學習的博士的等級。這些問題，只有當領域專家自己 <strong>理解</strong> 之後，不是只有 <strong>解決</strong> 問題，才能夠算是真正的 <strong>解決</strong> 了。</p>
<p>這系列文獻給擁有機器學習基礎，想繼續晉升到深度學習領域的朋友們。</p>
<p>感謝大家的支持！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/31/30-conclusions/#post-footer" class="postShorten-excerpt_link link">
                                留言與分享
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/29/29-autoregressive-generative-model/">
                            29 Autoregressive generative model
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-29T23:07:42+08:00">
	
		    10月 29, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。</p>
<p>在 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a> 這篇文章以及他的論文當中又在重述了這件事。</p>
<p>他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？</p>
                    
                        <a href="/2018/10/29/29-autoregressive-generative-model/" class="postShorten-excerpt_link link">
                            繼續閱讀
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/28/28-transformer/">
                            28 Transformer
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-28T22:58:09+08:00">
	
		    10月 28, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。</p>
<p>這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。</p>
<p>Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。</p>
<p>（跟變形金剛一樣的名字耶！帥吧！）</p>
                    
                        <a href="/2018/10/28/28-transformer/" class="postShorten-excerpt_link link">
                            繼續閱讀
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/27/27-attention-model/">
                            27 Attention model
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-27T23:10:46+08:00">
	
		    10月 27, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。</p>
<p>Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。</p>
<p>Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。</p>
                    
                        <a href="/2018/10/27/27-attention-model/" class="postShorten-excerpt_link link">
                            繼續閱讀
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/26/26-sequence-to-sequence-model/">
                            26 seq2seq model
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-26T23:40:23+08:00">
	
		    10月 26, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>前面有提到 seq2seq model，我們就從這邊開始。</p>
<p>Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！</p>
<p><img src="/images/seq2seq.svg" alt=""></p>
                    
                        <a href="/2018/10/26/26-sequence-to-sequence-model/" class="postShorten-excerpt_link link">
                            繼續閱讀
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/25/25-death-of-recurrent-model/">
                            25 Recurrent model 之死
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-25T23:21:36+08:00">
	
		    10月 25, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。</p>
<p>不要再用 RNN 為基礎的模型了！！</p>
<p>就是這篇 <a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" target="_blank" rel="noopener">The fall of RNN / LSTM</a></p>
<p>為什麼呢？</p>
<p>基本上裏面提到 vanishing gradient 的問題一直沒有解決以外，還有沒有辦法善用硬體的侷限在。</p>
<p>像這種循序型的模型，模型天生無法平行化運算，所以 GPU 就無用武之地，只能靠 CPU 慢慢跑。</p>
<p>那有什麼解決辦法呢？</p>
<h2 id="Self-attention-model"><a href="#Self-attention-model" class="headerlink" title="Self-attention model"></a>Self-attention model</h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> 這篇文章提出了 Transformer 這個模型，基本上這個模型使用了 self-attention 的機制。</p>
<p>要講這個之前我們要先聊聊 attention model。在 attention model 之前，sequence-to-sequence model 做出了重大的突破。一個具有彈性，可以任意組合的模型誕生了，管你是要生成句子還是怎麼樣。原本是只有 RNN 一個單元一個單元慢慢去對映 X 到 Y，sequence-to-sequence model 將這樣的對應關係解耦，由一個 encoder 負責將 X 的資訊萃取出來，再經由 decoder 將資訊轉換成 Y 輸出。</p>
<p>但是 LSTM 還是沒辦法記憶夠長的，後來 attention model 就誕生了。乾脆就將 encoder 所萃取到的資訊紀錄下來，變成一個，然後再丟到 decoder 去將資訊還原成目標語言，就可以完成機器翻譯了。</p>
<p>但是這種方式還是不脫 recurrent model，那就乾脆做成 self-attention 的機制，也就是這邊的 Transformer，完全摒棄了 recurrent 的限制。</p>
<h2 id="Autoregressive-generative-model"><a href="#Autoregressive-generative-model" class="headerlink" title="Autoregressive generative model"></a>Autoregressive generative model</h2><p>接著是今年6月的文章 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a>，當你的 recurrent model 不必再 recurrent！</p>
<p>也就是將 RNN 的問題又重述了一遍，並且提出大家都漸漸以 autoregressive generative model 來解決這樣的問題。</p>
<p>這篇算這引言，我接下來會開始一一解釋模型。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/25/25-death-of-recurrent-model/#post-footer" class="postShorten-excerpt_link link">
                                留言與分享
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
          <li class="pagination-prev">
            <a class="btn btn--default btn--small" href="/page/2/">
              <i class="fa fa-angle-left text-base icon-mr"></i>
              <span>上一頁</span>
            </a>
          </li>
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/page/4/">
              <span>下一頁</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">第 3 頁 共 10 頁</li>
    </ul>
</div>

</section>


                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2019 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>目標是計算生物學家！</br>Systems Biology, Computational Biology, Machine Learning</br>Julia Taiwan 發起人</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>研發替代役研究助理</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->


    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
