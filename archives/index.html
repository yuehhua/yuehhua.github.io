
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Archives - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{}</script>
    <meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="Dream Maker">
<meta property="og:url" content="https://yuehhua.github.io/archives/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dream Maker">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="2">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="2">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="2"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/11/11-generalized-linear-model/">
                            11 廣義線性模型
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-11T13:16:28+08:00">
	
		    Oct 11, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>我們前面探討了不同的資料型態可以對應不同的迴歸模型。</p>
<p>不覺得每個迴歸模型都有那麼點相似的地方嗎？</p>
<p>線性迴歸：</p>
<p>$$<br>\mathbb{E}[y] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>羅吉斯迴歸：</p>
<p>$$<br>ln(\frac{\mathbb{E}[y]}{1 - \mathbb{E}[y]}) = ln(\frac{p}{1 - p}) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>Poisson 迴歸：</p>
<p>$$<br>ln(\mathbb{E}[y]) = ln(\lambda) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>在右手邊的部份都是一樣的，是一樣的線性組合加上一個常數。</p>
<p>差別在於預測出來的數值是怎麼連結到目標變量的平均值上 $\mathbb{E}[y]$。</p>
<p>是的，我們在預測的都是目標變量的平均值。</p>
<h2 id="鏈結函數（link-function）"><a href="#鏈結函數（link-function）" class="headerlink" title="鏈結函數（link function）"></a>鏈結函數（link function）</h2><p>要連結目標變量的平均值 $\mathbb{E}[y]$ 跟線性組合加上一個常數…..，姑且叫他 $\eta$ 好了。</p>
<p>$$<br>\mathbb{E}[y] \leftrightarrow \eta<br>$$</p>
<p>統計學家發展出使用鏈結函數來連結這兩者，所以不同的資料型態會對應不同的鏈結函數。</p>
<p>線性迴歸使用 identity function $y = x$：</p>
<p>$$<br>\mathbb{E}[y] = \eta<br>$$</p>
<p>羅吉斯迴歸使用 logit function $y = ln(\frac{x}{1 - x})$：</p>
<p>$$<br>ln(\frac{\mathbb{E}[y]}{1 - \mathbb{E}[y]}) = \eta<br>$$</p>
<p>Poisson 迴歸使用 log function $y = ln(x)$：</p>
<p>$$<br>ln(\mathbb{E}[y]) = \eta<br>$$</p>
<h2 id="廣義線性模型（generalized-linear-model）"><a href="#廣義線性模型（generalized-linear-model）" class="headerlink" title="廣義線性模型（generalized linear model）"></a>廣義線性模型（generalized linear model）</h2><p>這麼一來我們就可以把三個模型搓一搓做成 <del>撒尿牛丸</del> 廣義線性模型啦！</p>
<p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y] \leftrightarrow \eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>對應不同的目標變量，我們就有了萬用的模型，就像物理的大一統理論一樣。</p>
<p>廣義線性模型其實包含了三個部份：</p>
<ol>
<li>鏈結函數</li>
<li>線性預測子</li>
<li>指數族</li>
</ol>
<h2 id="線性預測子（linear-predictor）"><a href="#線性預測子（linear-predictor）" class="headerlink" title="線性預測子（linear predictor）"></a>線性預測子（linear predictor）</h2><p>統計學家特別給了一個線性預測子這樣的名字。</p>
<p>$$<br>\eta = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>這代表要從預測變量 $\mathbf{x}$ 去預測我們的目標變量，其中 $\mathbf{x}$ 的變數之間都是 <strong>互相獨立</strong> 的。</p>
<p>互相獨立的變數之間，要以 <strong>線性組合</strong> 來預測我們的目標變量。</p>
<h2 id="指數族（exponential-family）"><a href="#指數族（exponential-family）" class="headerlink" title="指數族（exponential family）"></a>指數族（exponential family）</h2><p>可是每一種資料的機率分佈都可以接上廣義線性模型嗎？答案是否定的。</p>
<p>$$<br>y \overset{f}{\longleftrightarrow} \mathbb{E}[y]<br>$$</p>
<p>統計學家研究了一下這個模型，發現只有符合指數族的條件才能夠用。</p>
<p>指數族長成這樣：</p>
<p>$$<br>f(\mathbf{y} \mid \boldsymbol{\theta}) = \frac{1}{Z(\boldsymbol{\theta})} h(\mathbf{y}) exp(\boldsymbol{\theta}^T \phi(\mathbf{y}))<br>$$</p>
<p>$\boldsymbol{\theta}$ 是機率分佈的期望值，或是稱為 natural parameter。</p>
<p>$\phi(\mathbf{y})$ 是 sufficient statisitcs，這邊有非常多有趣的東西，不過也有點理論。</p>
<p>$Z(\boldsymbol{\theta})$ 稱為 partition function，是機率分佈的分母，常常會在不同的領域見到他，像是物理。</p>
<p>$h(\mathbf{y})$ 就是個縮放因子，沒什麼重要性，常常是 1。</p>
<p>我知道大家可能會有很多疑問，但是礙於篇幅，我就不再繼續介紹下去了，這邊下去又是統計所一門課了。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/11/11-generalized-linear-model/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/10/10-from-linear-regression-to-poisson-regression/">
                            10 從線性迴歸到 Poisson 迴歸
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-10T22:56:34+08:00">
	
		    Oct 10, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>上次我們講完了線性迴歸跟羅吉斯迴歸的差異。</p>
<p>可是並不是每一種資料都是連續型的或是類別型的。</p>
<p>這次要來介紹 Poisson 迴歸，當你要預測的是計數型資料（count data）就可以用他。</p>
<h2 id="Poisson-distribution"><a href="#Poisson-distribution" class="headerlink" title="Poisson distribution"></a>Poisson distribution</h2><p>在講 Poisson 迴歸之前要先來講講 Poisson 分佈，他的公式大概是長的像這樣：</p>
<p>$$<br>P(Y=y; \lambda) = \frac{e^{- \lambda} \lambda^y}{y!}<br>$$</p>
<p>圖形的話看起來是這樣。</p>
<p><img src="/images/Poisson_distribution.svg" alt=""></p>
<blockquote>
<p>圖片來自維基百科</p>
</blockquote>
<p>要怎麼看懂這個分佈呢？</p>
<p>我們先想像一個情境好了，假設我們經營一家便利商店，在一天之中來光臨這家商店的人數不同時段不一樣。即使是同一個時段，你也很難準確預測會有多少人進到店裡來。這時候我們就會用機率的描述方式，在這邊 k 指的是當我們觀察每段時間區間內進入店裡的客人數量，那麼 $\lambda$ 就是平均來說，每個時間區間的來客人數。你可以看到在 $\lambda = 1$ 的分佈上，來客人數是 0 或是 1 的機率其實很高，但是大於 1 的情形並不是沒有，只是機率比較低罷了。</p>
<p>因此，我們可以用這樣的分佈來估算計數型的資料</p>
<h2 id="Poisson-迴歸"><a href="#Poisson-迴歸" class="headerlink" title="Poisson 迴歸"></a>Poisson 迴歸</h2><p>計數型的資料難道不能用一般的線性迴歸嗎？</p>
<p>其實這兩者有非常大的差別：</p>
<ol>
<li>計數型的資料不會有負值</li>
<li>計數型的資料不會有小數點</li>
</ol>
<p>基於以上兩點資料性質上的差異，我們必須把不同資料分別看待。</p>
<p>但是也不是完全不能用線性迴歸，只是需要動點手腳，就是對資料取 log。</p>
<p>如果你對上面的 Poisson 分佈取 log 的話會發生什麼事呢？</p>
<p>$$<br>ln(P(Y=y; \lambda)) = - \lambda + y ln(\lambda) - \sum_{y}^{j=1} j<br>$$</p>
<p>看到了吧！$\lambda$ 跳出來了！而平均數 $\lambda$ 是連續型的數值，可以作為線性迴歸要預測的對象的。</p>
<blockquote>
<p>注意：以上並非正式的證明，請勿用於正式推導</p>
</blockquote>
<p>其實我們的 Poisson 迴歸是長成這樣的：</p>
<p>$$<br>ln(\mathbb{E}[y]) = ln(\mathbb{E}[P(Y=y; \lambda)]) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>是不是跟我們前面提到的羅吉斯迴歸有 87% 像呢？</p>
<p>而且我們在上面有提到 $\lambda$ 是平均數，所以呢…</p>
<p>$$<br>ln(\mathbb{E}[y]) = ln(\lambda) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>Poisson 迴歸在預測的根本是 $ln(\lambda)$ 嘛！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/10/10-from-linear-regression-to-poisson-regression/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/09/09-from-linear-regression-to-logistic-regression/">
                            09 從線性迴歸到羅吉斯迴歸
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-09T22:59:32+08:00">
	
		    Oct 09, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。</p>
<p>現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。</p>
<h2 id="常態分佈"><a href="#常態分佈" class="headerlink" title="常態分佈"></a>常態分佈</h2><p>其實我們在前面的文章中有提到我們的線性迴歸模型假設了誤差是會呈現一個常態分佈。</p>
<p>對於誤差的假設這件事情其實很大程度影響了我們模型的長相，也會影響這個模型所適用的資料。</p>
<p>像是線性迴歸模型適用的是連續型變數的資料，更進一步來說，他是要求他的應變量（y）要是連續型的，看起來是這樣的。</p>
<p>$$<br>\mathbb{E}[y] = \mathbb{E}[Normal(Y=y; \mu, \sigma)] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>$Normal(Y=y; \mu, \sigma)$：代表常態分佈，以 $\mu$ 為平均數，$\sigma$ 為標準差，輸入是應變量（y），輸出是機率。</p>
<p>事實上，當你把你的一些 feature 輸入這個模型後，預測出來的是應變量的期望值或是平均數（$\mathbb{E}[y]$）。</p>
<p>如果我們要預測的應變量是二元的類別資料，那也就是把問題轉換成分類問題，我們就需要用不同的分佈。</p>
<h2 id="白努力分佈與二項式分佈"><a href="#白努力分佈與二項式分佈" class="headerlink" title="白努力分佈與二項式分佈"></a>白努力分佈與二項式分佈</h2><p>當我們的資料是分成兩類：0, 1，那麼他會對應到白努力分佈（Bernoulli distribution）。</p>
<p>當你蒐集了一群二元分類的資料，那麼就會對應到二項式分佈（Binomial distribution）。</p>
<p>如果你要預測的是一筆二元分類的資料，你應該要假設你的應變量（y）是白努力分佈。</p>
<p>$$<br>\mathbb{E}[y] = \mathbb{E}[Ber(Y=y; p)] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>$Ber(Y=y; p)$：代表白努力分佈，以 p 為成功機率，輸入是應變量（y），輸出是機率。</p>
<h2 id="羅吉斯迴歸"><a href="#羅吉斯迴歸" class="headerlink" title="羅吉斯迴歸"></a>羅吉斯迴歸</h2><p>羅吉斯迴歸的模型是以下這個樣子：</p>
<p>$$<br>y = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p>
<p>其中的 $\sigma$ 是 sigmoid function，由這個函數輸出的就是成功機率了：</p>
<p>$$<br>p = \sigma(s) = \frac{1}{1 + e^{-s}}<br>$$</p>
<p>那這跟我們前面提的分佈看起來都不太一樣阿！我們來反推一下，其實我們前面提到的分佈應該是跟 sigmoid function 的反函數有關係：</p>
<p>$$<br>\sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>我們從白努力分佈開始，當 $y \in {0, 1}$ 時：</p>
<p>$$<br>Ber(Y=y; p) = p^y (1-p)^{(1-y)}<br>$$</p>
<p>也就是，當 y 是 0 的時候，只會剩下後面那一項，當 y 是 1 的時候，只會剩下前面那一項。</p>
<p>那們我們要預測的其實是一個事件發生的機率 p，可是我們由 $\mathbf{w}^T\mathbf{x} + b$ 計算出來的是連續的數值。那要怎麼將這兩者接起來呢？</p>
<p>在統計學裡有一個叫作 logit function 的東西，中文翻譯叫作邏輯函數，但是跟邏輯沒什麼關係就是了。</p>
<p>$$<br>y = log \frac{x}{1 - x}<br>$$</p>
<p>如果將裡頭的 x 想成成功機率的話，分子就是成功的機率，分母就是失敗的機率，就非常像一個東西叫作勝算（odds），所以他又叫作 log odds。</p>
<p>用勝算的話就是一個連續的數值，同時也可以表達成功機率的含意，我們就從這邊開始吧！一路朝著 sigmoid function 邁進。</p>
<p>$$<br>s = ln \frac{p}{1 - p}<br>$$</p>
<p>$$<br>-s = ln \frac{1 - p}{p}<br>$$</p>
<p>$$<br>e^{-s} = \frac{1 - p}{p}<br>$$</p>
<p>$$<br>p + e^{-s}p = 1<br>$$</p>
<p>$$<br>(1 + e^{-s})p = 1<br>$$</p>
<p>$$<br>p = \frac{1}{1 + e^{-s}}<br>$$</p>
<p>看到了嗎？他是 sigmoid function！</p>
<p>也就是說，如果用線性迴歸預測的是 log odds，那就會跟羅吉斯迴歸一樣！</p>
<p>$$<br>ln \frac{p}{1 - p} = \sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>你如果把 sigmoid function 還原回來的話：</p>
<p>$$<br>y = \frac{1}{1 + e^{- \mathbf{w}^T\mathbf{x} - b}} = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p>
<p>就是羅吉斯迴歸的樣子了！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/09/09-from-linear-regression-to-logistic-regression/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/08/08-l2-regularized-linear-model/">
                            08 l2-regularized 線性模型
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-08T23:06:37+08:00">
	
		    Oct 08, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>我們來回顧一下 SVM 模型。</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b, \boldsymbol{\xi}} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N \xi_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1 - \xi_n \\<br>\text{ } &amp;\ \ \ \<br>    \forall n, \xi_n \ge 0<br>\end{align}<br>$$</p>
<p>他可以被進一步轉成</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N max(1 - y_i (\mathbf{w}^T\mathbf{x_i} + b), 0) \\<br>\end{align}<br>$$</p>
<p>在 SVM 的陳述當中，有沒有發現 $\frac{1}{2} \mathbf{w}^T\mathbf{w}$ 這部份看起來跟 $l_2$ regularization 一樣。後半部份是跟誤差有關。</p>
<p>如果分類分對的話，$y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 就會是大於等於 1，這樣的話後面整項計算起來就會是 0。</p>
<p>如果分類分對但是離線太近的話，$y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 就會是小於 1，這樣後面整項會是一個小於 1 的值，代表離線太近了，算是些微的誤差。</p>
<p>如果分類都分錯，$y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 根本就是負值，後面整項會是一個大於 1 的值，代表離線太遠了，算是很大的誤差。</p>
<h2 id="Hinge-error"><a href="#Hinge-error" class="headerlink" title="Hinge error"></a>Hinge error</h2><p>這樣的誤差計算方式稱為 hinge error，也就是：</p>
<p>$$<br>E(y, \hat{y}) = max(1 - y\hat{y}, 0)<br>$$</p>
<h2 id="l-2-regularized-linear-model"><a href="#l-2-regularized-linear-model" class="headerlink" title="$l_2$ -regularized linear model"></a>$l_2$ -regularized linear model</h2><p>所以我們根本可以把模型看成是一個 $l_2$ -regularized 的線性模型。</p>
<p>$$<br>\mathop{\arg\min}_{\mathbf{w}, b} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N E(y, \hat{y})<br>$$</p>
<p>不懂 regulariztion 的朋友請左轉 <a href="https://ithelp.ithome.com.tw/articles/10186405" target="_blank" rel="noopener">我之前寫過的鐵人文章</a>。</p>
<p>基本上，你想把誤差函數換成其他的東西都可以，像是變成 kernel $l_2$ -regularized linear regression (kernel ridge regression)。</p>
<p>$$<br>y = \mathbf{w}^T\phi(\mathbf{x}) + b \\<br>E(y, \hat{y}) = \frac{1}{N} (y - \hat{y})^2 \\<br>\mathop{\arg\min}_{\mathbf{w}, b} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N E(y, \hat{y})<br>$$</p>
<p>或是你想要 kernel $l_2$ -regularized logistic regression。</p>
<p>$$<br>y = \mathbf{w}^T\phi(\mathbf{x}) + b \\<br>E(y, \hat{y}) = \frac{1}{N} log(1 + exp(-y\hat{y})) \\<br>\mathop{\arg\min}_{\mathbf{w}, b} \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N E(y, \hat{y})<br>$$</p>
<h2 id="Representer-theorem"><a href="#Representer-theorem" class="headerlink" title="Representer theorem"></a>Representer theorem</h2>
                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/08/08-l2-regularized-linear-model/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/07/07-standard-svm/">
                            07 標準 SVM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-07T17:20:54+08:00">
	
		    Oct 07, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>雖然標題是說”標準” SVM，不過模型這種東西從來就沒有什麼標準，有的不過是變體。</p>
<p>所以這篇是要跟大家總結一下我們一般在用的 SVM 模型的預設值是長什麼樣子。</p>
<p>我們前面從 maximum-margin classifier 出發，尋找最大 margin 的分類器。</p>
<p>接著，為了解決非線性跟計算效能問題，引進了 kernel trick。</p>
<p>最後，為了避免 overfitting，我們放寬了 margin 成為 soft-margin。</p>
<p>這些都是標準 SVM 的預設配備。</p>
<p>當然還有為了解決計算效能問題，使用了 Lagrange multiplier，由於背後的數學太過煩雜就不介紹了。</p>
<p>簡單來說，Lagrange multiplier 的使用，讓原本的問題（primal problem）可以有另外的對偶問題（dual problem）。</p>
<p>我們只要解了對偶問題，我們就可以解掉原本的問題，雖然有時候會碰到一些限制。</p>
<p>原本問題的陳述是這樣的：</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b, \boldsymbol{\xi}} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N \xi_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1 - \xi_n \\<br>\text{ } &amp;\ \ \ \<br>    \forall n, \xi_n \ge 0<br>\end{align}<br>$$</p>
<p>轉變成對偶問題然後簡化後的陳述是這樣的：</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\boldsymbol{\alpha}} &amp;\ \ \ \<br>    \frac{1}{2} \sum _{n=1}^{N} \sum _{m=1}^{N} \alpha_n \alpha_m y_n y_m \mathbf{z_n}^T\mathbf{z_m} - \sum _{n=1}^N \alpha_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \sum_{n=1}^{N} \alpha_n y_n = 0, 0 \le \alpha_n \le C \\<br>\text{ } &amp;\ \ \ \<br>    \mathbf{w} = \sum_{n=1}^{N} \alpha_n y_n \mathbf{z_n}<br>\end{align}<br>$$</p>
<p>不要問我怎麼來的，你一問我就要開始另一篇數學了，你會怕。</p>
<p>解完上面的問題，我們可以得到一些資訊，我們可以從 $\alpha_n$ 的數值範圍來得知一個資料點他是不是 support vector。</p>
<p>只要他的值在 $0 &lt; \alpha_n &lt; C$ 這個範圍，他就是 support vector。</p>
<p>我們最後在形成整個 SVM 模型的時候其實只需要這些 support vector 就可以了。</p>
<p>$$<br>SVM(\mathbf{x}) = sign(\sum_{SV} \alpha_n y_n K(\mathbf{x_n}, \mathbf{x}) + b)<br>$$</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/07/07-standard-svm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/06/06-from-hard-margin-svm-to-soft-margin-svm/">
                            06 從 hard-margin SVM 到 soft-margin SVM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-06T21:10:06+08:00">
	
		    Oct 06, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>從前面的 kernel SVM 當中我們已經獲得了很強大的模型，可是他還是會有不足之處，像是當資料有雜訊的時候就容易將每個資料點都個別分開。</p>
<p>有時候我們反而希望模型在面對雜訊上不要那麼敏感，或是不要把每個資料點都分對，這時候怎麼辦呢？</p>
<h2 id="Hard-margin-SVM"><a href="#Hard-margin-SVM" class="headerlink" title="Hard-margin SVM"></a>Hard-margin SVM</h2><p><img src="/images/linear-separable2.svg" alt=""></p>
<p>從這張圖來看，我們或許可以接受這樣的線其實還不錯，只是資料多了一點雜訊。如果讓模型硬要把所有資料點都分對的話，邊界就會非常複雜，就會變成 overfitting。</p>
<p>我們是不是有什麼辦法去修正這個模型呢？</p>
<p>我們原本的模型是：</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>\end{align}<br>$$</p>
<p>我們可以藉由在最佳化目標上加上一些 regularization $\sum _{n=1}^N [y_n \ne sign(\mathbf{w}^T\mathbf{z_n} + b) ]$。</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N [y_n \ne sign(\mathbf{w}^T\mathbf{z_n} + b) ] \\<br>\text{subject to} &amp;\ \ \ \<br>    …<br>\end{align}<br>$$</p>
<p>這樣的 regularization 項我們可以仿照之前的方法，把他改成 $\sum _{n=1}^N y_n (\mathbf{w}^T\mathbf{z_n} + b)$。</p>
<p>如果考慮 $y_n (\mathbf{w}^T\mathbf{z_n} + b)$，如果答對的話，他會是大於等於 1 的值，相反，如果答錯的話………..，我們只能確定他是負值，沒辦法確定他的範圍。</p>
<p><img src="/images/soft-margin_SVM.svg" alt=""></p>
<p>這樣的話，我們 直接引入一個值 $\xi$ 來代表 $y_n (\mathbf{w}^T\mathbf{z_n} + b)$ 到底有多大程度答錯了。</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b, \mathbf{\xi}} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} + C \sum _{n=1}^N \xi_n \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1 - \xi_n \\<br>\text{ } &amp;\ \ \ \<br>    \forall n, \xi_n \ge 0<br>\end{align}<br>$$</p>
<p>所以在最佳化目標上有 regularization，然後放寬一下限制，讓 $y_i (\mathbf{w}^T\mathbf{x_i} + b)$ 的值可以有個容忍度 $1 - \xi_n$。</p>
<p>也就是，我們可以在 margin 儘量大的情形下，去限制讓模型不能做錯太多，這個程度可以藉由 C 做調整。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/06/06-from-hard-margin-svm-to-soft-margin-svm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/05/05-from-max-margin-classifier-to-kernel-svm/">
                            05 從 maximum-margin classifier 到 kernel SVM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-05T23:07:15+08:00">
	
		    Oct 05, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>注意：整篇文章極度數學高能！！</p>
<p>沒有把前一篇文章看完的朋友別擔心，我們會在開頭先回顧一下。在一番數學技巧的替換過後，我們的 maximum-margin classifier 會被化成一個最佳化問題。這個最佳化問題可以用二次規劃（quadratic programming, QP）來解。</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>\end{align}<br>$$</p>
<p>當模型被轉化成一個最佳化問題之後，<del>你還有什麼不滿的？</del> 問題都解決了嗎？</p>
<p>當然還沒有阿！雖然說我們解決了前面的第二個缺點，但是第一個缺點還是在啊！</p>
<p>那如果資料不是線性可分的話，是不是支援非線性的分類問題就可以了？</p>
<h2 id="非線性轉換"><a href="#非線性轉換" class="headerlink" title="非線性轉換"></a>非線性轉換</h2><p>那問題簡單！既然這個分類器只能處理線性問題，那就把所有非線性問題透過一個轉換變成線性問題不就可以了？</p>
<p>當然用講的比較簡單…</p>
<p>我們就先假設有個非線性轉換，可以把原本的非線性資料 $\mathbf{x_i}$ 轉成線性資料 $\mathbf{z_i}$：</p>
<p>$$<br>\mathbf{z_i} = \phi(\mathbf{x_i})<br>$$</p>
<p>然後再把線性資料塞進模型裡不就得了？</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{z_i} + b) \ge 1<br>\end{align}<br>$$</p>
<p>嗯……可是瑞凡，你知道要放到 QP 裏面算之前，要先計算出 $[\mathbf{z_i}^T\mathbf{z_j}]$，也就是把資料經過一個非線性轉換過後，還需要將每個資料向量兩兩之間內積，得到一整個矩陣才行。</p>
<p>計算這所有的組合可是非常花時間（$O(N^2)$）的好嗎！！尤其你的資料點很多的時候，大數據都算不下去了。</p>
<h2 id="Kernel-method"><a href="#Kernel-method" class="headerlink" title="Kernel method"></a>Kernel method</h2><p>我們先來看一下比較簡單的非線性轉換好了，假設是二次多項式的轉換。</p>
<p>我們想像中的二次多項式應該是 $\phi_2(x) = (1, x, x^2)$，這邊有三項，可是廣義的二次多項式會有：</p>
<p>$$<br>\mathbf{z} = \phi_2(\mathbf{x}) = (1, x_1, x_2, \dots, x_d, \<br>    x_1^2, x_1x_2, \dots, x_1x_d, \<br>    x_2x_1, x_2^2, \dots, x_2x_d, \<br>    \dots, x_d^2)<br>$$</p>
<p>有一個常數項、d 個一次項跟 $d + \frac{1}{2} d(d+1)$ 個二次項，如果這個向量內積的話，乘法的時間複雜度就有 $O(d^2)$ 了。</p>
<p>然後再讓 $\mathbf{z_i}$ 跟 $\mathbf{z_j}$ 兩兩內積，利用內積的交換性扣掉一些不用算的，好歹也需要算 $N + \frac{1}{2} N(N+1)$ 次內積，是 $O(N^2)$。</p>
<p>總共應該會有 $O(d^2N^2)$。</p>
<blockquote>
<p>註：d 為資料維度，N 為資料筆數</p>
</blockquote>
<p>有沒有什麼方法可以降低這個時間呢？</p>
<p>我們把內積的部份拿出來看看。</p>
<p>$$<br>\phi_2(\mathbf{x})^T\phi_2(\mathbf{x’}) = 1 + \sum_{i=1}^{d} x_ix_i’ + \sum_{i=1}^{d} \sum_{j=1}^{d} x_ix_jx_i’x_j’<br>$$</p>
<p>咦！是不是可以進一步整理成這樣。</p>
<p>$$<br>\phi_2(\mathbf{x})^T\phi_2(\mathbf{x’}) = 1 + \sum_{i=1}^{d} x_ix_i’ + \sum_{i=1}^{d} x_ix_i’ \sum_{j=1}^{d} x_jx_j’ \\<br>= 1 + \mathbf{x}^T\mathbf{x’} + (\mathbf{x}^T\mathbf{x’})(\mathbf{x}^T\mathbf{x’}) \\<br>= 1 + \mathbf{x}^T\mathbf{x’} + (\mathbf{x}^T\mathbf{x’})^2<br>$$</p>
<p>這樣的話需要多少時間複雜度呢？掐指一算，$\mathbf{x}^T\mathbf{x’}$ 需要 $O(d)$，$\phi_2(\mathbf{x})^T\phi_2(\mathbf{x’})$ 總共需要 $O(d) + 1$！</p>
<p>大大降低了！！</p>
<p>所以我們就把這樣的方法稱為 kernel method，並且定義 $K_\phi(\mathbf{x}, \mathbf{x’}) = \phi(\mathbf{x})^T\phi(\mathbf{x’})$。</p>
<p>不過大家常用的是比較廣義的版本：</p>
<p>$$<br>K_2(\mathbf{x}, \mathbf{x’}) = 1 + 2 \mathbf{x}^T\mathbf{x’} + (\mathbf{x}^T\mathbf{x’})^2 = (1 + \mathbf{x}^T\mathbf{x’})^2<br>$$</p>
<p>然後使用者通常會想要細緻控制 kernel 的強度，所以會引入一個參數：</p>
<p>$$<br>K_2(\mathbf{x}, \mathbf{x’}) = 1 + 2 \gamma \mathbf{x}^T\mathbf{x’} + \gamma^2 (\mathbf{x}^T\mathbf{x’})^2 = (1 + \gamma \mathbf{x}^T\mathbf{x’})^2<br>$$</p>
<p>我們來看一下用起來的效果如何？</p>
<p><img src="/images/poly2_kernel.svg" alt=""></p>
<blockquote>
<p>picture from coursera, 《機器學習技法》 - 林軒田</p>
</blockquote>
<h2 id="無限維度-kernel"><a href="#無限維度-kernel" class="headerlink" title="無限維度 kernel"></a>無限維度 kernel</h2><p>既然可以有二次的那是不是到 M 次都可以？那有沒有無限次的呢？</p>
<p>有！</p>
<p>他長成這樣：</p>
<p>$$<br>K(x, x’) = exp(-(x - x’)^2)<br>$$</p>
<p>嗯？你不要騙我！你以為擺到次方項就可以變成無限維度？</p>
<p>那就來證明一下啦-~~</p>
<p>$$<br>= exp(-x^2 + 2xx’ - x’^2) \\<br>= exp(-x^2) exp(-x’^2) exp(2xx’) \\<br>= exp(-x^2) exp(-x’^2) (\sum_{i=0}^{\infty} \frac{(2xx’)^i}{i})\text{ (Taylor expansion)}<br>$$</p>
<p>透過泰勒展開式展開之後我們就看到無限維度的影子了，再進一步簡化。</p>
<p>$$<br>= \sum_{i=0}^{\infty} \big ( exp(-x^2) exp(-x’^2) \frac{2^i}{i} x^i x’^i) \big ) \\<br>= \sum_{i=0}^{\infty} \big ( exp(-x^2) exp(-x’^2) \frac{\sqrt{2^i}}{i} \frac{\sqrt{2^i}}{i} x^i x’^i) \big ) \\<br>= \sum_{i=0}^{\infty} \big ( \frac{\sqrt{2^i}}{i} x^i exp(-x^2) \big ) \sum_{i=0}^{\infty} \big ( \frac{\sqrt{2^i}}{i} x’^i exp(-x’^2)) \big ) \\<br>= \phi(x)^T\phi(x’)<br>$$</p>
<p>我們的無限維度非線性轉換就完成啦！</p>
<p>$$<br>\phi(x) = exp(-x^2) \cdot \big ( 1, \frac{\sqrt{2}}{1} x, \frac{\sqrt{2^2}}{2} x^2, \dots, \big )<br>$$</p>
<p>所以這種 kernel 叫作 Gaussian kernel，使用上也是有個參數可以調整強度。</p>
<p>$$<br>K(\mathbf{x}, \mathbf{x’}) = exp(- \gamma \left\lVert \mathbf{x} - \mathbf{x’} \right\rVert ^2), \gamma \gt 0<br>$$</p>
<p>在現在的支持向量機（Support vector machine, SVM）預設是會使用 Gaussian kernel 的，這就是這個模型強大的祕密！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/05/05-from-max-margin-classifier-to-kernel-svm/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/04/04-from-perceptron-to-max-margin-classifier/">
                            04 從感知器到 maximum-margin classifier
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-04T23:43:46+08:00">
	
		    Oct 04, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>上次我們完成了感知器的介紹，感知器也有他相對應的學習演算法：perceptron learning algorithm (PLA)。</p>
<p>不過我們今天沒有要講 PLA，我們來講講感知器的缺點。</p>
<p>感知器這個模型有很多的缺點，但是因為他是最簡單的模型，我們就放他一馬（？）。</p>
<p>他的缺點有：</p>
<ol>
<li>只能處理線性可分（linear-separable）的資料</li>
<li>只要能分，大家都是好的分類器（黑貓白貓，能抓老鼠的就是好貓？）</li>
</ol>
<h2 id="線性可分"><a href="#線性可分" class="headerlink" title="線性可分"></a>線性可分</h2><p>我們一項一項來講，什麼是線性可分（linear-separable）？</p>
<p><img src="/images/perceptron4.svg" alt=""></p>
<p>像這樣是線性可分的，可以用一條直線把不同的點分開。</p>
<p><img src="/images/linear-separable1.svg" alt=""></p>
<p>像這樣根本找不到一條線可以將點分開的，不是線性可分。</p>
<p><img src="/images/linear-separable2.svg" alt=""></p>
<p>差一點點也不行！很尷尬，差一點就可以變成線性可分的了！但是他就不是！</p>
<p>所以整體說起來，感知器在使用上限制頗大。</p>
<h2 id="好的分類器？"><a href="#好的分類器？" class="headerlink" title="好的分類器？"></a>好的分類器？</h2><p>接下來，什麼叫作一個好的分類器？</p>
<p><img src="/images/perceptron4.svg" alt=""></p>
<p><img src="/images/perceptron5.svg" alt=""></p>
<p><img src="/images/perceptron6.svg" alt=""></p>
<p>以上三張圖，大家有沒有認為哪一張分的最好？</p>
<p>每個人可能有不同的答案，但是有沒有覺得第一張是比較好的分類器？</p>
<p>為什麼？</p>
<p>資料點都多多少少會有一些誤差，而資料我們都假設他比較相似的會在一起，所以當新的資料點出現的時候，我們通常預期他會在舊的資料點附近。所以說，如果線可以離資料點遠一點比較好，這樣的話就不會不小心分錯。</p>
<p>其實你心裡想的是這個樣子的：</p>
<p><img src="/images/max-margin_classifier.svg" alt=""></p>
<p>我們可以定一個東西叫作 margin，他就是線到最近的資料點的距離，然後希望 margin 可以越大越好。</p>
<h2 id="Maximum-margin-classifier"><a href="#Maximum-margin-classifier" class="headerlink" title="Maximum-margin classifier"></a>Maximum-margin classifier</h2><p>所以我們希望的是一個讓 margin 最大化的分類器，也就是 Maximum-margin classifier。</p>
<p>那麼他要怎麼以數學表達呢？</p>
<p>注意：以下數學高能！！</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\max}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) \\<br>\text{subject to} &amp;\ \ \ \<br>    \text{classify } (\mathbf{x_n}, y_n) \text{ correctly} \\<br>\text{ } &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \text{distance}(\mathbf{x_n}, \mathbf{w}, b)<br>\end{align}<br>$$</p>
<p>總的來說，你希望 margin 越大越好，那就是我們的最佳化目標，但是會帶有一些條件，像是我們需要將點都分對，以及 margin 的定義是線到各個點的距離最短的那個。</p>
<p>上方式子中的 argmax 指的是改變某些變數（$\mathbf{w}, b$），讓後方的式子的值最大。subject to 則是限制式，也就是必須要滿足的條件。</p>
<p>但是這樣的式子無法直接套用數學方法計算，所以我們要進一步將式子公式化。</p>
<h2 id="進一步公式化"><a href="#進一步公式化" class="headerlink" title="進一步公式化"></a>進一步公式化</h2><p>注意：以下極度數學高能！！</p>
<p>談到距離的話，我們要先講講高中數學裡的點到線距離公式：</p>
<p>$$<br>\text{distance}(\mathbf{x_i}, \mathbf{w}, b) = \frac{|ax_i + by_i + c|}{\sqrt{a^2 + b^2}}<br>$$</p>
<p>不記得的話，回去翻翻課本或是 google 一下。可以觀察一下，會發現分子是線的權重向量跟資料點向量的相乘再相加，再加上一個常數項。相乘再相加的動作其實就是兩個向量的 <strong>內積運算</strong>。分母部份是線的權重向量的 <strong>長度</strong>。我們可以把這個二維的公式擴充到 n 維，就變成了：</p>
<p>$$<br>\text{distance}(\mathbf{x_i}, \mathbf{w}, b) = \frac{|\mathbf{w}^T\mathbf{x_i} + b|}{\left\lVert \mathbf{w} \right\rVert}<br>$$</p>
<p>是不是看起來漂亮許多？</p>
<p>接著，如果要把資料點分對，那就表示說，將資料點 $\mathbf{x_i}$ 代入模型中算出來的值會跟真實答案 $y_i$ 在線的同側，也就是正負號是相同的。利用這點，我們發現如果將資料點代入模型中算出來的值 $(\mathbf{w}^T\mathbf{x_i} + b)$ 與真實答案 $y_i$ 相乘的話，必定為正，所以 $y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0$。</p>
<p>整個問題會被進一步變成這樣：</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\max}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0 \\<br>\text{ } &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \frac{|\mathbf{w}^T\mathbf{x_i} + b|}{\left\lVert \mathbf{w} \right\rVert}<br>\end{align}<br>$$</p>
<p>$|\mathbf{w}^T\mathbf{x_i} + b|$ 的絕對值部份不是那麼好處理，我們可以把他替換成 $y_i (\mathbf{w}^T\mathbf{x_i} + b)$，反正他一定恆正。</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\max}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0 \\<br>\text{ } &amp;\ \ \ \<br>    \text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \frac{y_i (\mathbf{w}^T\mathbf{x_i} + b)}{\left\lVert \mathbf{w} \right\rVert}<br>\end{align}<br>$$</p>
<h2 id="Scalling-trick"><a href="#Scalling-trick" class="headerlink" title="Scalling trick"></a>Scalling trick</h2><p>對於以下的部份，我們可以進一步簡化。</p>
<p>$$<br>\text{margin}(\mathbf{w}, b) = \mathop{\min}_{i = 1 \dots n} \frac{y_i (\mathbf{w}^T\mathbf{x_i} + b)}{\left\lVert \mathbf{w} \right\rVert}<br>$$</p>
<p>對於一條直線 $\mathbf{w}^T\mathbf{x_i} + b = 0$ 來說，縮放 c 倍基本上是沒有影響的 $c\mathbf{w}^T\mathbf{x_i} + cb = 0$。所以我們就乾脆將這個值縮放成剛好是 1，像下面這樣：</p>
<p>$$<br>\mathop{\min}_{i = 1 \dots n} y_i (\mathbf{w}^T\mathbf{x_i} + b) = 1<br>$$</p>
<p>這樣有什麼好處呢？如果你把他代回去我們的最佳化目標的話，margin 的大小是不是變成這樣了呢？</p>
<p>$$<br>\text{margin}(\mathbf{w}, b) = \frac{1}{\left\lVert \mathbf{w} \right\rVert}<br>$$</p>
<p>而且我們也省掉了 $y_i (\mathbf{w}^T\mathbf{x_i} + b) \gt 0$ 這個限制，畢竟最小的資料點計算出來至少有 1。整個問題就變成了這樣：</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\max} _{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{\left\lVert \mathbf{w} \right\rVert} \\<br>\text{subject to} &amp;\ \ \ \<br>    \mathop{\min} _{i = 1 \dots n} y_i (\mathbf{w}^T \mathbf{x_i} + b) = 1<br>\end{align}<br>$$</p>
<p>min 還是個不好處理的運算，我們與其求某個資料點代入最小值會是 1，我們不如放寬限制，變成所有資料點帶入公式都會 $\ge 1$。</p>
<p>$$<br>\mathop{\min}_{i = 1 \dots n} y_i (\mathbf{w}^T\mathbf{x_i} + b) = 1<br>\Rightarrow<br>y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>$$</p>
<p>最後，數學家都會有點潔癖，將問題變成他們喜歡的形式。像是求倒數的最大值，不如我們求最小值。要求 $\mathbf{w}$ 的長度太麻煩了，我們把長度中的根號拿掉。再加個二分之一上去。</p>
<p>$$<br>\begin{align}<br>\mathop{\arg\min}_{\mathbf{w}, b} &amp;\ \ \ \<br>    \frac{1}{2} \mathbf{w}^T\mathbf{w} \\<br>\text{subject to} &amp;\ \ \ \<br>    \forall i, y_i (\mathbf{w}^T\mathbf{x_i} + b) \ge 1<br>\end{align}<br>$$</p>
<p>maximum-margin classifier 的最終形式就完成了！這也是支持向量機（support vector machine）的雛型。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/04/04-from-perceptron-to-max-margin-classifier/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/03/03-from-linear-regression-to-perceptron/">
                            03 從線性迴歸到感知器
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-03T23:24:32+08:00">
	
		    Oct 03, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>感知器（perceptron）是在 1957 年就被發明出來的的模型，對電腦的發展或是人工智慧來說都是非常早期的。</p>
<p>感知器模型他是一個二元分類的分類器，他解的是分類問題。相對我們前面的線性迴歸解的是迴歸問題，兩者在問題的定義上有根本性的不一樣，那他們兩個有什麼關聯性呢？</p>
                    
                        <a href="/2018/10/03/03-from-linear-regression-to-perceptron/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/03/02-linear-regression/">
                            02 線性迴歸 -- 迴歸問題中的線性模型
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-03T15:31:54+08:00">
	
		    Oct 03, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>這個模型大概已經被人講過很多次，講到都快要爛掉了XD</p>
<p>其實我自己在兩年前的鐵人賽中也有講過同一個模型，所以我就不用講太多基礎的部份：</p>
                    
                        <a href="/2018/10/03/02-linear-regression/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/archives/page/2/">
              <span>OLDER POSTS</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">page 1 of 4</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>Aim to be a computational biologist!</br>Systems Biology, Computational Biology, Machine Learning</br>Organizer of Julia Taiwan community</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Research assistant, Taiwan CDC</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
