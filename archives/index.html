
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Archives - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{}</script>
    <meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="Dream Maker">
<meta property="og:url" content="https://yuehhua.github.io/archives/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dream Maker">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="2">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="2">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="2"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/31/30-conclusions/">
                            30 結語
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-31T00:14:33+08:00">
	
		    Oct 31, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>原本是算到今天就會發完 30 天的文章了，不過系統似乎把第一天跟第二天的文章判定是第一天的了。</p>
<p>我記得我最早參加鐵人賽的時候，一次接受兩個挑戰，分別寫了 Julia 語言以及基礎的機器學習。</p>
<p>這次的系列文章是在基礎之上，讓大家得以理解不同模型之間的來龍去脈以及變化性，這樣才有辦法更進一步進展到深度學習的領域。</p>
<p>完成這次鐵人賽的意義在於讓大家理解模型的來龍去脈，以及為什麼要用什麼樣的數學元件去兜一個模型。</p>
<p>當你遇到問題的時候，不可能會有一個 ready-to-use 的模型等在那邊給你用，對於人工智慧的技術應用，你必須要為自己所面對的問題和狀況自己去量身訂作自己的模型。</p>
<p>是的！你沒看錯，必須要由領域專家去理解自己要的是什麼，然後自己做出專門給這個情境的模型。</p>
<p>當你的情境非常特殊的時候，讓深度學習專家來深入其他知識領域是非常花時間的。如果以領域專家及深度學習專家之間以合作模式進行，那將會花費更高的成本在溝通上，因為人工智慧的技術應用需要對特定領域非常敏感。我個人認為只有領域專家繼續鑽研成為深度學習專家才有辦法徹底解決特定領域的問題。當然這條路非常的漫長，等於是需要一個特定領域的博士，以及深度學習的博士的等級。這些問題，只有當領域專家自己 <strong>理解</strong> 之後，不是只有 <strong>解決</strong> 問題，才能夠算是真正的 <strong>解決</strong> 了。</p>
<p>這系列文獻給擁有機器學習基礎，想繼續晉升到深度學習領域的朋友們。</p>
<p>感謝大家的支持！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/31/30-conclusions/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/29/29-autoregressive-generative-model/">
                            29 Autoregressive generative model
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-29T23:07:42+08:00">
	
		    Oct 29, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>在前面的 Transformer 的文章中有提到了 auto-regressive 的特質。</p>
<p>在 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a> 這篇文章以及他的論文當中又在重述了這件事。</p>
<p>他們探討了是不是所有 recurrent 模型都可以被換成 auto-regressive 的模型呢？</p>
<p>答案幾乎是肯定的。為什麼說幾乎是肯定的呢？因為他需要滿足一些條件，才能達成訓練上模型的穩定性要求。</p>
<p>（內容產生中…）</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/29/29-autoregressive-generative-model/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/28/28-transformer/">
                            28 Transformer
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-28T22:58:09+08:00">
	
		    Oct 28, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>繼 Attention model 之後，由於 recurrent 架構的特性一直無法善用 GPU 的資源做加速。</p>
<p>這時 Google Brain 團隊就看到別人在用 self-attention 機制，也是基於 recurrent 架構，解決了不少問題，用在閱讀理解、摘要、文字蘊涵（textual entailment）及語句表示的問題上。</p>
<p>Google Brain 團隊就提出了史上第一個不需要依賴任何 recurrent 架構的 self-attention 機制模型，Transformer。</p>
<p>（跟變形金剛一樣的名字耶！帥吧！）</p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>這個架構上延續了 encoder-decoder 的架構，encoder 會將輸入的序列 $(x_1, x_2, …, x_n)$ 轉換成 $\mathbf{z} = (z_1, z_2, …, z_n)$，而 decoder 會將 $\mathbf{z}$ 轉換成 $(y_1, y_2, …, y_m)$，一次轉換一個。在每一步當中，模型都是 auto-regressive 的，也就是說，前一次產生的結果會被當成下一次的輸入進行運算。</p>
<p>整個 Transformer 的架構就是在 encoder 及 decoder 上使用了 stacked self-attention 以及全連接的網路。我們來看圖，在圖的左半邊就是 encoder，右半邊就是 decoder：</p>
<p><img src="/images/transformer1.svg" alt=""></p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder 由 N 個完全一樣的層堆疊（stack）起來（$N = 6$）。每層包含兩個子層，第一個是一個 multi-head self-attention 的機制，第二個是簡單的全連接層網路。每個子層外都包了 residual connection 以及 layer normalization，看起來就像 $LayerNorm(x + Sublayer(x))$。</p>
<p>Residual connection 主要可以將前層的資訊繞過一層，直接與下一層做運算。Layer normalization 有穩定學習效果的作用。</p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder 大致上與 encoder 相同，差別是在用了兩個 multi-head self-attention 的機制，所以總共有3個子層。</p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>在 attention 的機制上，我們在上一篇講過了。這邊我們要進一步探討這個模型用到的 scaled dot product attention。在這邊就是分成 query、key 跟 value 三者，首先要先將 query 跟所有 key 做過一次的內積，並且除以 $\sqrt{d_k}$，然後過一次 softmax 函數。計算到這邊就是權重的部份，最後權重再跟 value 去計算結果。其中 $d_k$ 是 key 向量的維度。公式在這邊：</p>
<p>$$<br>Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p>
<p><img src="/images/transformer2.svg" alt=""></p>
<p>圖的左邊是 scaled dot product attention。為什麼要除以 $\sqrt{d_k}$ 呢？文章中有提到，內積會讓整個結果變很大，會讓梯度變得極小，這會不利於訓練，所以需要除以 $\sqrt{d_k}$。</p>
<p>在圖的右邊，是 multi-head self-attention，核心就是平行計算多個 scaled dot product attention 的結果，並把這些結果都串接起來。有了這樣的機制就可以不只注意一個地方，可以有多個關注點。</p>
<p>在 self-attention 的機制，意味著所有的 query、key 跟 value 都來自於自己。不像之前的 attention 橫跨 encoder 跟 decoder，所以資訊會從雙方而來。</p>
<p>在 Transformer 模型當中，有一個是 encoder-decoder attention layer，然後 encoder 跟 decoder 各有一個 self-attention layer，就是各自的第一個子層。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/28/28-transformer/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/27/27-attention-model/">
                            27 Attention model
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-27T23:10:46+08:00">
	
		    Oct 27, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>繼 Seq2seq model 之後，真正可以做到 end-to-end 翻譯的，很多都是用了 attention model。</p>
<p>Attention model，正確來說，不是指特定的一個模型，他是模型的一個部份或是一種設計，所以有人叫他 attention mechanism。</p>
<p>Attention model 被用在機器翻譯、語句的摘要、語音辨識、影像理解（image caption），算是用途非常廣泛。</p>
<p>有幾篇論文算是開始用這樣的機制：</p>
<ol>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="https://arxiv.org/abs/1509.00685" target="_blank" rel="noopener">A Neural Attention Model for Abstractive Sentence Summarization</a></li>
<li><a href="https://arxiv.org/abs/1509.06664" target="_blank" rel="noopener">Reasoning about Entailment with Neural Attention</a></li>
</ol>
<p>Attention model 在很多模型當中都是做為 encoder-decoder 之間的橋樑，原本的 encoder 跟 decoder 之間是只有一個 vector 來傳遞所有的訊息，但是多了 attention mechanism 就不一樣了。</p>
<p><img src="/images/seq2seq2.svg" alt=""></p>
<p>Attention mechanism 主要可以動態的去抓到 encoder 中傳遞的訊息，並且將這些訊息與 decoder 輸出的前一個訊息互相比對之後，透過線性組合之後輸出。這樣的輸出有什麼效果呢？他可以動態地去找到兩邊最相符的資訊，並且將他重要的部份以權重的方式凸顯出來，所以這部份是做線性組合。</p>
<p><img src="/images/attention.svg" alt=""></p>
<p>我看到一個廣義的描述方法，他是這樣說的，我們可以把 attention model 想成是一個函數，這個函數會吃兩種東西，一種是 query，另一種是 key-value 的資料結構，讓 query 去比對所有的 key 找到吻合的，會透過一個 compatibility function 去計算吻合的程度，並且作為權重，最後將權重與相對應的 value 做內積。在這邊 query、key、value 三者都是向量。</p>
<p>在這邊 query 會是 decoder 的 $z_0$，而 key 就是 encoder 的 $h^1, h^2, …$。每一個 key 都會去跟 query 個別通過 compatibility function 算一次吻合程度 $\alpha_0^1, \alpha_0^2, …$。這些 $\alpha_0^1, \alpha_0^2, …$ 就是權重，會去跟 value $h^1, h^2, …$ 做線性組合。在這邊為了簡單所以讓 value 跟 key 是一樣的，其實可以是不同的東西。計算出來的結果 $c^1$ 就是 context vector，會作為 decoder 的輸入，與 $z_0$ 一起計算出 $z_1$。</p>
<p>這樣就算是完成一輪 attention mechanism 了。下一次再繼續用 $z_1$ 當成 query 進行比對。</p>
<p>如此一來，就可以以動態的方式去產生序列了。Encoder 負責的是將輸入的序列轉成固定大小的向量，decoder 將這樣的向量轉換回序列，而中間需要動態調整的部份就像人的注意力一樣，會去掃視跟比對哪個部份的翻譯是最吻合的，然後將他做一個線性組合的調整。</p>
<p>今天的解析就到這邊啦！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/27/27-attention-model/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/26/26-sequence-to-sequence-model/">
                            26 seq2seq model
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-26T23:40:23+08:00">
	
		    Oct 26, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>前面有提到 seq2seq model，我們就從這邊開始。</p>
<p>Seq2seq model 他採用了 encoder-decoder 架構，這時候就要來點 paper 的圖啦！</p>
<p><img src="/images/seq2seq.svg" alt=""></p>
                    
                        <a href="/2018/10/26/26-sequence-to-sequence-model/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/25/25-death-of-recurrent-model/">
                            25 Recurrent model 之死
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-25T23:21:36+08:00">
	
		    Oct 25, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>當大家正在開心的用著 RNN 跟 LSTM 等等模型之時，就有人跳出來了。</p>
<p>不要再用 RNN 為基礎的模型了！！</p>
<p>就是這篇 <a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" target="_blank" rel="noopener">The fall of RNN / LSTM</a></p>
<p>為什麼呢？</p>
<p>基本上裏面提到 vanishing gradient 的問題一直沒有解決以外，還有沒有辦法善用硬體的侷限在。</p>
<p>像這種循序型的模型，模型天生無法平行化運算，所以 GPU 就無用武之地，只能靠 CPU 慢慢跑。</p>
<p>那有什麼解決辦法呢？</p>
<h2 id="Self-attention-model"><a href="#Self-attention-model" class="headerlink" title="Self-attention model"></a>Self-attention model</h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> 這篇文章提出了 Transformer 這個模型，基本上這個模型使用了 self-attention 的機制。</p>
<p>要講這個之前我們要先聊聊 attention model。在 attention model 之前，sequence-to-sequence model 做出了重大的突破。一個具有彈性，可以任意組合的模型誕生了，管你是要生成句子還是怎麼樣。原本是只有 RNN 一個單元一個單元慢慢去對映 X 到 Y，sequence-to-sequence model 將這樣的對應關係解耦，由一個 encoder 負責將 X 的資訊萃取出來，再經由 decoder 將資訊轉換成 Y 輸出。</p>
<p>但是 LSTM 還是沒辦法記憶夠長的，後來 attention model 就誕生了。乾脆就將 encoder 所萃取到的資訊紀錄下來，變成一個，然後再丟到 decoder 去將資訊還原成目標語言，就可以完成機器翻譯了。</p>
<p>但是這種方式還是不脫 recurrent model，那就乾脆做成 self-attention 的機制，也就是這邊的 Transformer，完全摒棄了 recurrent 的限制。</p>
<h2 id="Autoregressive-generative-model"><a href="#Autoregressive-generative-model" class="headerlink" title="Autoregressive generative model"></a>Autoregressive generative model</h2><p>接著是今年6月的文章 <a href="http://www.offconvex.org/2018/07/27/approximating-recurrent/" target="_blank" rel="noopener">When Recurrent Models Don’t Need to be Recurrent</a>，當你的 recurrent model 不必再 recurrent！</p>
<p>也就是將 RNN 的問題又重述了一遍，並且提出大家都漸漸以 autoregressive generative model 來解決這樣的問題。</p>
<p>這篇算這引言，我接下來會開始一一解釋模型。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/25/25-death-of-recurrent-model/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/25/24-recurrent-neural-network-1/">
                            24 Recurrent neural network
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-25T16:41:15+08:00">
	
		    Oct 25, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>接續上一篇。</p>
<h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p>
<ul>
<li>狀態都是 <strong>連續</strong> 的。</li>
<li>時間是離散的。</li>
<li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li>
<li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li>
<li><strong>允許在每個時間點給輸入</strong></li>
<li><strong>引入非線性</strong></li>
</ul>
                    
                        <a href="/2018/10/25/24-recurrent-neural-network-1/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/23/24-recurrent-neural-network/">
                            24 Recurrent neural network
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-23T21:43:49+08:00">
	
		    Oct 23, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>接續上一篇。</p>
<h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p>
<ul>
<li>狀態都是 <strong>連續</strong> 的。</li>
<li>時間是離散的。</li>
<li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li>
<li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li>
<li><strong>允許在每個時間點給輸入</strong></li>
<li><strong>引入非線性</strong></li>
</ul>
<p>首先，在這邊的狀態會以一個向量做表示，大家應該也知道 RNN 的 input 是一個向量，當中的狀態也是一個向量，最後的 output 也是一個向量。而這些向量當中的的值都是連續的 $\mathbb{R}^n$（假設向量大小為 n），不像上面的模型都是離散的 $k$（假設有 k 個狀態），所以在空間上的大小可以說是擴大非常多。</p>
<p>接下來我們來看看時間的狀態轉換：</p>
<p><br></p>
<p><img src="/images/rnn_time.svg" alt=""></p>
<p><br></p>
<p><img src="/images/rnn_expand_time.svg" alt=""></p>
<p><br></p>
<p>在 RNN 中一樣含有內在狀態，但不同的是 RNN 可以在每個時間點上給輸入向量（$\mathbf{x^{(t)}}$），所以可以根據前一個時間點的內在狀態（$\mathbf{h^{(t)}}$）跟輸入向量去計算輸出，或是外在狀態（$\mathbf{y^{(t)}}$）。</p>
<p>所以大家會在一些論文上看到模型的狀態關係式長下面這個樣子：</p>
<p>$$<br>\mathbf{h^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}}) = \mathbf{x^{(t)}} W_x + \mathbf{h^{(t-1)}} W_h + \mathbf{b}<br>$$</p>
<p>$$<br>\mathbf{y^{(t)}} = g(\mathbf{h^{(t)}}) = sigm(\mathbf{h^{(t)}} W_y)<br>$$</p>
<p>這邊特別引入了非線性的轉換（$sigm$）來讓模型更強大。</p>
<p>隨著從一開始的馬可夫模型到這邊應該對這幾個模型有點感覺，其實 RNN 可以說是很大的突破，在假設上放了很多元素讓模型變得更強大。</p>
<h2 id="Long-short-term-memory"><a href="#Long-short-term-memory" class="headerlink" title="Long short-term memory"></a>Long short-term memory</h2><p>人們為了改進 RNN這個模型的記憶性，希望他可以記住更遠以前的東西，所以設計了 LSTM 來替換他的 hidden layer 的運作模式，後期更有 GRU，還有人說只需要 forget gate 就有很強大的效能的 MGU。這些都是對於記憶性做的改進，個人覺得這些在工程上的貢獻比較大，真正學術上的突破其實還好。</p>
<p>今天的整理就先到這邊啦！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/23/24-recurrent-neural-network/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/23/23-markov-chain-and-hmm/">
                            23 Markov chain 及 HMM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-23T09:21:38+08:00">
	
		    Oct 23, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。</p>
<p>這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。</p>
<p>在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。</p>
                    
                        <a href="/2018/10/23/23-markov-chain-and-hmm/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/22/22-convolutional-encoder-decoder-architecture/">
                            22 Convolutional encoder-decoder 架構
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-22T23:10:35+08:00">
	
		    Oct 22, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>標題這不是一個專有名詞。</p>
<p>在電腦視覺的領域中有幾個有名的問題：</p>
<ol>
<li>影像辨識（Image recognition）</li>
<li>物件辨識（Object detection）</li>
<li>語意分割（Semantic segmentation）</li>
</ol>
                    
                        <a href="/2018/10/22/22-convolutional-encoder-decoder-architecture/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/archives/page/2/">
              <span>OLDER POSTS</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">page 1 of 6</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>Aim to be a computational biologist!</br>Systems Biology, Computational Biology, Machine Learning</br>Organizer of Julia Taiwan community</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Research assistant, Taiwan CDC</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
