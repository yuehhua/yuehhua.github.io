
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Archives: 2018 - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{}</script>
    <meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="Dream Maker">
<meta property="og:url" content="https://yuehhua.github.io/archives/2018/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dream Maker">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="2">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="2">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="2"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/09/12/energy-model-bayesian/">
                            Energy-based model 以及 Bayesian model 的關聯
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-09-12T14:22:48+08:00">
	
		    Sep 12, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>在機器學習領域，我們常常會聽到 Energy-based model。</p>
<p>基本上，他是借了物理的能量觀點來的，在數學上，與物理的公式如出一徹。</p>
<p>我們先來看看他是長成什麼樣子。</p>
<p>在物理統計力學中，如果一個系統的狀態的機率分佈，是由狀態的能量決定，並且狀態能量及系統溫度與狀態之間可以寫成一個函數關係，我們稱為 Boltzmann distribution。</p>
<p>$$<br>\large p_i = \frac{1}{Z} e^{- \epsilon_i / kT}<br>$$</p>
<p>一個系統中，狀態 $i$ 的出現機率 $p_i$ 是狀態能量 $\epsilon_i$ 以及系統溫度 $T$ 的函數，其中 $k$ 為波茲曼常數，$Z$ 為機率分佈的分母，稱為 partition function。</p>
<p>在機器學習中，借了同樣的概念變成了以下式子：</p>
<p>$$<br>\large P(X = x) = \frac{1}{Z} e^{f(x)}<br>$$</p>
<p>主要是將次方項 $- \epsilon_i / kT$ 替換成了更廣義的函數 $f(x)$ 形式，而這個函數會跟系統的狀態 $x$ 相關。</p>
<p>其中 $Z$ 就變成了以下的樣子：</p>
<p>$$<br>Z = \sum_{x \in X} e^{f(x)}<br>$$</p>
<p>如果代入以上的函數中就會是：</p>
<p>$$<br>\large P(X = x) = \frac{e^{f(x)}}{\sum_{x \in X} e^{f(x)}}<br>$$</p>
<p>當我們把這樣的模型廣義化之後就稱為 Energy-based model。</p>
<p>這樣的模型在統計或是機器學習中有非常有趣的連結，$f(x)$ 所代表的應該是一個能量的函數。</p>
<p>$$<br>f(x) = -E(x)<br>$$</p>
<p>然而這樣的函數我們可以將他等同於機器學習中常用的 loss function。</p>
<p>$$<br>f(x) = -E(x) = - \text{loss function}<br>$$</p>
<p>這樣的等號是基於在熱力學第二定律的描述，熱力學第二定律描述一個封閉系統（closed system），封閉系統不允許系統與環境有任何的物質交換，在環境及熵不變的情況下，而系統的內能會降低，當內能降到最低的時候，系統會達成動態平衡。這稱為最小能量原則（principle of minimum energy），是熱力學第二定律另一個面向的描述。</p>
<blockquote>
<p>相對的是，在一個孤立系統（isolated system），孤立系統不允許系統與環境有任何的物質與能量交換，在環境不變的情況下，熵會持續增加。</p>
</blockquote>
<p>依據最小能量原則，能量最小跟我們希望的 loss function 最低有同樣的目標，所以我們可以將他們同等起來。不過要強調的是，這裡並沒有意義上的同等，只是在最佳化的方向上是同等的。</p>
<h3 id="Bayesian-model"><a href="#Bayesian-model" class="headerlink" title="Bayesian model"></a>Bayesian model</h3><p>在 Bayesian model 的方法中就更有趣了。當我們想要最小化 loss function 的時候，他其實在最大化 likelihood function。換句話說，降低 loss 是在降低資料與模型之間的誤差，讓模型更貼近資料，提升 likelihood 則是另一個面向的描述，likelihood 是在測量模型與資料的相似度，最大化 likelihood 也是讓模型愈貼近資料，所以我們可以將他變成這個樣子：</p>
<p>$$<br>f(x) = -E(x) = \text{likelihood function}<br>$$</p>
<p>這樣的解釋是什麼呢？</p>
<p>也就是，在統計學裡常用的 Maximum likelihood estimation （MLE）方法會跟最小能量原則有一致的目標。</p>
<p>我們從貝氏定理出發。我們先把貝氏定理的分母省略掉。</p>
<p>$$<br>\large P(\theta \mid x) = \frac{P(x \mid \theta)P(\theta)}{P(x)} = \frac{1}{Z} P(x \mid \theta)P(\theta)<br>$$</p>
<p>接者，貝氏定理的分子部份其實代表的各自是 likelihood 以及 prior。</p>
<p>$$<br>\propto P(x \mid \theta)P(\theta)<br>$$</p>
<p>$$<br>\text{    (likelihood)(prior)}<br>$$</p>
<p>我們可以將 Energy-based model 代入，容我省略分母。</p>
<p>$$<br>\propto e^{-E(x)} \times P(\theta)<br>$$</p>
<p>旁邊的 prior 我們可以忽略他，但是我們可以代入 normal distribution，會有有趣的結果。</p>
<p>$$<br>\large = e^{-E(x)} \times \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{\theta^2}{2 \sigma^2}}<br>$$</p>
<p>接著我們把等號的兩側都取 ln。</p>
<p>$$<br>-ln P(\theta \mid x) \propto E(x) + \frac{\theta^2}{2 \sigma^2} - ln(\frac{1}{\sqrt{2 \pi \sigma^2}}) + ln(Z)<br>$$</p>
<p>有沒有看到有趣的部份了？能量的部份顯露出來，當能量最小時，$P(\theta \mid x)$ 就會是最大，而 $P(\theta \mid x)$ 則是 posterior，最小能量原則會對應到貝氏方法的 Maximum a posteriori（MAP）。如果 MAP，將 prior 移除的話，就等同於 MLE 了！</p>
<p>所以我們可以看到在統計力學與統計學跟機器學習之間非常緊密的關係，而且他們都可以借用物理的熱力學第二定律來加以解釋，如此一來，可以賦予機器學習模型一個物理意義。</p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>還有嗎？當然還有阿！</p>
<p>我們可以進一步將式子整理一下，我們將 $\theta^2$ 分離出來，將 ln 中的分母倒過來：</p>
<p>$$<br>= E(x) + \frac{1}{2 \sigma^2} \theta^2 + ln(\sqrt{2 \pi \sigma^2}) + ln(Z)<br>$$</p>
<p>有沒有注意到 $\theta^2$ 非常像什麼？</p>
<p>是的，他就是一個模型參數的 $\mathcal{l}_2$-norm，也就是，他是一個 regularization term。</p>
<p>我們再進一步將他凸顯出來。</p>
<p>$$<br>\text{let } \lambda = \frac{1}{2 \sigma^2}<br>$$</p>
<p>$$<br>= E(x) + \lambda \theta^2 + ln(\frac{\sqrt{\pi}}{\lambda}) + ln(Z)<br>$$</p>
<p>你會發現式子的前半部份像極了一般的 loss function，含有 error function 跟 regularization term。$\lambda$ 部份就是你在調整模型的時候，regularization 的強度。</p>
<p>整體來說，我們在前面的 prior 代入了 normal distribution，最後導出來得到 $\mathcal{l}_2$ regularization。這說明了，如果我們在模型中加入 $\mathcal{l}_2$ regularization，等於是對模型的參數做了假設，也就是假設模型參數會服從 normal distribution。這樣的結果與貝氏的方法與解釋一致。</p>
<h3 id="只有最小值沒有-0"><a href="#只有最小值沒有-0" class="headerlink" title="只有最小值沒有 0"></a>只有最小值沒有 0</h3><p>我們可以觀察一下 $ln(\sqrt{2 \pi \sigma^2}) + ln(Z)$ 項。</p>
<p>因為 $\sigma$ 代表 normal distribution 的 標準差，$\sigma \ge 0$。</p>
<p>$\sqrt{2 \pi \sigma^2} \ge 0$，然後 $ln(\sqrt{2 \pi \sigma^2}) \ge 0$。</p>
<p>然而 $Z$ 代表的是 $e^{f(x)}$ 的總和，自然也是 $Z \ge 0$，所以 $ln(Z) \ge 0$。</p>
<p>$ln(\sqrt{2 \pi \sigma^2}) + ln(Z) \ge 0$，所以在你的 loss function 中應該會有一個理論最小值，你永遠不會達到 0。</p>
<p>最後總結一下，energy-based model 借用了物理的概念，與最佳化的機制有一致的方向，應該有人證明這點，只是我還沒去找。我們也看到 energy-based model 放到貝氏統計中是很自然的。在貝氏統計中的 prior 會對應到 regularization 的機制。這邊說明了物理、統計跟機器學習模型之間的關係。</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/09/12/energy-model-bayesian/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/08/24/mastery/">
                            《喚醒你心中的大師：偷學48位大師精進的藝術，做個厲害的人》
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-08-24T22:21:21+08:00">
	
		    Aug 24, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Book/">Book</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/xM0HUZ4E5I8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<p>摘要筆記：</p>
<ol>
<li>前置階段<ul>
<li>發掘內心的召喚</li>
</ul>
</li>
<li>學徒訓練階段（5~10 年）<ul>
<li>實踐<ul>
<li>深入觀察</li>
<li>技能習得（一萬小時）</li>
<li>實驗</li>
</ul>
</li>
<li>策略<ul>
<li>選擇成長潛力最大，而不是賺最多錢的</li>
<li>持續拓展視野</li>
<li>讓認知框架保持開放</li>
</ul>
</li>
</ul>
</li>
<li>積極創造階段<ul>
<li>尋找具有創造性的任務</li>
<li>實踐創造力的策略<ul>
<li>培養 negative capability</li>
<li>不要一直處於專注模式</li>
<li>注意細節及異常現象</li>
</ul>
</li>
<li>設定工作期限</li>
</ul>
</li>
<li>大師境界階段<ul>
<li>掌握部分跟整體的關係</li>
<li>連結萬物的追求</li>
<li>成為你自己</li>
</ul>
</li>
</ol>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/08/24/mastery/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/08/08/11-usual-topology-and-subbasis/">
                            Usual topology and subbasis
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-08-08T13:38:50+08:00">
	
		    Aug 08, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Topology/">Topology</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>我們今天就來談談一些常見的拓樸。</p>
                    
                        <a href="/2018/08/08/11-usual-topology-and-subbasis/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/08/03/the-nutshell-of-ai/">
                            AI 的核心
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-08-03T00:17:30+08:00">
	
		    Aug 03, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <!-- toc -->
<h2 id="AI-是什麼？"><a href="#AI-是什麼？" class="headerlink" title="AI 是什麼？"></a>AI 是什麼？</h2><p>很多人探討 AI 會追隨前人的探討，去探討什麼是智慧？或是什麼樣的是強人工智慧？</p>
<p>當然我也做過一樣的事情，只是今天我想從比較 <strong>技術層面</strong>、比較 <strong>務實</strong> 的角度切入這一大類技術。</p>
<p>從比較技術層面跟務實的角度切入，就表示我想討論的是現今人工智慧的實作層面，也就是 <strong>AI 是怎麼被做出來的？（how）</strong>，並非討論 <strong>AI 是什麼？(what)</strong></p>
<p><strong>AI 是什麼？</strong> 這議題會牽涉到什麼是智慧？而智慧這種事情連人類自身都說不清楚，有的人從哲學層面討論智慧，動物有動物的智慧，人類有人類的智慧，憑什麼說人類的『智慧』才稱為智慧？有的人從生物角度切入， 從大腦的結構與神經元的連結，到神經元的觸發，這一系列的科學探索，或許我們未來可以回答智慧是什麼？但目前仍舊是一個大謎團。</p>
<p>我今天要談的都不是這些，我要談的是深度學習的核心，不！是機器學習的核心…… 嗯……等等，所以那是什麼？</p>
                    
                        <a href="/2018/08/03/the-nutshell-of-ai/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/27/activation-function/">
                            Activation function 到底怎麼影響模型？
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-27T11:02:54+08:00">
	
		    Jul 27, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>今天我們來談談 activation function 吧！</p>
<h2 id="先談談線性轉換"><a href="#先談談線性轉換" class="headerlink" title="先談談線性轉換"></a>先談談線性轉換</h2><p>談 activation function 之前先要談談線性轉換。</p>
<p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。</p>
<p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p>
<p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p>
                    
                        <a href="/2018/07/27/activation-function/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/25/10-basis-for-topology/">
                            Basis for topology
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-25T23:57:56+08:00">
	
		    Jul 25, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Topology/">Topology</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>上一篇介紹完基本的拓樸結構，接下來我們來看基底（basis）的部份。</p>
<p>有上過線性代數的朋友們應該會知道，向量如果滿足線性獨立可以 span 到整個空間，而一個空間有他們的基底。</p>
<p>你可以把向量看成一種數學物件，空間的話就是很多這種數學物件的集合，那相對基底的話就是要擴展成整個空間的基本元素。</p>
<p>拓樸也是一樣的，開集（open set）也是一種數學物件，一個拓樸空間中所包含的元素就是開集，那麼就會很自然的想知道他的基底是什麼？</p>
                    
                        <a href="/2018/07/25/10-basis-for-topology/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/22/09-topology/">
                            Topology space and topology
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-22T00:32:23+08:00">
	
		    Jul 22, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Topology/">Topology</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>我們終於來到拓樸學的大門口了！</p>
<p>（謎：前面走那麼多圈是在幹什麼的！</p>
<p>拓樸其實是幾何學的拓展，他往更基礎的方向去，當我們在探討幾何學的時候，其實我們研究的是空間關係。</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/8/85/Stereographic_projection_in_3D.png" alt=""></p>
                    
                        <a href="/2018/07/22/09-topology/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/22/08-infinite-sets/">
                            Infinite sets
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-22T00:31:24+08:00">
	
		    Jul 22, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Topology/">Topology</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>我們已經遇到一些無限集（infinite set），接下來會討論他的一些特性，然後會自然地討論到選擇公理（axiom of choice）。</p>
<blockquote>
<p> <strong><em>Theorem</em></strong></p>
</blockquote>
<p>$A$ 是一個集合，以下的命題等價：</p>
<ol>
<li>$\exists \enspace injective \enspace f: \mathbb{N} \rightarrow A$</li>
<li>$B \subset A, \exists \enspace bijective \enspace f: A \rightarrow B$</li>
<li>$A$ is infinite</li>
</ol>
                    
                        <a href="/2018/07/22/08-infinite-sets/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/22/07-countable-sets/">
                            Countable sets
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-22T00:29:45+08:00">
	
		    Jul 22, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Topology/">Topology</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>前面有提到正整數可以用來作為有限集的原型，我們會把所有正整數的集合稱為 <strong>可數無限集（countably infinite sets）</strong>。</p>
                    
                        <a href="/2018/07/22/07-countable-sets/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/07/22/06-finite-sets/">
                            Finite sets
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-07-22T00:27:18+08:00">
	
		    Jul 22, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Topology/">Topology</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>接下來我們會來討論幾個常見的概念，像是有限集及無限集、可數集及不可數集。</p>
<p>有限集（finite set）：</p>
<blockquote>
<p> <strong><em>Def.</em></strong></p>
</blockquote>
<p>A set A is <strong><em>finite</em></strong> if</p>
<p>$$<br>\exists f: A \rightarrow \{ 1, …, n \}, f \enspace is \enspace bijective.<br>$$</p>
<p>這時我們會說 <em>set</em> $A$ 的 cardinality 是 n。</p>
                    
                        <a href="/2018/07/22/06-finite-sets/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/archives/2018/page/2/">
              <span>OLDER POSTS</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">page 1 of 3</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>Aim to be a computational biologist!</br>Systems Biology, Computational Biology, Machine Learning</br>Organizer of Julia Taiwan community</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Research assistant, Taiwan CDC</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
