
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Archives: 2018 - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{}</script>
    <meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="Dream Maker">
<meta property="og:url" content="https://yuehhua.github.io/archives/2018/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dream Maker">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="2">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="2">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="2"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/23/24-recurrent-neural-network/">
                            24 Recurrent neural network
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-23T21:43:49+08:00">
	
		    Oct 23, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-content">
                    <p>接續上一篇。</p>
<h2 id="Recurrent-neural-network"><a href="#Recurrent-neural-network" class="headerlink" title="Recurrent neural network"></a>Recurrent neural network</h2><p>那大家所熟知的 RNN 是怎麼回事呢？我們把假設改了一下：</p>
<ul>
<li>狀態都是 <strong>連續</strong> 的。</li>
<li>時間是離散的。</li>
<li>內在狀態是不能被觀察的，外在狀態是可被觀察的。</li>
<li>以一個 <strong>隨機向量</strong> 作為一個狀態。</li>
<li><strong>允許在每個時間點給輸入</strong></li>
<li><strong>引入非線性</strong></li>
</ul>
<p>首先，在這邊的狀態會以一個向量做表示，大家應該也知道 RNN 的 input 是一個向量，當中的狀態也是一個向量，最後的 output 也是一個向量。而這些向量當中的的值都是連續的 $\mathbb{R}^n$（假設向量大小為 n），不像上面的模型都是離散的 $k$（假設有 k 個狀態），所以在空間上的大小可以說是擴大非常多。</p>
<p>接下來我們來看看時間的狀態轉換：</p>
<p><br></p>
<p><img src="/images/rnn_time.svg" alt=""></p>
<p><br></p>
<p><img src="/images/rnn_expand_time.svg" alt=""></p>
<p><br></p>
<p>在 RNN 中一樣含有內在狀態，但不同的是 RNN 可以在每個時間點上給輸入向量（$\mathbf{x^{(t)}}$），所以可以根據前一個時間點的內在狀態（$\mathbf{h^{(t)}}$）跟輸入向量去計算輸出，或是外在狀態（$\mathbf{y^{(t)}}$）。</p>
<p>所以大家會在一些論文上看到模型的狀態關係式長下面這個樣子：</p>
<p>$$<br>\mathbf{h^{(t)}} = f(\mathbf{x^{(t)}}, \mathbf{h^{(t-1)}}) = \mathbf{x^{(t)}} W_x + \mathbf{h^{(t-1)}} W_h + \mathbf{b}<br>$$</p>
<p>$$<br>\mathbf{y^{(t)}} = g(\mathbf{h^{(t)}}) = sigm(\mathbf{h^{(t)}} W_y)<br>$$</p>
<p>這邊特別引入了非線性的轉換（$sigm$）來讓模型更強大。</p>
<p>隨著從一開始的馬可夫模型到這邊應該對這幾個模型有點感覺，其實 RNN 可以說是很大的突破，在假設上放了很多元素讓模型變得更強大。</p>
<h2 id="Long-short-term-memory"><a href="#Long-short-term-memory" class="headerlink" title="Long short-term memory"></a>Long short-term memory</h2><p>人們為了改進 RNN這個模型的記憶性，希望他可以記住更遠以前的東西，所以設計了 LSTM 來替換他的 hidden layer 的運作模式，後期更有 GRU，還有人說只需要 forget gate 就有很強大的效能的 MGU。這些都是對於記憶性做的改進，個人覺得這些在工程上的貢獻比較大，真正學術上的突破其實還好。</p>
<p>今天的整理就先到這邊啦！</p>

                    
                        

                    
                    
                        <p>
                            <a href="/2018/10/23/24-recurrent-neural-network/#post-footer" class="postShorten-excerpt_link link">
                                Comment and share
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/23/23-markov-chain-and-hmm/">
                            23 Markov chain 及 HMM
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-23T09:21:38+08:00">
	
		    Oct 23, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>上次我們講完在空間上，我們可以知道資料的區域性，並且利用 convolution 來萃取特徵。</p>
<p>這次我們來講時間，其實不一定要是”時間”序列資料，只要是有先後順序的資料就可以。</p>
<p>在時間序列分析及統計的領域中，我們有基礎的馬可夫模型（Markov chain）。</p>
                    
                        <a href="/2018/10/23/23-markov-chain-and-hmm/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/22/22-convolutional-encoder-decoder-architecture/">
                            22 Convolutional encoder-decoder 架構
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-22T23:10:35+08:00">
	
		    Oct 22, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>標題這不是一個專有名詞。</p>
<p>在電腦視覺的領域中有幾個有名的問題：</p>
<ol>
<li>影像辨識（Image recognition）</li>
<li>物件辨識（Object detection）</li>
<li>語意分割（Semantic segmentation）</li>
</ol>
                    
                        <a href="/2018/10/22/22-convolutional-encoder-decoder-architecture/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/19/21-activation-functions-and-ReLU/">
                            21 Activation functions and ReLU
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-19T22:34:55+08:00">
	
		    Oct 19, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>今天我們來談談 activation function 吧！</p>
<h2 id="先談談線性轉換"><a href="#先談談線性轉換" class="headerlink" title="先談談線性轉換"></a>先談談線性轉換</h2><p>談 activation function 之前先要談談線性轉換。</p>
<p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。</p>
<p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p>
<p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p>
                    
                        <a href="/2018/10/19/21-activation-functions-and-ReLU/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/17/20-convolutional-neural-network/">
                            20 Convolutional neural network
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-17T22:58:40+08:00">
	
		    Oct 17, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <h2 id="Convolution-layer"><a href="#Convolution-layer" class="headerlink" title="Convolution layer"></a>Convolution layer</h2><p>這邊我們回到我們的 convolution layer，如果把以上的一維向量拓展到二維的矩陣資料會長什麼樣子呢？</p>
<p>我們先來看二維的 cross-correlation 長什麼樣子。</p>
                    
                        <a href="/2018/10/17/20-convolutional-neural-network/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/17/19-convolution-operation/">
                            19 Convolution 運算
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-17T22:58:29+08:00">
	
		    Oct 17, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>熱身運動都做好了，接下來我們就一路往影像處理上的重要技術 CNN 前進啦！</p>
<p>Convolutional neural network，顧名思義，他是一種神經網路架構，裡頭包含著 convolution 的運算。</p>
<p>那為什麼 convolution 這麼重要，重要到要放在名稱上呢？</p>
                    
                        <a href="/2018/10/17/19-convolution-operation/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/17/18-multi-layer-preceptron/">
                            18 Multi-layer preceptron
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-17T22:58:18+08:00">
	
		    Oct 17, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>我們來更具體一點講 multi-layer perceptron (MLP)。</p>
<p>最簡單的版本莫過於 linear MLP，不過不太會有人去用他，其實只是每層 layer 的 activation function 都是採用 identity。你可以想像他是有很多的線性轉換所疊起來的模型。</p>
                    
                        <a href="/2018/10/17/18-multi-layer-preceptron/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/17/17-autoencoder/">
                            17 Autoencoder
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-17T22:36:06+08:00">
	
		    Oct 17, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>既然前一篇提到學習特徵是一件重要的事，那麼我們就來講講 autoencoder 吧！</p>
<p>Autoencoder 就是一個 unsupervised 方法，試圖學習出可以用的特徵。雖然不少人可能會說他是一個壓縮的方法或是一個降維的方法，其實他都是。</p>
                    
                        <a href="/2018/10/17/17-autoencoder/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/16/16-representation-learning/">
                            16 Representation learning
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-16T23:28:04+08:00">
	
		    Oct 16, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>機器學習的技術已經發展了非常久的時間，我們有非常多的模型可以幫我們做預測，包含像是 regression、classification、clustering、semi-supervised learning、reinforcement learning。這些都可以幫助我們去做出預測，或是從資料當中去挖掘知識跟資訊。這些模型需要數學與統計作為基礎。</p>
<p>當你使用這些模型之後你會發現，你輸入的資料會大大的影響整個成效，像是你給的特徵不夠好，模型的表現就變得很糟糕，或是模型要預測的資訊根本不在這些資料當中，那麼模型根本就預測不出來，所以玩過機器學習的人就會知道特徵工程的重要性。</p>
                    
                        <a href="/2018/10/16/16-representation-learning/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    
    <article class="postShorten postShorten--thumbnailimg-right">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title">
                    
                        <a class="link-unstyled" href="/2018/10/15/15-why-deep/">
                            15 為什麼要深？
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time datetime="2018-10-15T14:56:39+08:00">
	
		    Oct 15, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

            </div>
            
                <div class="postShorten-excerpt">
                    <p>接著我們就來到了蠻重要的問題，既然一個 hidden layer 的網路架構就可以逼近任何連續函數，那麼為什麼要深度學習？</p>
                    
                        <a href="/2018/10/15/15-why-deep/" class="postShorten-excerpt_link link">
                            Continue reading
                        </a>
                        
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
          <li class="pagination-next">
            <a class="btn btn--default btn--small" href="/archives/2018/page/2/">
              <span>OLDER POSTS</span>
              <i class="fa fa-angle-right text-base icon-ml"></i>
            </a>
          </li>
        
        <li class="pagination-number">page 1 of 5</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>Aim to be a computational biologist!</br>Systems Biology, Computational Biology, Machine Learning</br>Organizer of Julia Taiwan community</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Research assistant, Taiwan CDC</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
