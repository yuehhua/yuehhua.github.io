
<!DOCTYPE html>
<html lang="zh-tw,en,default">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>09 從線性迴歸到羅吉斯迴歸 - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"},"articleBody":"我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。\n現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。\n\n常態分佈其實我們在前面的文章中有提到我們的線性迴歸模型假設了誤差是會呈現一個常態分佈。\n對於誤差的假設這件事情其實很大程度影響了我們模型的長相，也會影響這個模型所適用的資料。\n像是線性迴歸模型適用的是連續型變數的資料，更進一步來說，他是要求他的應變量（y）要是連續型的，看起來是這樣的。\n$$\\mathbb{E}[y] = \\mathbb{E}[Normal(Y=y; \\mu, \\sigma)] = \\mu = \\mathbf{w}^T\\mathbf{x} + b$$\n$Normal(Y=y; \\mu, \\sigma)$：代表常態分佈，以 $\\mu$ 為平均數，$\\sigma$ 為標準差，輸入是應變量（y），輸出是機率。\n事實上，當你把你的一些 feature 輸入這個模型後，預測出來的是應變量的期望值或是平均數（$\\mathbb{E}[y]$）。\n如果我們要預測的應變量是二元的類別資料，那也就是把問題轉換成分類問題，我們就需要用不同的分佈。\n白努力分佈與二項式分佈當我們的資料是分成兩類：0, 1，那麼他會對應到白努力分佈（Bernoulli distribution）。\n當你蒐集了一群二元分類的資料，那麼就會對應到二項式分佈（Binomial distribution）。\n如果你要預測的是一筆二元分類的資料，你應該要假設你的應變量（y）是白努力分佈。\n$$\\mathbb{E}[y] = \\mathbb{E}[Ber(Y=y; p)] = \\mu = \\mathbf{w}^T\\mathbf{x} + b$$\n$Ber(Y=y; p)$：代表白努力分佈，以 p 為成功機率，輸入是應變量（y），輸出是機率。\n羅吉斯迴歸羅吉斯迴歸的模型是以下這個樣子：\n$$y = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)$$\n其中的 $\\sigma$ 是 sigmoid function，由這個函數輸出的就是成功機率了：\n$$p = \\sigma(s) = \\frac{1}{1 + e^{-s}}$$\n那這跟我們前面提的分佈看起來都不太一樣阿！我們來反推一下，其實我們前面提到的分佈應該是跟 sigmoid function 的反函數有關係：\n$$\\sigma^{-1}(y) = \\mathbf{w}^T\\mathbf{x} + b$$\n我們從白努力分佈開始，當 $y \\in {0, 1}$ 時：\n$$Ber(Y=y; p) = p^y (1-p)^{(1-y)}$$\n也就是，當 y 是 0 的時候，只會剩下後面那一項，當 y 是 1 的時候，只會剩下前面那一項。\n那們我們要預測的其實是一個事件發生的機率 p，可是我們由 $\\mathbf{w}^T\\mathbf{x} + b$ 計算出來的是連續的數值。那要怎麼將這兩者接起來呢？\n在統計學裡有一個叫作 logit function 的東西，中文翻譯叫作邏輯函數，但是跟邏輯沒什麼關係就是了。\n$$y = log \\frac{x}{1 - x}$$\n如果將裡頭的 x 想成成功機率的話，分子就是成功的機率，分母就是失敗的機率，就非常像一個東西叫作勝算（odds），所以他又叫作 log odds。\n用勝算的話就是一個連續的數值，同時也可以表達成功機率的含意，我們就從這邊開始吧！一路朝著 sigmoid function 邁進。\n$$s = ln \\frac{p}{1 - p}$$\n$$-s = ln \\frac{1 - p}{p}$$\n$$e^{-s} = \\frac{1 - p}{p}$$\n$$p + e^{-s}p = 1$$\n$$(1 + e^{-s})p = 1$$\n$$p = \\frac{1}{1 + e^{-s}}$$\n看到了嗎？他是 sigmoid function！\n也就是說，如果用線性迴歸預測的是 log odds，那就會跟羅吉斯迴歸一樣！\n$$ln \\frac{p}{1 - p} = \\sigma^{-1}(y) = \\mathbf{w}^T\\mathbf{x} + b$$\n你如果把 sigmoid function 還原回來的話：\n$$y = \\frac{1}{1 + e^{- \\mathbf{w}^T\\mathbf{x} - b}} = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)$$\n就是羅吉斯迴歸的樣子了！\n","dateCreated":"2018-10-09T22:59:32+08:00","dateModified":"2018-10-15T15:04:15+08:00","datePublished":"2018-10-09T22:59:32+08:00","description":"我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。\n現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。","headline":"09 從線性迴歸到羅吉斯迴歸","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/"},"publisher":{"@type":"Organization","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd","logo":{"@type":"ImageObject","url":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"}},"url":"https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/"}</script>
    <meta name="description" content="我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。 現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。">
<meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="09 從線性迴歸到羅吉斯迴歸">
<meta property="og:url" content="https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:description" content="我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。 現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。">
<meta property="og:locale" content="zh-tw">
<meta property="og:updated_time" content="2018-10-15T07:04:15.957Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="09 從線性迴歸到羅吉斯迴歸">
<meta name="twitter:description" content="我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。 現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="首頁"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首頁</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="分類"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分類</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="標籤"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">標籤</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="所有文章"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">所有文章</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="搜尋"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">搜尋</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="關於"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">關於</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Email">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Email</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="Atom"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Atom</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
    <article class="post">
        
                
                    <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            09 從線性迴歸到羅吉斯迴歸
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2018-10-09T22:59:32+08:00">
	
		    10月 09, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>, <a class="category-link" href="/categories/Machine-Learning/機器學習模型圖書館：從傳統模型到深度學習/">機器學習模型圖書館：從傳統模型到深度學習</a>


    
</div>

    
</div>

                        
                            <div class="post-content markdown">
                                <div class="main-content-wrap">
                                    <p>我們從前面的模型演化可以了解一個機器學習模型可以怎麼樣衍生出其他的變體來解決問題。</p>
<p>現在我們要切換到另外一條跑道上，我們一樣是從線性迴歸模型出發，我們或許可以換成其他的不同的分佈。</p>
<a id="more"></a>
<h2 id="常態分佈"><a href="#常態分佈" class="headerlink" title="常態分佈"></a>常態分佈</h2><p>其實我們在前面的文章中有提到我們的線性迴歸模型假設了誤差是會呈現一個常態分佈。</p>
<p>對於誤差的假設這件事情其實很大程度影響了我們模型的長相，也會影響這個模型所適用的資料。</p>
<p>像是線性迴歸模型適用的是連續型變數的資料，更進一步來說，他是要求他的應變量（y）要是連續型的，看起來是這樣的。</p>
<p>$$<br>\mathbb{E}[y] = \mathbb{E}[Normal(Y=y; \mu, \sigma)] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>$Normal(Y=y; \mu, \sigma)$：代表常態分佈，以 $\mu$ 為平均數，$\sigma$ 為標準差，輸入是應變量（y），輸出是機率。</p>
<p>事實上，當你把你的一些 feature 輸入這個模型後，預測出來的是應變量的期望值或是平均數（$\mathbb{E}[y]$）。</p>
<p>如果我們要預測的應變量是二元的類別資料，那也就是把問題轉換成分類問題，我們就需要用不同的分佈。</p>
<h2 id="白努力分佈與二項式分佈"><a href="#白努力分佈與二項式分佈" class="headerlink" title="白努力分佈與二項式分佈"></a>白努力分佈與二項式分佈</h2><p>當我們的資料是分成兩類：0, 1，那麼他會對應到白努力分佈（Bernoulli distribution）。</p>
<p>當你蒐集了一群二元分類的資料，那麼就會對應到二項式分佈（Binomial distribution）。</p>
<p>如果你要預測的是一筆二元分類的資料，你應該要假設你的應變量（y）是白努力分佈。</p>
<p>$$<br>\mathbb{E}[y] = \mathbb{E}[Ber(Y=y; p)] = \mu = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>$Ber(Y=y; p)$：代表白努力分佈，以 p 為成功機率，輸入是應變量（y），輸出是機率。</p>
<h2 id="羅吉斯迴歸"><a href="#羅吉斯迴歸" class="headerlink" title="羅吉斯迴歸"></a>羅吉斯迴歸</h2><p>羅吉斯迴歸的模型是以下這個樣子：</p>
<p>$$<br>y = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p>
<p>其中的 $\sigma$ 是 sigmoid function，由這個函數輸出的就是成功機率了：</p>
<p>$$<br>p = \sigma(s) = \frac{1}{1 + e^{-s}}<br>$$</p>
<p>那這跟我們前面提的分佈看起來都不太一樣阿！我們來反推一下，其實我們前面提到的分佈應該是跟 sigmoid function 的反函數有關係：</p>
<p>$$<br>\sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>我們從白努力分佈開始，當 $y \in {0, 1}$ 時：</p>
<p>$$<br>Ber(Y=y; p) = p^y (1-p)^{(1-y)}<br>$$</p>
<p>也就是，當 y 是 0 的時候，只會剩下後面那一項，當 y 是 1 的時候，只會剩下前面那一項。</p>
<p>那們我們要預測的其實是一個事件發生的機率 p，可是我們由 $\mathbf{w}^T\mathbf{x} + b$ 計算出來的是連續的數值。那要怎麼將這兩者接起來呢？</p>
<p>在統計學裡有一個叫作 logit function 的東西，中文翻譯叫作邏輯函數，但是跟邏輯沒什麼關係就是了。</p>
<p>$$<br>y = log \frac{x}{1 - x}<br>$$</p>
<p>如果將裡頭的 x 想成成功機率的話，分子就是成功的機率，分母就是失敗的機率，就非常像一個東西叫作勝算（odds），所以他又叫作 log odds。</p>
<p>用勝算的話就是一個連續的數值，同時也可以表達成功機率的含意，我們就從這邊開始吧！一路朝著 sigmoid function 邁進。</p>
<p>$$<br>s = ln \frac{p}{1 - p}<br>$$</p>
<p>$$<br>-s = ln \frac{1 - p}{p}<br>$$</p>
<p>$$<br>e^{-s} = \frac{1 - p}{p}<br>$$</p>
<p>$$<br>p + e^{-s}p = 1<br>$$</p>
<p>$$<br>(1 + e^{-s})p = 1<br>$$</p>
<p>$$<br>p = \frac{1}{1 + e^{-s}}<br>$$</p>
<p>看到了嗎？他是 sigmoid function！</p>
<p>也就是說，如果用線性迴歸預測的是 log odds，那就會跟羅吉斯迴歸一樣！</p>
<p>$$<br>ln \frac{p}{1 - p} = \sigma^{-1}(y) = \mathbf{w}^T\mathbf{x} + b<br>$$</p>
<p>你如果把 sigmoid function 還原回來的話：</p>
<p>$$<br>y = \frac{1}{1 + e^{- \mathbf{w}^T\mathbf{x} - b}} = \sigma(\mathbf{w}^T\mathbf{x} + b)<br>$$</p>
<p>就是羅吉斯迴歸的樣子了！</p>

                                        

                                </div>
                            </div>
                            <div id="post-footer" class="post-footer main-content-wrap">
                                
                                        
                                            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/10/10/10-from-linear-regression-to-poisson-regression/" data-tooltip="10 從線性迴歸到 Poisson 迴歸" aria-label="上一篇: 10 從線性迴歸到 Poisson 迴歸">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/10/08/08-l2-regularized-linear-model/" data-tooltip="08 l2-regularized 線性模型" aria-label="下一篇: 08 l2-regularized 線性模型">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                                                
                                                    
                                                        
                                                            <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
                                                                
                                                                            
                            </div>
                            
                                <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
                                <script>
                                    if (window.mermaid) {
                                        mermaid.initialize({ theme: 'forest' });
                                    }
                                </script>
                                
    </article>

                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/10/10/10-from-linear-regression-to-poisson-regression/" data-tooltip="10 從線性迴歸到 Poisson 迴歸" aria-label="上一篇: 10 從線性迴歸到 Poisson 迴歸">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/10/08/08-l2-regularized-linear-model/" data-tooltip="08 l2-regularized 線性模型" aria-label="下一篇: 08 l2-regularized 線性模型">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="5">
    <i id="btn-close-shareoptions" class="fa fa-close"></i>
    <ul class="share-options">
        
            
            
            <li class="share-option">
                <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i><span>分享到 Facebook</span>
                </a>
            </li>
        
    </ul>
</div>

            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>目標是計算生物學家！</br>Systems Biology, Computational Biology, Machine Learning</br>Julia Taiwan 發起人</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>研發替代役研究助理</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->


<script>
    var disqus_config = function () {
        this.page.url = 'https://yuehhua.github.io/2018/10/09/09-from-linear-regression-to-logistic-regression/';
                 
            this.page.identifier = '2018/10/09/09-from-linear-regression-to-logistic-regression/';
                 
             };
    (function () {
        var d = document, s = d.createElement('script');
        var disqus_shortname = 'dream-maker';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
