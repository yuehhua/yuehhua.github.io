
<!DOCTYPE html>
<html lang="en">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Activation function 到底怎麼影響模型？ - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"},"articleBody":"今天我們來談談 activation function 吧！\n先談談線性轉換談 activation function 之前先要談談線性轉換。\n有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。\n推薦可以看周老師的線代啟示錄 奇異值分解 (SVD)\n我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：\n\n$$A = U \\Sigma V^{T}$$\n這 3 個矩陣分別是有意義的，$U$、$\\Sigma$、$V$ 的意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：\n\n\n圖來自以上提到的文章\n\n所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。\n在模型上，我們還會加上一個 bias 來達到平移的效果。\n我們可以來造一些點：\n\n我們可以 random 造一個矩陣：\n$$A =\\begin{bmatrix}0.35166263&amp; 0.1124966 \\\\0.25249535&amp; 0.65481947 \\\\\\end{bmatrix}$$\n所以我們可以對每個點做運算 $Ax$：\n\n去吧！Activation function！接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？\n\n大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。\n所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。\n那如果是用 sigmoid 的效果呢？\n\n他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。\n我們放大來看看他整體形狀有沒有什麼變化。\n\n整體形狀有些微被扭曲了，不知道大家有沒有發現呢？\n所以在引進 activation function 之後，模型擁有了 扭曲 的能力！\n那麼 activation function 到底實際上做了什麼事呢？\n雕塑的工匠以 ReLU 來說，他就像一個工匠正在雕塑一個作品。\nReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：\n\n\n圖來自 漫遊‧藝術網＠成大校園\n\n原諒我私心用成大的雕塑品當範例。XD\nReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。\n消失的梯度以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。\n其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。\n那如果放在 CNN 的話，就會發生梯度消失的問題。\n在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。\n讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。\n這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。\n模型怎麼知道要從哪裡下刀？我想蠻多人應該會有跟我一樣的問題。\n我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？\n答案是 backpropagation (gradient descent method) 會告訴你！\n藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。\n為什麼 activation function 一定要長這樣？其實沒有規定 activation function 一定要長怎樣。\n但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。\n所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。\n結論不同的模型跟問題其實適用不同的 activation function。\n像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。\n所以在 feature extraction 的階段就需要找適用的 activation function。\n那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。\n這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD\n今天就到這邊告一個段落囉。\n","dateCreated":"2018-07-27T11:02:54+08:00","dateModified":"2018-07-27T11:16:24+08:00","datePublished":"2018-07-27T11:02:54+08:00","description":"今天我們來談談 activation function 吧！\n先談談線性轉換談 activation function 之前先要談談線性轉換。\n有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。\n推薦可以看周老師的線代啟示錄 奇異值分解 (SVD)\n我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：","headline":"Activation function 到底怎麼影響模型？","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuehhua.github.io/2018/07/27/activation-function/"},"publisher":{"@type":"Organization","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd","logo":{"@type":"ImageObject","url":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"}},"url":"https://yuehhua.github.io/2018/07/27/activation-function/"}</script>
    <meta name="description" content="今天我們來談談 activation function 吧！ 先談談線性轉換談 activation function 之前先要談談線性轉換。 有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。 推薦可以看周老師的線代啟示錄 奇異值分解 (SVD) 我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：">
<meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="Activation function 到底怎麼影響模型？">
<meta property="og:url" content="https://yuehhua.github.io/2018/07/27/activation-function/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:description" content="今天我們來談談 activation function 吧！ 先談談線性轉換談 activation function 之前先要談談線性轉換。 有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。 推薦可以看周老師的線代啟示錄 奇異值分解 (SVD) 我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&h=300">
<meta property="og:image" content="https://yuehhua.github.io/images/activation1.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation2.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation3.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation4.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation5.png">
<meta property="og:image" content="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/image/a51d2-dsc_7226.jpg">
<meta property="og:updated_time" content="2018-07-27T03:16:24.932Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Activation function 到底怎麼影響模型？">
<meta name="twitter:description" content="今天我們來談談 activation function 吧！ 先談談線性轉換談 activation function 之前先要談談線性轉換。 有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。 推薦可以看周老師的線代啟示錄 奇異值分解 (SVD) 我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：">
<meta name="twitter:image" content="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&h=300">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="Home"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="Categories"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="Tags"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="Archives"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="Search"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Search</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="About"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Mail">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Mail</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            Activation function 到底怎麼影響模型？
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2018-07-27T11:02:54+08:00">
	
		    Jul 27, 2018
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>今天我們來談談 activation function 吧！</p>
<h2 id="先談談線性轉換"><a href="#先談談線性轉換" class="headerlink" title="先談談線性轉換"></a>先談談線性轉換</h2><p>談 activation function 之前先要談談線性轉換。</p>
<p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。</p>
<p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p>
<p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p>
<a id="more"></a>
<p>$$<br>A = U \Sigma V^{T}<br>$$</p>
<p>這 3 個矩陣分別是有意義的，$U$、$\Sigma$、$V$ 的意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：</p>
<p><img src="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300" alt=""></p>
<blockquote>
<p>圖來自以上提到的文章</p>
</blockquote>
<p>所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。</p>
<p>在模型上，我們還會加上一個 bias 來達到平移的效果。</p>
<p>我們可以來造一些點：</p>
<p><img src="/images/activation1.png" alt=""></p>
<p>我們可以 random 造一個矩陣：</p>
<p>$$<br>A =<br>\begin{bmatrix}<br>0.35166263&amp; 0.1124966 \\<br>0.25249535&amp; 0.65481947 \\<br>\end{bmatrix}<br>$$</p>
<p>所以我們可以對每個點做運算 $Ax$：</p>
<p><img src="/images/activation2.png" alt=""></p>
<h2 id="去吧！Activation-function！"><a href="#去吧！Activation-function！" class="headerlink" title="去吧！Activation function！"></a>去吧！Activation function！</h2><p>接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？</p>
<p><img src="/images/activation3.png" alt=""></p>
<p>大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。</p>
<p>所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。</p>
<p>那如果是用 sigmoid 的效果呢？</p>
<p><img src="/images/activation4.png" alt=""></p>
<p>他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。</p>
<p>我們放大來看看他整體形狀有沒有什麼變化。</p>
<p><img src="/images/activation5.png" alt=""></p>
<p>整體形狀有些微被扭曲了，不知道大家有沒有發現呢？</p>
<p>所以在引進 activation function 之後，模型擁有了 <strong>扭曲</strong> 的能力！</p>
<p>那麼 activation function 到底實際上做了什麼事呢？</p>
<h2 id="雕塑的工匠"><a href="#雕塑的工匠" class="headerlink" title="雕塑的工匠"></a>雕塑的工匠</h2><p>以 ReLU 來說，他就像一個工匠正在雕塑一個作品。</p>
<p>ReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：</p>
<p><img src="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/image/a51d2-dsc_7226.jpg" alt=""></p>
<blockquote>
<p>圖來自 <a href="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/" target="_blank" rel="noopener">漫遊‧藝術網＠成大校園</a></p>
</blockquote>
<p>原諒我私心用成大的雕塑品當範例。XD</p>
<p>ReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。</p>
<h2 id="消失的梯度"><a href="#消失的梯度" class="headerlink" title="消失的梯度"></a>消失的梯度</h2><p>以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。</p>
<p>其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。</p>
<p>那如果放在 CNN 的話，就會發生梯度消失的問題。</p>
<p>在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。</p>
<p>讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。</p>
<p>這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。</p>
<h2 id="模型怎麼知道要從哪裡下刀？"><a href="#模型怎麼知道要從哪裡下刀？" class="headerlink" title="模型怎麼知道要從哪裡下刀？"></a>模型怎麼知道要從哪裡下刀？</h2><p>我想蠻多人應該會有跟我一樣的問題。</p>
<p>我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？</p>
<p>答案是 backpropagation (gradient descent method) 會告訴你！</p>
<p>藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。</p>
<h2 id="為什麼-activation-function-一定要長這樣？"><a href="#為什麼-activation-function-一定要長這樣？" class="headerlink" title="為什麼 activation function 一定要長這樣？"></a>為什麼 activation function 一定要長這樣？</h2><p>其實沒有規定 activation function 一定要長怎樣。</p>
<p>但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。</p>
<p>所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。</p>
<h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>不同的模型跟問題其實適用不同的 activation function。</p>
<p>像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。</p>
<p>所以在 feature extraction 的階段就需要找適用的 activation function。</p>
<p>那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。</p>
<p>這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD</p>
<p>今天就到這邊告一個段落囉。</p>

            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/07/25/10-basis-for-topology/" data-tooltip="Basis for topology" aria-label="NEXT: Basis for topology">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/07/27/activation-function/" title="Share on Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/07/25/10-basis-for-topology/" data-tooltip="Basis for topology" aria-label="NEXT: Basis for topology">
                
                    <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/07/27/activation-function/" title="Share on Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="5">
    <i id="btn-close-shareoptions" class="fa fa-close"></i>
    <ul class="share-options">
        
            
            
            <li class="share-option">
                <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/07/27/activation-function/">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i><span>Share on Facebook</span>
                </a>
            </li>
        
    </ul>
</div>

            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>Aim to be a computational biologist!</br>Systems Biology, Computational Biology, Machine Learning</br>Organizer of Julia Taiwan community</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>Research assistant, Taiwan CDC</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->

    
        <script>
             var disqus_config = function () {
                 this.page.url = 'https://yuehhua.github.io/2018/07/27/activation-function/';
                 
                    this.page.identifier = '2018/07/27/activation-function/';
                 
             };
            (function() {
                var d = document, s = d.createElement('script');
                var disqus_shortname = 'dream-maker';
                s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
        </script>
    



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
