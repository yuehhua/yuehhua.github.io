
<!DOCTYPE html>
<html lang="zh-tw,en,default">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>Activation function 到底怎麼影響模型？ - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"},"articleBody":"今天我們來談談 activation function 吧！\n前言\n在神經網路模型（neural network）及深度學習（deep learning）中，activation function 一直扮演重要的角色。\n網路模型中的層的概念基本上是由以下的式子構成：\n$$\n\\sigma(Wx + b)\n$$\n其中 $Wx$ 的矩陣相乘是大家在線性代數中常見的線性運算（linear operation），$Wx + b$ 嚴格來說是稱為仿射運算（affine operation）。相對應線性空間，仿射運算會構成仿射空間，不過兩者只差一個位移 $b$ 而已，但是大家都把它叫做線性（其實是錯誤的阿阿阿阿阿阿阿阿）。最後就是最外層的 $\\sigma$ 了。這就是我們今天要談的 activation function。大家應該知道 activation function 會提供神經網路模型非線性的特性，而在沒有非線性特性前的神經網路長什麼樣子呢？\n仿射？線性？\n其實是可以將仿射看成線性的。假設\n$$\nWx + b = \\begin{bmatrix}\nw_{11}&amp; w_{12}&amp; \\cdots&amp; w_{1n} \\\\\n\\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots \\\\\nw_{m1}&amp; w_{m2}&amp; \\cdots&amp; w_{mn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\n\\vdots \\\\\nx_{n} \\\\\n\\end{bmatrix} + \\begin{bmatrix}\nb_{1} \\\\\n\\vdots \\\\\nb_{m} \\\\\n\\end{bmatrix}\n$$\n然後把 $W$ 跟 $b$ 合併，有點像高中學的增廣矩陣那樣，就變成了：\n$$\n= \\begin{bmatrix}\nw_{11}&amp; w_{12}&amp; \\cdots&amp; w_{1n}&amp; b_{1} \\\\\n\\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots&amp; \\vdots \\\\\nw_{m1}&amp; w_{m2}&amp; \\cdots&amp; w_{mn}&amp; b_{m} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\n\\vdots \\\\\nx_{n} \\\\\n1\n\\end{bmatrix} = W’x’\n$$\n這樣就可以把 $b$ 吸收到 $W$ 裡面，整體就變成線性的了。\n先談談線性轉換\n談 activation function 之前先要談談線性轉換。去除掉 activation function 後的神經網路層只剩下 $Wx + b$ 的部份，而如果講這部份看成線性，我們就可以用線性代數裡的東西來解釋它。\n有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。基本上，SVD 是一種矩陣分解的技巧，可以適用各式的矩陣（只要是可分解的）。SVD 可以告訴我們一些關於線性運算的特質。\n推薦可以看周老師的線代啟示錄 奇異值分解 (SVD)\n我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：\n\n$$\nA = U \\Sigma V^T\n$$\n這 3 個矩陣分別是有意義的，$U$、$\\Sigma$、$V$ 的幾何意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：\n\n\n圖來自以上提到的文章\n\n所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。\n在模型上，我們還會加上一個 bias 來達到平移的效果。\n我們可以來造一些點：\n\n我們可以 random 造一個矩陣：\n$$\nA =\n\\begin{bmatrix}\n0.35166263&amp; 0.1124966 \\\\\n0.25249535&amp; 0.65481947 \\\\\n\\end{bmatrix}\n$$\n所以我們可以對每個點做運算 $Ax$：\n\n去吧！Activation function！\n接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？\n\n大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。\n所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。\n那如果是用 sigmoid 的效果呢？\n\n他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。\n我們放大來看看他整體形狀有沒有什麼變化。\n\n整體形狀有些微被扭曲了，不知道大家有沒有發現呢？\n所以在引進 activation function 之後，模型擁有了 扭曲 的能力！\n那麼 activation function 到底實際上做了什麼事呢？\n雕塑的工匠\n以 ReLU 來說，他就像一個工匠正在雕塑一個作品。\nReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：\n\n\n圖來自 漫遊‧藝術網＠成大校園\n\n原諒我私心用成大的雕塑品當範例。XD\nReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。\n消失的梯度\n以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。\n其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。\n那如果放在 CNN 的話，就會發生梯度消失的問題。\n在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。\n讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。\n這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。\n模型怎麼知道要從哪裡下刀？\n我想蠻多人應該會有跟我一樣的問題。\n我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？\n答案是 backpropagation (gradient descent method) 會告訴你！\n藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。\n為什麼 activation function 一定要長這樣？\n其實沒有規定 activation function 一定要長怎樣。\n但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。\n所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。\n結論\n不同的模型跟問題其實適用不同的 activation function。\n像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。\n所以在 feature extraction 的階段就需要找適用的 activation function。\n那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。\n這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD\n今天就到這邊告一個段落囉。\n","dateCreated":"2018-07-27T11:02:54+08:00","dateModified":"2019-07-08T17:10:27+08:00","datePublished":"2018-07-27T11:02:54+08:00","description":"今天我們來談談 activation function 吧！\n前言\n在神經網路模型（neural network）及深度學習（deep learning）中，activation function 一直扮演重要的角色。\n網路模型中的層的概念基本上是由以下的式子構成：\n$$\n\\sigma(Wx + b)\n$$\n其中 $Wx$ 的矩陣相乘是大家在線性代數中常見的線性運算（linear operation），$Wx + b$ 嚴格來說是稱為仿射運算（affine operation）。相對應線性空間，仿射運算會構成仿射空間，不過兩者只差一個位移 $b$ 而已，但是大家都把它叫做線性（其實是錯誤的阿阿阿阿阿阿阿阿）。最後就是最外層的 $\\sigma$ 了。這就是我們今天要談的 activation function。大家應該知道 activation function 會提供神經網路模型非線性的特性，而在沒有非線性特性前的神經網路長什麼樣子呢？\n仿射？線性？\n其實是可以將仿射看成線性的。假設\n$$\nWx + b = \\begin{bmatrix}\nw_{11}&amp; w_{12}&amp; \\cdots&amp; w_{1n} \\\\\n\\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots \\\\\nw_{m1}&amp; w_{m2}&amp; \\cdots&amp; w_{mn} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\n\\vdots \\\\\nx_{n} \\\\\n\\end{bmatrix} + \\begin{bmatrix}\nb_{1} \\\\\n\\vdots \\\\\nb_{m} \\\\\n\\end{bmatrix}\n$$\n然後把 $W$ 跟 $b$ 合併，有點像高中學的增廣矩陣那樣，就變成了：\n$$\n= \\begin{bmatrix}\nw_{11}&amp; w_{12}&amp; \\cdots&amp; w_{1n}&amp; b_{1} \\\\\n\\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots&amp; \\vdots \\\\\nw_{m1}&amp; w_{m2}&amp; \\cdots&amp; w_{mn}&amp; b_{m} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_{1} \\\\\n\\vdots \\\\\nx_{n} \\\\\n1\n\\end{bmatrix} = W’x’\n$$\n這樣就可以把 $b$ 吸收到 $W$ 裡面，整體就變成線性的了。\n先談談線性轉換\n談 activation function 之前先要談談線性轉換。去除掉 activation function 後的神經網路層只剩下 $Wx + b$ 的部份，而如果講這部份看成線性，我們就可以用線性代數裡的東西來解釋它。\n有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。基本上，SVD 是一種矩陣分解的技巧，可以適用各式的矩陣（只要是可分解的）。SVD 可以告訴我們一些關於線性運算的特質。\n推薦可以看周老師的線代啟示錄 奇異值分解 (SVD)\n我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：","headline":"Activation function 到底怎麼影響模型？","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuehhua.github.io/2018/07/27/activation-function/"},"publisher":{"@type":"Organization","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd","logo":{"@type":"ImageObject","url":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"}},"url":"https://yuehhua.github.io/2018/07/27/activation-function/"}</script>
    <meta name="description" content="今天我們來談談 activation function 吧！ 前言 在神經網路模型（neural network）及深度學習（deep learning）中，activation function 一直扮演重要的角色。 網路模型中的層的概念基本上是由以下的式子構成： $$ \sigma(Wx + b) $$ 其中 $Wx$ 的矩陣相乘是大家在線性代數中常見的線性運算（linear operatio">
<meta property="og:type" content="blog">
<meta property="og:title" content="Activation function 到底怎麼影響模型？">
<meta property="og:url" content="https://yuehhua.github.io/2018/07/27/activation-function/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:description" content="今天我們來談談 activation function 吧！ 前言 在神經網路模型（neural network）及深度學習（deep learning）中，activation function 一直扮演重要的角色。 網路模型中的層的概念基本上是由以下的式子構成： $$ \sigma(Wx + b) $$ 其中 $Wx$ 的矩陣相乘是大家在線性代數中常見的線性運算（linear operatio">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300">
<meta property="og:image" content="https://yuehhua.github.io/images/activation1.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation2.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation3.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation4.png">
<meta property="og:image" content="https://yuehhua.github.io/images/activation5.png">
<meta property="og:image" content="https://yuehhua.github.io/images/ncku-statue.jpg">
<meta property="article:published_time" content="2018-07-27T03:02:54.000Z">
<meta property="article:modified_time" content="2019-07-08T09:10:27.484Z">
<meta property="article:author" content="Yueh-Hua Tu">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="topology">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">

    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/%20">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/%20"
                            
                            title="首頁"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首頁</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="分類"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分類</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="標籤"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">標籤</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="所有文章"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">所有文章</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="搜尋"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">搜尋</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="關於"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">關於</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Email">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Email</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="Atom"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Atom</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
    <article class="post">
        
                
                    <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            Activation function 到底怎麼影響模型？
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2018-07-27T11:02:54+08:00">
	
		    7月 27, 2018
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Deep-Learning/">Deep Learning</a>


    
</div>

    
</div>

                        
                            <div class="post-content markdown">
                                <div class="main-content-wrap">
                                    <p>今天我們來談談 activation function 吧！</p>
<h2 id="前言">前言</h2>
<p>在神經網路模型（neural network）及深度學習（deep learning）中，activation function 一直扮演重要的角色。</p>
<p>網路模型中的層的概念基本上是由以下的式子構成：</p>
<p>$$<br>
\sigma(Wx + b)<br>
$$</p>
<p>其中 $Wx$ 的矩陣相乘是大家在線性代數中常見的線性運算（linear operation），$Wx + b$ 嚴格來說是稱為仿射運算（affine operation）。相對應線性空間，仿射運算會構成仿射空間，不過兩者只差一個位移 $b$ 而已，但是大家都把它叫做線性（其實是錯誤的阿阿阿阿阿阿阿阿）。最後就是最外層的 $\sigma$ 了。這就是我們今天要談的 activation function。大家應該知道 activation function 會提供神經網路模型非線性的特性，而在沒有非線性特性前的神經網路長什麼樣子呢？</p>
<h2 id="仿射？線性？">仿射？線性？</h2>
<p>其實是可以將仿射看成線性的。假設</p>
<p>$$<br>
Wx + b = \begin{bmatrix}<br>
w_{11}&amp; w_{12}&amp; \cdots&amp; w_{1n} \\<br>
\vdots&amp; \vdots&amp; \ddots&amp; \vdots \\<br>
w_{m1}&amp; w_{m2}&amp; \cdots&amp; w_{mn} \\<br>
\end{bmatrix}<br>
\begin{bmatrix}<br>
x_{1} \\<br>
\vdots \\<br>
x_{n} \\<br>
\end{bmatrix} + \begin{bmatrix}<br>
b_{1} \\<br>
\vdots \\<br>
b_{m} \\<br>
\end{bmatrix}<br>
$$</p>
<p>然後把 $W$ 跟 $b$ 合併，有點像高中學的增廣矩陣那樣，就變成了：</p>
<p>$$<br>
= \begin{bmatrix}<br>
w_{11}&amp; w_{12}&amp; \cdots&amp; w_{1n}&amp; b_{1} \\<br>
\vdots&amp; \vdots&amp; \ddots&amp; \vdots&amp; \vdots \\<br>
w_{m1}&amp; w_{m2}&amp; \cdots&amp; w_{mn}&amp; b_{m} \\<br>
\end{bmatrix}<br>
\begin{bmatrix}<br>
x_{1} \\<br>
\vdots \\<br>
x_{n} \\<br>
1<br>
\end{bmatrix} = W’x’<br>
$$</p>
<p>這樣就可以把 $b$ 吸收到 $W$ 裡面，整體就變成線性的了。</p>
<h2 id="先談談線性轉換">先談談線性轉換</h2>
<p>談 activation function 之前先要談談線性轉換。去除掉 activation function 後的神經網路層只剩下 $Wx + b$ 的部份，而如果講這部份看成線性，我們就可以用線性代數裡的東西來解釋它。</p>
<p>有上到比較後面的線性代數的同學，應該有爬過 SVD 這座高山。基本上，SVD 是一種矩陣分解的技巧，可以適用各式的矩陣（只要是可分解的）。SVD 可以告訴我們一些關於線性運算的特質。</p>
<p>推薦可以看周老師的線代啟示錄 <a href="https://ccjou.wordpress.com/2009/09/01/%E5%A5%87%E7%95%B0%E5%80%BC%E5%88%86%E8%A7%A3-svd/" target="_blank" rel="noopener">奇異值分解 (SVD)</a></p>
<p>我們可以知道一個矩陣可以被看成線性轉換，而矩陣這個線性轉換可以被分解成 3 個矩陣：</p>
<a id="more"></a>
<p>$$<br>
A = U \Sigma V^T<br>
$$</p>
<p>這 3 個矩陣分別是有意義的，$U$、$\Sigma$、$V$ 的幾何意義分別是旋轉、伸縮、旋轉的意思。大家可以參考以下的圖：</p>
<p><img src="https://ccjou.files.wordpress.com/2009/09/svd32.jpg?w=400&amp;h=300" alt=""></p>
<blockquote>
<p>圖來自以上提到的文章</p>
</blockquote>
<p>所以我們可以知道矩陣能做的事情大概就是旋轉伸縮這些事情了。</p>
<p>在模型上，我們還會加上一個 bias 來達到平移的效果。</p>
<p>我們可以來造一些點：</p>
<p><img src="/images/activation1.png" alt=""></p>
<p>我們可以 random 造一個矩陣：</p>
<p>$$<br>
A =<br>
\begin{bmatrix}<br>
0.35166263&amp; 0.1124966 \\<br>
0.25249535&amp; 0.65481947 \\<br>
\end{bmatrix}<br>
$$</p>
<p>所以我們可以對每個點做運算 $Ax$：</p>
<p><img src="/images/activation2.png" alt=""></p>
<h2 id="去吧！Activation-function！">去吧！Activation function！</h2>
<p>接下來我們先來試試看在 CNN 用最多的 ReLU，我們把上面的點通過 ReLU 之後會發生什麼事呢？</p>
<p><img src="/images/activation3.png" alt=""></p>
<p>大家會發現只留下第一象限的點是沒有動到的，剩下的象限的點都被擠到 x 軸跟 y 軸上了。</p>
<p>所以在高維度的世界中，點都會保留第一象限不變，其他象限被擠壓到軸上。</p>
<p>那如果是用 sigmoid 的效果呢？</p>
<p><img src="/images/activation4.png" alt=""></p>
<p>他將所有的點都壓到 (0, 1) 之間，所以整個形狀就縮小很多。</p>
<p>我們放大來看看他整體形狀有沒有什麼變化。</p>
<p><img src="/images/activation5.png" alt=""></p>
<p>整體形狀有些微被扭曲了，不知道大家有沒有發現呢？</p>
<p>所以在引進 activation function 之後，模型擁有了 <strong>扭曲</strong> 的能力！</p>
<p>那麼 activation function 到底實際上做了什麼事呢？</p>
<h2 id="雕塑的工匠">雕塑的工匠</h2>
<p>以 ReLU 來說，他就像一個工匠正在雕塑一個作品。</p>
<p>ReLU 就是工匠手上那把彫刻刀，他會把第一象限以外的部份削掉。看起來就會像朱銘大師的作品這樣：</p>
<p><img src="/images/ncku-statue.jpg" alt=""></p>
<blockquote>
<p>圖來自 <a href="http://digitalarchives.artcenter.ncku.edu.tw/walkncku_cht/" target="_blank" rel="noopener">漫遊‧藝術網＠成大校園</a></p>
</blockquote>
<p>原諒我私心用成大的雕塑品當範例。XD</p>
<p>ReLU 會將不重要的部份削掉，剩下重要的特徵接續後面的特徵萃取。</p>
<h2 id="消失的梯度">消失的梯度</h2>
<p>以 sigmoid 來說，他將點壓到 (0, 1) 之間看似很難以理解。</p>
<p>其實這個 activation function 在影像辨識當中比較不是主流的方法，可能不是那麼適用，不過在 NLP 領域算是還蠻常用的方法。</p>
<p>那如果放在 CNN 的話，就會發生梯度消失的問題。</p>
<p>在比較早期的時候，大家在影像處理上都遇到梯度消失的問題。如果直觀上看來，如果每過一次 convolution layer 就會被壓縮到 (0, 1) 一次，那麼後面再接 subsampling 的處理，又會縮小一次，並且失去某些訊息，想當然爾特徵就在不斷縮小的過程中慢慢不見了。這樣的效果讓早期的模型無法變得更深。</p>
<p>讓 convolution layer 去篩選哪些 feature 該留下來，讓 subsampling layer 去做縮小的動作，各自負責各自的功能，這樣看來是比較好的作法。</p>
<p>這是一個比較直觀的解釋方式，歡迎大家提出不同的看法。</p>
<h2 id="模型怎麼知道要從哪裡下刀？">模型怎麼知道要從哪裡下刀？</h2>
<p>我想蠻多人應該會有跟我一樣的問題。</p>
<p>我知道如果用 ReLU 可以把不要的部份削掉，那麼我怎麼知道要削哪裡？</p>
<p>答案是 backpropagation (gradient descent method) 會告訴你！</p>
<p>藉由 forward 將訊息傳遞到 output layer，backward 所回饋的 gradient 正提供一個訊息，這個訊息會告訴模型要怎麼調整線性轉換的矩陣，來讓 ReLU 可以切在對的位置。</p>
<h2 id="為什麼-activation-function-一定要長這樣？">為什麼 activation function 一定要長這樣？</h2>
<p>其實沒有規定 activation function 一定要長怎樣。</p>
<p>但是拿多項式函數或是其他函數會讓整個模型非常難以理解，而且當中的參數還不少。</p>
<p>所以依照 Occam’s razor 的原則，我們先拿簡單的函數來用會比較好。</p>
<h2 id="結論">結論</h2>
<p>不同的模型跟問題其實適用不同的 activation function。</p>
<p>像影像辨識中，CNN 的設計是要先做 feature extraction，再進行分類。</p>
<p>所以在 feature extraction 的階段就需要找適用的 activation function。</p>
<p>那像在 NLP 領域，他們關心的是機率分佈，而 sigmoid 就很合機率分佈的調調。</p>
<p>這邊原諒我草率帶過 NLP 的部份，今天的主軸都擺在 CNN 上。XD</p>
<p>今天就到這邊告一個段落囉。</p>

                                        

                                </div>
                            </div>
                            <div id="post-footer" class="post-footer main-content-wrap">
                                
                                        
                                            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/08/03/the-nutshell-of-ai/" data-tooltip="AI 的核心" aria-label="上一篇: AI 的核心">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/07/25/10-basis-for-topology/" data-tooltip="Basis for topology" aria-label="下一篇: Basis for topology">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/07/27/activation-function/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                                                
                                                    
                                                        
                                                            <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
</div>
                                                                
                                                                            
                            </div>
                            
                                <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
                                <script>
                                    if (window.mermaid) {
                                        mermaid.initialize({ theme: 'forest' });
                                    }
                                </script>
                                
    </article>

                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2021 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/08/03/the-nutshell-of-ai/" data-tooltip="AI 的核心" aria-label="上一篇: AI 的核心">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2018/07/25/10-basis-for-topology/" data-tooltip="Basis for topology" aria-label="下一篇: Basis for topology">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/07/27/activation-function/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="5">
    <i id="btn-close-shareoptions" class="fa fa-close"></i>
    <ul class="share-options">
        
            
            
            <li class="share-option">
                <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2018/07/27/activation-function/">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i><span>分享到 Facebook</span>
                </a>
            </li>
        
    </ul>
</div>

            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>目標是計算生物學家！</br>Systems Biology, Computational Biology, Machine Learning</br>Julia Taiwan 發起人</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>研發替代役研究助理</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>

<!--SCRIPTS END-->


<script>
    var disqus_config = function () {
        this.page.url = 'https://yuehhua.github.io/2018/07/27/activation-function/';
                 
            this.page.identifier = '2018/07/27/activation-function/';
                 
             };
    (function () {
        var d = document, s = d.createElement('script');
        var disqus_shortname = 'dream-maker';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/latest.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



    </body>
</html>
