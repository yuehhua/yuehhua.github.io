
<!DOCTYPE html>
<html lang="zh-tw,en,default">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>神經微分方程（Neural Ordinary Differential Equations） - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"},"articleBody":"這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。\n核心觀念\n概念上來說，就是將神經網路離散的層觀念打破，將他貫通成為連續的層的網路架構。\n連續和離散的差別來自於倒傳遞的過程：\n$$\n\\mathbb{y}_{t+1} = \\mathbb{y}_t - \\eta \\nabla \\mathcal{L}\n$$\n其中 $\\nabla \\mathcal{L}$ 就是梯度的部份，是向量的，然而我們把他簡化成純量來看的話，他不過就是\n$$\n\\frac{d \\mathcal{L}}{dt}\n$$\n廣義上來說，一個函數的微分，如果是離散的版本就會是\n$$\n\\frac{dy}{dt} = \\frac{y(t + \\Delta) - y(t)}{\\Delta}\n$$\n如此一來，所形成的方程式就會是差分方程，然而連續的版本就是\n$$\n\\frac{dy}{dt} = \\lim_{\\Delta \\rightarrow 0} \\frac{y(t + \\Delta) - y(t)}{\\Delta}\n$$\n這個所形成的會是微分方程。\n從離散到連續\n我們可以從離散的版本\n$$\n\\frac{dy}{dt} = \\frac{y(t + \\Delta) - y(t)}{\\Delta}\n$$\n把他轉成以下的樣貌\n$$\ny(t + \\Delta) = y(t) + \\Delta \\frac{dy}{dt}\n$$\n要將他貫通的話，我們就得由從神經網路的基礎開始，如果是一般的前回饋網路（feed-forward network）當中的隱藏層是像下列這個樣子：\n$$\nh_{t+1} = f(h_t, \\theta)\n$$\n我們可以發現像是 ResNet 這類的網路有 skip connection 的設置，所以跟一般的前回饋網路不同\n$$\nh_{t+1} = h_t + f(h_t, \\theta)\n$$\n而 RNN 等等有序列概念的模型也有類似的結構，就是會是前一層的結果加上通過 $f$ 運算後的結果，成為下一層的結果。\n這樣的形式跟我們前面提到的形式不謀而合\n$$\ny(t + \\Delta) = y(t) + \\Delta \\frac{dy}{dt}\n$$\n只要我們把 $\\Delta = 1$ 代入，就成了\n$$\ny(t+1) = y(t) + \\frac{dy}{dt}\n$$\n以下給大家比對一下\n$$\nh_{t+1} = h_t + f(h_t, \\theta) \\\\\ny(t+1) = y(t) + \\frac{dy}{dt}\n$$\n也就是，我們可以讓\n$$\n\\frac{dy}{dt} = f(h_t, \\theta)\n$$\n神奇的事情就發生了！神經網路 $f$ 就可以被我們拿來計算微分 $\\frac{dy}{dt}$！\n比較精確的說法是，把神經網路的層 $f$ 拿來逼近微分項，或是說梯度。這樣我們後面就可以用數值方法來逼近解。\n$$\ny(t + \\Delta) = y(t) + \\Delta \\frac{dy}{dt} \\\\\n\\downarrow \\\\\ny(t + \\Delta) = y(t) + \\Delta f(t, h(t), \\theta_t)\n$$\n要拉成連續的還有一個重要的手段，就是將不同的層 $t$ 從離散的變成連續的，所以作者將 $t$ 做了參數化，將他變成 $f$ 的參數之一，如此一來，就可以在任意的層中放入資料做運算。\n最重要的概念導出了這樣的式子\n$$\nh(t) \\rightarrow \\frac{dy(t)}{dt} = f(h(t), t, \\theta) \\rightarrow y(t)\n$$\n神經網路作為一個系統的微分形式\n在傳統科學或是工程領域，我們會以微分式來表達以及建構一個系統。\n$$\n\\nu = \\frac{dx}{dt} = t + 1\n$$\n其實在這邊是一樣的道理，整體來說，我們是換成用神經網路去描述一個微分式，其實本質上就是這樣。\n原本的層的概念就是用數學函數來建立的，而層與層之間傳遞著計算的結果。\n$$\n\\mathbb{h_1} = \\sigma(W_1 \\mathbb{x} + \\mathbb{b_1}) \\\\\n\\mathbb{y} = \\sigma(W_2 \\mathbb{h_1} + \\mathbb{b_2})\n$$\n然而變成連續之後，我們等於是用神經網路中的層去建立跟描繪微分形式。\n$$\n\\frac{d h(t)}{dt} = \\sigma(W(t) \\mathbb{x}(t) + \\mathbb{b(t)}) \\\\\n\\frac{d y(t)}{dt} = \\sigma(W(t) \\mathbb{h}(t) + \\mathbb{b(t)})\n$$\n是不是跟如出一轍呢？\n$$\n\\frac{dy(t)}{dt} = f(h(t), t, \\theta)\n$$\n向前傳遞解微分式\n我們可以來計算看看隱藏層是長什麼樣子的。在隱藏層的微分式中，也是利用隱藏層去計算出來的。\n$$\n\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\n$$\n基本上，我們只要對上式做積分就可以了。\n$$\nh(t) = \\int f(h(t), t, \\theta) dt\n$$\n這是一個怎樣的概念呢？我們可以來看看下圖。\n\n我們做積分這件事其實是用 $h(t_0)$ 來推斷 $h(t_1)$ 的，這跟神經網路的向前傳遞是一樣的行為。\n$$\nh(t_1) = F(h(t), t, \\theta) \\bigg|_{t=t_0}\n$$\n這樣的積分動作，我們可以用 $t_0$ 時間點的資訊來解 $h(t_1)$。\n這樣的解法在程式上就會交由 ODE Solver 去處理。\n$$\nh(t_1) = ODESolve(h(t_0), t_0, t_1, \\theta, f)\n$$\n反向傳遞解函數\n$$\n\\mathcal{L}(t_0, t, \\theta) = \\mathcal{L}(ODESolve(\\cdot))\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h(t)} = -a(t)\n$$\nadjoint state\n$$\na(t) = \\int -a(t)^T \\frac{\\partial f}{\\partial h} dt = - \\frac{\\partial \\mathcal{L}}{\\partial h(t)}\n$$\n$$\na(t) = \\int_{t_1}^{t_0} -a(t)^T \\frac{\\partial f(h(t), t, \\theta)}{\\partial h(t)} dt\n$$\n\n擴充狀態（augmented state）\n$\\frac{d \\theta}{dt} = 0$\n$\\frac{dt}{dt} = 1$\nlet $\\begin{bmatrix}\nh \\\\\n\\theta \\\\\nt\n\\end{bmatrix}$ be a augmented state\naugmented state function:\n$$\nf_{aug}(\\begin{bmatrix}\nh \\\\\n\\theta \\\\\nt\n\\end{bmatrix}) =\n\\begin{bmatrix}\nf(h(t), t, \\theta) \\\\\n0 \\\\\n1\n\\end{bmatrix}\n$$\naugmented state dynamics:\n$$\n\\frac{d}{dt}\n\\begin{bmatrix}\nh \\\\\n\\theta \\\\\nt\n\\end{bmatrix}\nf_{aug}(\n\\begin{bmatrix}\nh \\\\\n\\theta \\\\\nt\n\\end{bmatrix})\n$$\naugmented adjoint state:\n$$\n\\begin{bmatrix}\na \\\\\na_{\\theta} \\\\\na_t\n\\end{bmatrix}\n$$\n$a = \\frac{\\partial \\mathcal{L}}{\\partial h}$\n$a_{\\theta} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta}$\n$a_t = \\frac{\\partial \\mathcal{L}}{\\partial t}$\n$$\n\\frac{d a_{aug}}{dt} = -\n\\begin{bmatrix}\na \\frac{\\partial f}{\\partial h} \\\\\na \\frac{\\partial f}{\\partial \\theta} \\\\\na \\frac{\\partial f}{\\partial t}\n\\end{bmatrix}\n$$\n","dateCreated":"2019-02-24T15:47:10+08:00","dateModified":"2019-02-26T00:18:15+08:00","datePublished":"2019-02-24T15:47:10+08:00","description":"","headline":"神經微分方程（Neural Ordinary Differential Equations）","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuehhua.github.io/2019/02/24/neural-ode/"},"publisher":{"@type":"Organization","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd","logo":{"@type":"ImageObject","url":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"}},"url":"https://yuehhua.github.io/2019/02/24/neural-ode/"}</script>
    <meta name="description" content="這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。 核心觀念 概念上來說，就是將神經網路離散的層觀念打破，將他貫通成為連續的層的網路架構。 連續和離散的差別來自於倒傳遞的過程： $$ \mathbb{y}_{t+1} &#x3D; \mathbb{y}_t">
<meta property="og:type" content="blog">
<meta property="og:title" content="神經微分方程（Neural Ordinary Differential Equations）">
<meta property="og:url" content="https://yuehhua.github.io/2019/02/24/neural-ode/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:description" content="這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。 核心觀念 概念上來說，就是將神經網路離散的層觀念打破，將他貫通成為連續的層的網路架構。 連續和離散的差別來自於倒傳遞的過程： $$ \mathbb{y}_{t+1} &#x3D; \mathbb{y}_t">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://yuehhua.github.io/images/hidden-state1.svg">
<meta property="og:image" content="https://yuehhua.github.io/images/adjoint-state.svg">
<meta property="article:published_time" content="2019-02-24T07:47:10.000Z">
<meta property="article:modified_time" content="2019-02-25T16:18:15.950Z">
<meta property="article:author" content="Yueh-Hua Tu">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="topology">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yuehhua.github.io/images/hidden-state1.svg">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">

    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/%20">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/%20"
                            
                            title="首頁"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首頁</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="分類"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分類</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="標籤"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">標籤</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="所有文章"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">所有文章</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="搜尋"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">搜尋</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="關於"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">關於</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Email">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Email</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="Atom"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Atom</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
    <article class="post">
        
                
                    <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            神經微分方程（Neural Ordinary Differential Equations）
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2019-02-24T15:47:10+08:00">
	
		    2月 24, 2019
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>


    
</div>

    
</div>

                        
                            <div class="post-content markdown">
                                <div class="main-content-wrap">
                                    <p>這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。</p>
<h2 id="核心觀念">核心觀念</h2>
<p>概念上來說，就是將神經網路<strong>離散的層</strong>觀念打破，將他貫通成為<strong>連續的層</strong>的網路架構。</p>
<p>連續和離散的差別來自於倒傳遞的過程：</p>
<p>$$<br>
\mathbb{y}_{t+1} = \mathbb{y}_t - \eta \nabla \mathcal{L}<br>
$$</p>
<p>其中 $\nabla \mathcal{L}$ 就是梯度的部份，是向量的，然而我們把他簡化成純量來看的話，他不過就是</p>
<p>$$<br>
\frac{d \mathcal{L}}{dt}<br>
$$</p>
<p>廣義上來說，一個函數的微分，如果是離散的版本就會是</p>
<p>$$<br>
\frac{dy}{dt} = \frac{y(t + \Delta) - y(t)}{\Delta}<br>
$$</p>
<p>如此一來，所形成的方程式就會是差分方程，然而連續的版本就是</p>
<p>$$<br>
\frac{dy}{dt} = \lim_{\Delta \rightarrow 0} \frac{y(t + \Delta) - y(t)}{\Delta}<br>
$$</p>
<p>這個所形成的會是微分方程。</p>
<h2 id="從離散到連續">從離散到連續</h2>
<p>我們可以從離散的版本</p>
<p>$$<br>
\frac{dy}{dt} = \frac{y(t + \Delta) - y(t)}{\Delta}<br>
$$</p>
<p>把他轉成以下的樣貌</p>
<p>$$<br>
y(t + \Delta) = y(t) + \Delta \frac{dy}{dt}<br>
$$</p>
<p>要將他貫通的話，我們就得由從神經網路的基礎開始，如果是一般的前回饋網路（feed-forward network）當中的隱藏層是像下列這個樣子：</p>
<p>$$<br>
h_{t+1} = f(h_t, \theta)<br>
$$</p>
<p>我們可以發現像是 ResNet 這類的網路有 skip connection 的設置，所以跟一般的前回饋網路不同</p>
<p>$$<br>
h_{t+1} = h_t + f(h_t, \theta)<br>
$$</p>
<p>而 RNN 等等有序列概念的模型也有類似的結構，就是會是前一層的結果加上通過 $f$ 運算後的結果，成為下一層的結果。</p>
<p>這樣的形式跟我們前面提到的形式不謀而合</p>
<p>$$<br>
y(t + \Delta) = y(t) + \Delta \frac{dy}{dt}<br>
$$</p>
<p>只要我們把 $\Delta = 1$ 代入，就成了</p>
<p>$$<br>
y(t+1) = y(t) + \frac{dy}{dt}<br>
$$</p>
<p>以下給大家比對一下</p>
<p>$$<br>
h_{t+1} = h_t + f(h_t, \theta) \\<br>
y(t+1) = y(t) + \frac{dy}{dt}<br>
$$</p>
<p>也就是，我們可以讓</p>
<p>$$<br>
\frac{dy}{dt} = f(h_t, \theta)<br>
$$</p>
<p>神奇的事情就發生了！神經網路 $f$ 就可以被我們拿來計算微分 $\frac{dy}{dt}$！</p>
<p>比較精確的說法是，把神經網路的層 $f$ 拿來逼近微分項，或是說梯度。這樣我們後面就可以用數值方法來逼近解。</p>
<p>$$<br>
y(t + \Delta) = y(t) + \Delta \frac{dy}{dt} \\<br>
\downarrow \\<br>
y(t + \Delta) = y(t) + \Delta f(t, h(t), \theta_t)<br>
$$</p>
<p>要拉成連續的還有一個重要的手段，就是將不同的層 $t$ 從離散的變成連續的，所以作者將 $t$ 做了參數化，將他變成 $f$ 的參數之一，如此一來，就可以在任意的層中放入資料做運算。</p>
<p>最重要的概念導出了這樣的式子</p>
<p>$$<br>
h(t) \rightarrow \frac{dy(t)}{dt} = f(h(t), t, \theta) \rightarrow y(t)<br>
$$</p>
<h2 id="神經網路作為一個系統的微分形式">神經網路作為一個系統的微分形式</h2>
<p>在傳統科學或是工程領域，我們會以微分式來表達以及建構一個系統。</p>
<p>$$<br>
\nu = \frac{dx}{dt} = t + 1<br>
$$</p>
<p>其實在這邊是一樣的道理，整體來說，我們是換成用神經網路去描述一個微分式，其實本質上就是這樣。</p>
<p>原本的層的概念就是用數學函數來建立的，而層與層之間傳遞著計算的結果。</p>
<p>$$<br>
\mathbb{h_1} = \sigma(W_1 \mathbb{x} + \mathbb{b_1}) \\<br>
\mathbb{y} = \sigma(W_2 \mathbb{h_1} + \mathbb{b_2})<br>
$$</p>
<p>然而變成連續之後，我們等於是用神經網路中的層去建立跟描繪微分形式。</p>
<p>$$<br>
\frac{d h(t)}{dt} = \sigma(W(t) \mathbb{x}(t) + \mathbb{b(t)}) \\<br>
\frac{d y(t)}{dt} = \sigma(W(t) \mathbb{h}(t) + \mathbb{b(t)})<br>
$$</p>
<p>是不是跟如出一轍呢？</p>
<p>$$<br>
\frac{dy(t)}{dt} = f(h(t), t, \theta)<br>
$$</p>
<h2 id="向前傳遞解微分式">向前傳遞解微分式</h2>
<p>我們可以來計算看看隱藏層是長什麼樣子的。在隱藏層的微分式中，也是利用隱藏層去計算出來的。</p>
<p>$$<br>
\frac{dh(t)}{dt} = f(h(t), t, \theta)<br>
$$</p>
<p>基本上，我們只要對上式做積分就可以了。</p>
<p>$$<br>
h(t) = \int f(h(t), t, \theta) dt<br>
$$</p>
<p>這是一個怎樣的概念呢？我們可以來看看下圖。</p>
<p><img src="/images/hidden-state1.svg" alt=""></p>
<p>我們做積分這件事其實是用 $h(t_0)$ 來推斷 $h(t_1)$ 的，這跟神經網路的向前傳遞是一樣的行為。</p>
<p>$$<br>
h(t_1) = F(h(t), t, \theta) \bigg|_{t=t_0}<br>
$$</p>
<p>這樣的積分動作，我們可以用 $t_0$ 時間點的資訊來解 $h(t_1)$。</p>
<p>這樣的解法在程式上就會交由 ODE Solver 去處理。</p>
<p>$$<br>
h(t_1) = ODESolve(h(t_0), t_0, t_1, \theta, f)<br>
$$</p>
<h2 id="反向傳遞解函數">反向傳遞解函數</h2>
<p>$$<br>
\mathcal{L}(t_0, t, \theta) = \mathcal{L}(ODESolve(\cdot))<br>
$$</p>
<p>$$<br>
\frac{\partial \mathcal{L}}{\partial h(t)} = -a(t)<br>
$$</p>
<p>adjoint state</p>
<p>$$<br>
a(t) = \int -a(t)^T \frac{\partial f}{\partial h} dt = - \frac{\partial \mathcal{L}}{\partial h(t)}<br>
$$</p>
<p>$$<br>
a(t) = \int_{t_1}^{t_0} -a(t)^T \frac{\partial f(h(t), t, \theta)}{\partial h(t)} dt<br>
$$</p>
<p><img src="/images/adjoint-state.svg" alt=""></p>
<h2 id="擴充狀態（augmented-state）">擴充狀態（augmented state）</h2>
<p>$\frac{d \theta}{dt} = 0$</p>
<p>$\frac{dt}{dt} = 1$</p>
<p>let $\begin{bmatrix}<br>
h \\<br>
\theta \\<br>
t<br>
\end{bmatrix}$ be a augmented state</p>
<p>augmented state function:</p>
<p>$$<br>
f_{aug}(\begin{bmatrix}<br>
h \\<br>
\theta \\<br>
t<br>
\end{bmatrix}) =<br>
\begin{bmatrix}<br>
f(h(t), t, \theta) \\<br>
0 \\<br>
1<br>
\end{bmatrix}<br>
$$</p>
<p>augmented state dynamics:</p>
<h1>$$<br>
\frac{d}{dt}<br>
\begin{bmatrix}<br>
h \\<br>
\theta \\<br>
t<br>
\end{bmatrix}</h1>
<p>f_{aug}(<br>
\begin{bmatrix}<br>
h \\<br>
\theta \\<br>
t<br>
\end{bmatrix})<br>
$$</p>
<p>augmented adjoint state:</p>
<p>$$<br>
\begin{bmatrix}<br>
a \\<br>
a_{\theta} \\<br>
a_t<br>
\end{bmatrix}<br>
$$</p>
<p>$a = \frac{\partial \mathcal{L}}{\partial h}$</p>
<p>$a_{\theta} = \frac{\partial \mathcal{L}}{\partial \theta}$</p>
<p>$a_t = \frac{\partial \mathcal{L}}{\partial t}$</p>
<p>$$<br>
\frac{d a_{aug}}{dt} = -<br>
\begin{bmatrix}<br>
a \frac{\partial f}{\partial h} \\<br>
a \frac{\partial f}{\partial \theta} \\<br>
a \frac{\partial f}{\partial t}<br>
\end{bmatrix}<br>
$$</p>

                                        

                                </div>
                            </div>
                            <div id="post-footer" class="post-footer main-content-wrap">
                                
                                        
                                            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/03/01/money-is-the-cheapest-expression/" data-tooltip="金錢是最廉價的表達" aria-label="上一篇: 金錢是最廉價的表達">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/02/24/note-of-vector-space-to-function-space/" data-tooltip="筆記 - 從向量空間到函數空間" aria-label="下一篇: 筆記 - 從向量空間到函數空間">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2019/02/24/neural-ode/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                                                
                                                    
                                                        
                                                            <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
</div>
                                                                
                                                                            
                            </div>
                            
                                <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
                                <script>
                                    if (window.mermaid) {
                                        mermaid.initialize({ theme: 'forest' });
                                    }
                                </script>
                                
    </article>

                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2020 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/03/01/money-is-the-cheapest-expression/" data-tooltip="金錢是最廉價的表達" aria-label="上一篇: 金錢是最廉價的表達">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/02/24/note-of-vector-space-to-function-space/" data-tooltip="筆記 - 從向量空間到函數空間" aria-label="下一篇: 筆記 - 從向量空間到函數空間">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2019/02/24/neural-ode/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="5">
    <i id="btn-close-shareoptions" class="fa fa-close"></i>
    <ul class="share-options">
        
            
            
            <li class="share-option">
                <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2019/02/24/neural-ode/">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i><span>分享到 Facebook</span>
                </a>
            </li>
        
    </ul>
</div>

            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>目標是計算生物學家！</br>Systems Biology, Computational Biology, Machine Learning</br>Julia Taiwan 發起人</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>研發替代役研究助理</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->

<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>

<!--SCRIPTS END-->


<script>
    var disqus_config = function () {
        this.page.url = 'https://yuehhua.github.io/2019/02/24/neural-ode/';
                 
            this.page.identifier = '2019/02/24/neural-ode/';
                 
             };
    (function () {
        var d = document, s = d.createElement('script');
        var disqus_shortname = 'dream-maker';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/latest.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



    </body>
</html>
