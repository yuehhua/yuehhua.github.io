
<!DOCTYPE html>
<html lang="zh-tw,en,default">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Dream Maker">
    <title>神經微分方程（Neural Ordinary Differential Equations） - Dream Maker</title>
    <meta name="author" content="Yueh-Hua Tu">
    
        <meta name="keywords" content="machine learning,deep learning,topology,">
    
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"},"articleBody":"這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。\n核心觀念概念上來說，就是將神經網路離散的層觀念打破，將他貫通成為連續的層的網路架構。\n連續和離散的差別來自於倒傳遞的過程：\n$$\\mathbb{y}_{t+1} = \\mathbb{y}_t - \\eta \\nabla \\mathcal{L}$$\n其中 $\\nabla \\mathcal{L}$ 就是梯度的部份，是向量的，然而我們把他簡化成純量來看的話，他不過就是\n$$\\frac{d \\mathcal{L}}{dt}$$\n廣義上來說，一個函數的微分，如果是離散的版本就會是\n$$\\frac{dy}{dt} = \\frac{y(t + \\Delta) - y(t)}{\\Delta}$$\n如此一來，所形成的方程式就會是差分方程，然而連續的版本就是\n$$\\frac{dy}{dt} = \\lim_{\\Delta \\rightarrow 0} \\frac{y(t + \\Delta) - y(t)}{\\Delta}$$\n這個所形成的會是微分方程。\n從離散到連續我們可以從離散的版本\n$$\\frac{dy}{dt} = \\frac{y(t + \\Delta) - y(t)}{\\Delta}$$\n把他轉成以下的樣貌\n$$y(t + \\Delta) = y(t) + \\Delta \\frac{dy}{dt}$$\n要將他貫通的話，我們就得由從神經網路的基礎開始，如果是一般的前回饋網路（feed-forward network）當中的隱藏層是像下列這個樣子：\n$$h_{t+1} = f(h_t, \\theta)$$\n我們可以發現像是 ResNet 這類的網路有 skip connection 的設置，所以跟一般的前回饋網路不同\n$$h_{t+1} = h_t + f(h_t, \\theta)$$\n而 RNN 等等有序列概念的模型也有類似的結構，就是會是前一層的結果加上通過 $f$ 運算後的結果，成為下一層的結果。\n這樣的形式跟我們前面提到的形式不謀而合\n$$y(t + \\Delta) = y(t) + \\Delta \\frac{dy}{dt}$$\n只要我們把 $\\Delta = 1$ 代入，就成了\n$$y(t+1) = y(t) + \\frac{dy}{dt}$$\n以下給大家比對一下\n$$h_{t+1} = h_t + f(h_t, \\theta) \\\\y(t+1) = y(t) + \\frac{dy}{dt}$$\n也就是，我們可以讓\n$$\\frac{dy}{dt} = f(h_t, \\theta)$$\n神奇的事情就發生了！神經網路 $f$ 就可以被我們拿來計算微分 $\\frac{dy}{dt}$！\n比較精確的說法是，把神經網路的層 $f$ 拿來逼近微分項，或是說梯度。這樣我們後面就可以用數值方法來逼近解。\n$$y(t + \\Delta) = y(t) + \\Delta \\frac{dy}{dt} \\\\\\downarrow \\\\y(t + \\Delta) = y(t) + \\Delta f(t, h(t), \\theta_t)$$\n要拉成連續的還有一個重要的手段，就是將不同的層 $t$ 從離散的變成連續的，所以作者將 $t$ 做了參數化，將他變成 $f$ 的參數之一，如此一來，就可以在任意的層中放入資料做運算。\n最重要的概念導出了這樣的式子\n$$h(t) \\rightarrow \\frac{dy(t)}{dt} = f(h(t), t, \\theta) \\rightarrow y(t)$$\n神經網路作為一個系統的微分形式在傳統科學或是工程領域，我們會以微分式來表達以及建構一個系統。\n$$\\nu = \\frac{dx}{dt} = t + 1$$\n其實在這邊是一樣的道理，整體來說，我們是換成用神經網路去描述一個微分式，其實本質上就是這樣。\n原本的層的概念就是用數學函數來建立的，而層與層之間傳遞著計算的結果。\n$$\\mathbb{h_1} = \\sigma(W_1 \\mathbb{x} + \\mathbb{b_1}) \\\\\\mathbb{y} = \\sigma(W_2 \\mathbb{h_1} + \\mathbb{b_2})$$\n然而變成連續之後，我們等於是用神經網路中的層去建立跟描繪微分形式。\n$$\\frac{d h_1(t)}{dt} = \\sigma(W_1 \\mathbb{x}(t) + \\mathbb{b_1}) \\\\\\frac{d y(t)}{dt} = \\sigma(W_2 \\mathbb{h_1}(t) + \\mathbb{b_2})$$\n是不是跟如出一轍呢？\n$$\\frac{dy(t)}{dt} = f(h(t), t, \\theta)$$\n","dateCreated":"2019-02-24T15:47:10+08:00","dateModified":"2019-02-24T15:47:10+08:00","datePublished":"2019-02-24T15:47:10+08:00","description":"","headline":"神經微分方程（Neural Ordinary Differential Equations）","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://yuehhua.github.io/2019/02/24/neural-ode/"},"publisher":{"@type":"Organization","name":"Yueh-Hua Tu","sameAs":["https://github.com/yuehhua/","https://www.facebook.com/a504082002","https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/","mailto:a504082002@gmail.com"],"image":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd","logo":{"@type":"ImageObject","url":"https://www.gravatar.com/avatar/a565749fba4782b717234da670b273fd"}},"url":"https://yuehhua.github.io/2019/02/24/neural-ode/"}</script>
    <meta name="description" content="這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。 核心觀念概念上來說，就是將神經網路離散的層觀念打破，將他貫通成為連續的層的網路架構。 連續和離散的差別來自於倒傳遞的過程： $$\mathbb{y}_{t+1} = \mathbb{y}_t -">
<meta name="keywords" content="machine learning,deep learning,topology">
<meta property="og:type" content="blog">
<meta property="og:title" content="神經微分方程（Neural Ordinary Differential Equations）">
<meta property="og:url" content="https://yuehhua.github.io/2019/02/24/neural-ode/index.html">
<meta property="og:site_name" content="Dream Maker">
<meta property="og:description" content="這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。 核心觀念概念上來說，就是將神經網路離散的層觀念打破，將他貫通成為連續的層的網路架構。 連續和離散的差別來自於倒傳遞的過程： $$\mathbb{y}_{t+1} = \mathbb{y}_t -">
<meta property="og:locale" content="zh-tw">
<meta property="og:updated_time" content="2019-02-24T07:47:10.209Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神經微分方程（Neural Ordinary Differential Equations）">
<meta name="twitter:description" content="這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。 核心觀念概念上來說，就是將神經網路離散的層觀念打破，將他貫通成為連續的層的網路架構。 連續和離散的差別來自於倒傳遞的過程： $$\mathbb{y}_{t+1} = \mathbb{y}_t -">
    
    
        
    
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-3frockyt2j28isvdztjchy5nhkz8tjki9ermufc1ckptmvjdftux94m2ahub.min.css">
    <!--STYLES END-->
    
    <script type="text/javascript">
        (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-119690895-1', 'auto');
        ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    

<header id="header" data-behavior="5">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="/ ">Dream Maker</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
        </a>
    
</header>

            <!-- Define author's picture -->


<nav id="sidebar" data-behavior="5">
    <div class="sidebar-container">
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                            title="首頁"
                        >
                    
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首頁</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                            title="分類"
                        >
                    
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分類</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                            title="標籤"
                        >
                    
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">標籤</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                            title="所有文章"
                        >
                    
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">所有文章</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="搜尋"
                        >
                    
                        <i class="sidebar-button-icon fa fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">搜尋</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="關於"
                        >
                    
                        <i class="sidebar-button-icon fa fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">關於</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/yuehhua/" target="_blank" rel="noopener" title="GitHub">
                    
                        <i class="sidebar-button-icon fab fa-github" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.facebook.com/a504082002" target="_blank" rel="noopener" title="Facebook">
                    
                        <i class="sidebar-button-icon fab fa-facebook" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Facebook</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://www.linkedin.com/in/%E5%B2%B3%E8%8F%AF-%E6%9D%9C-6a3995a0/" target="_blank" rel="noopener" title="LinkedIn">
                    
                        <i class="sidebar-button-icon fab fa-linkedin" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">LinkedIn</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="mailto:a504082002@gmail.com" target="_blank" rel="noopener" title="Email">
                    
                        <i class="sidebar-button-icon fab fa-envelope" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Email</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/atom.xml"
                            
                            title="Atom"
                        >
                    
                        <i class="sidebar-button-icon fa fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Atom</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="5"
                 class="
                        hasCoverMetaIn
                        ">
                
    <article class="post">
        
                
                    <div class="post-header main-content-wrap text-left">
    
        <h1 class="post-title">
            神經微分方程（Neural Ordinary Differential Equations）
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2019-02-24T15:47:10+08:00">
	
		    2月 24, 2019
    	
    </time>
    
        <span>分類 </span>
        
    <a class="category-link" href="/categories/Machine-Learning/">Machine Learning</a>


    
</div>

    
</div>

                        
                            <div class="post-content markdown">
                                <div class="main-content-wrap">
                                    <p>這篇被選為 NeurIPS 2018 最佳論文，他將連續的概念帶入了神經網路架構中，並且善用以往解微分方程的方法來做逼近，可以做到跟原方法（倒傳遞）一樣好的程度，而參數使用複雜度卻是常數，更短的訓練時間。</p>
<h2 id="核心觀念"><a href="#核心觀念" class="headerlink" title="核心觀念"></a>核心觀念</h2><p>概念上來說，就是將神經網路<strong>離散的層</strong>觀念打破，將他貫通成為<strong>連續的層</strong>的網路架構。</p>
<p>連續和離散的差別來自於倒傳遞的過程：</p>
<p>$$<br>\mathbb{y}_{t+1} = \mathbb{y}_t - \eta \nabla \mathcal{L}<br>$$</p>
<p>其中 $\nabla \mathcal{L}$ 就是梯度的部份，是向量的，然而我們把他簡化成純量來看的話，他不過就是</p>
<p>$$<br>\frac{d \mathcal{L}}{dt}<br>$$</p>
<p>廣義上來說，一個函數的微分，如果是離散的版本就會是</p>
<p>$$<br>\frac{dy}{dt} = \frac{y(t + \Delta) - y(t)}{\Delta}<br>$$</p>
<p>如此一來，所形成的方程式就會是差分方程，然而連續的版本就是</p>
<p>$$<br>\frac{dy}{dt} = \lim_{\Delta \rightarrow 0} \frac{y(t + \Delta) - y(t)}{\Delta}<br>$$</p>
<p>這個所形成的會是微分方程。</p>
<h2 id="從離散到連續"><a href="#從離散到連續" class="headerlink" title="從離散到連續"></a>從離散到連續</h2><p>我們可以從離散的版本</p>
<p>$$<br>\frac{dy}{dt} = \frac{y(t + \Delta) - y(t)}{\Delta}<br>$$</p>
<p>把他轉成以下的樣貌</p>
<p>$$<br>y(t + \Delta) = y(t) + \Delta \frac{dy}{dt}<br>$$</p>
<p>要將他貫通的話，我們就得由從神經網路的基礎開始，如果是一般的前回饋網路（feed-forward network）當中的隱藏層是像下列這個樣子：</p>
<p>$$<br>h_{t+1} = f(h_t, \theta)<br>$$</p>
<p>我們可以發現像是 ResNet 這類的網路有 skip connection 的設置，所以跟一般的前回饋網路不同</p>
<p>$$<br>h_{t+1} = h_t + f(h_t, \theta)<br>$$</p>
<p>而 RNN 等等有序列概念的模型也有類似的結構，就是會是前一層的結果加上通過 $f$ 運算後的結果，成為下一層的結果。</p>
<p>這樣的形式跟我們前面提到的形式不謀而合</p>
<p>$$<br>y(t + \Delta) = y(t) + \Delta \frac{dy}{dt}<br>$$</p>
<p>只要我們把 $\Delta = 1$ 代入，就成了</p>
<p>$$<br>y(t+1) = y(t) + \frac{dy}{dt}<br>$$</p>
<p>以下給大家比對一下</p>
<p>$$<br>h_{t+1} = h_t + f(h_t, \theta) \\<br>y(t+1) = y(t) + \frac{dy}{dt}<br>$$</p>
<p>也就是，我們可以讓</p>
<p>$$<br>\frac{dy}{dt} = f(h_t, \theta)<br>$$</p>
<p>神奇的事情就發生了！神經網路 $f$ 就可以被我們拿來計算微分 $\frac{dy}{dt}$！</p>
<p>比較精確的說法是，把神經網路的層 $f$ 拿來逼近微分項，或是說梯度。這樣我們後面就可以用數值方法來逼近解。</p>
<p>$$<br>y(t + \Delta) = y(t) + \Delta \frac{dy}{dt} \\<br>\downarrow \\<br>y(t + \Delta) = y(t) + \Delta f(t, h(t), \theta_t)<br>$$</p>
<p>要拉成連續的還有一個重要的手段，就是將不同的層 $t$ 從離散的變成連續的，所以作者將 $t$ 做了參數化，將他變成 $f$ 的參數之一，如此一來，就可以在任意的層中放入資料做運算。</p>
<p>最重要的概念導出了這樣的式子</p>
<p>$$<br>h(t) \rightarrow \frac{dy(t)}{dt} = f(h(t), t, \theta) \rightarrow y(t)<br>$$</p>
<h2 id="神經網路作為一個系統的微分形式"><a href="#神經網路作為一個系統的微分形式" class="headerlink" title="神經網路作為一個系統的微分形式"></a>神經網路作為一個系統的微分形式</h2><p>在傳統科學或是工程領域，我們會以微分式來表達以及建構一個系統。</p>
<p>$$<br>\nu = \frac{dx}{dt} = t + 1<br>$$</p>
<p>其實在這邊是一樣的道理，整體來說，我們是換成用神經網路去描述一個微分式，其實本質上就是這樣。</p>
<p>原本的層的概念就是用數學函數來建立的，而層與層之間傳遞著計算的結果。</p>
<p>$$<br>\mathbb{h_1} = \sigma(W_1 \mathbb{x} + \mathbb{b_1}) \\<br>\mathbb{y} = \sigma(W_2 \mathbb{h_1} + \mathbb{b_2})<br>$$</p>
<p>然而變成連續之後，我們等於是用神經網路中的層去建立跟描繪微分形式。</p>
<p>$$<br>\frac{d h_1(t)}{dt} = \sigma(W_1 \mathbb{x}(t) + \mathbb{b_1}) \\<br>\frac{d y(t)}{dt} = \sigma(W_2 \mathbb{h_1}(t) + \mathbb{b_2})<br>$$</p>
<p>是不是跟如出一轍呢？</p>
<p>$$<br>\frac{dy(t)}{dt} = f(h(t), t, \theta)<br>$$</p>

                                        

                                </div>
                            </div>
                            <div id="post-footer" class="post-footer main-content-wrap">
                                
                                        
                                            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/02/24/note-of-vector-space-to-function-space/" data-tooltip="筆記 - 從向量空間到函數空間" aria-label="下一篇: 筆記 - 從向量空間到函數空間">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2019/02/24/neural-ode/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                                                
                                                    
                                                        
                                                            <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
                                                                
                                                                            
                            </div>
                            
                                <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
                                <script>
                                    if (window.mermaid) {
                                        mermaid.initialize({ theme: 'forest' });
                                    }
                                </script>
                                
    </article>

                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2019 Yueh-Hua Tu. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
                    <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    <a class="post-action-btn btn btn--disabled">
                
                    <i class="fa fa-angle-left" aria-hidden="true"></i>
                    <span class="hide-xs hide-sm text-small icon-ml">上一篇</span>
                </a>
            </li>
            <li class="post-action">
                
                    
                    <a class="post-action-btn btn btn--default tooltip--top" href="/2019/02/24/note-of-vector-space-to-function-space/" data-tooltip="筆記 - 從向量空間到函數空間" aria-label="下一篇: 筆記 - 從向量空間到函數空間">
                
                    <span class="hide-xs hide-sm text-small icon-mr">下一篇</span>
                    <i class="fa fa-angle-right" aria-hidden="true"></i>
                </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2019/02/24/neural-ode/" title="分享到 Facebook">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a class="post-action-btn btn btn--default" href="#disqus_thread">
                        <i class="fa fa-comment-o"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


                </div>
                <div id="share-options-bar" class="share-options-bar" data-behavior="5">
    <i id="btn-close-shareoptions" class="fa fa-close"></i>
    <ul class="share-options">
        
            
            
            <li class="share-option">
                <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://yuehhua.github.io/2019/02/24/neural-ode/">
                    <i class="fa fa-facebook-official" aria-hidden="true"></i><span>分享到 Facebook</span>
                </a>
            </li>
        
    </ul>
</div>

            
        </div>
        


<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <h4 id="about-card-name">Yueh-Hua Tu</h4>
        
            <div id="about-card-bio"><p>目標是計算生物學家！</br>Systems Biology, Computational Biology, Machine Learning</br>Julia Taiwan 發起人</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>研發替代役研究助理</p>

            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Taiwan
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-xzfezjobyekpxrjktw5tz6muvzqfsbmo5n6atk3p5om9ulfptldi3p7cyqd8.min.js"></script>
<!--SCRIPTS END-->


<script>
    var disqus_config = function () {
        this.page.url = 'https://yuehhua.github.io/2019/02/24/neural-ode/';
                 
            this.page.identifier = '2019/02/24/neural-ode/';
                 
             };
    (function () {
        var d = document, s = d.createElement('script');
        var disqus_shortname = 'dream-maker';
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>



    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
